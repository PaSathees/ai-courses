{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrame Basics\n",
    "\n",
    "Spark DataFrames are the workhouse and main way of working with Spark and Python post Spark 2.0. DataFrames act as powerful versions of tables, with rows and columns, easily handling large datasets. The shift to DataFrames provides many advantages:\n",
    "* A much simpler syntax\n",
    "* Ability to use SQL directly in the dataframe\n",
    "* Operations are automatically distributed across RDDs\n",
    "    \n",
    "If you've used R or even the pandas library with Python you are probably already familiar with the concept of DataFrames. Spark DataFrame expand on a lot of these concepts, allowing you to transfer that knowledge easily by understanding the simple syntax of Spark DataFrames. Remember that the main advantage to using Spark DataFrames vs those other programs is that Spark can handle data across many RDDs, huge data sets that would never fit on a single computer. That comes at a slight cost of some \"peculiar\" syntax choices, but after this course you will feel very comfortable with all those topics!\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## Creating a DataFrame\n",
    "\n",
    "First we need to start a SparkSession:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is PySpark?\n",
    "**PySpark** is the collaboration of **Apache Spark** and **Python**.\n",
    "\n",
    "Apache Spark is an open-source cluster-computing framework, built around speed, ease of use, and streaming analytics whereas Python is a general-purpose, high-level programming language. It provides a wide range of libraries and is majorly used for Machine Learning and Real-Time Streaming Analytics.\n",
    "\n",
    "In other words, it **is a Python API for Spark** that lets you harness the simplicity of Python and the power of Apache Spark in order to tame Big Data and perform massive distributed processing over resilient sets of data. It's a must for Big data’s lovers. \n",
    "\n",
    "\n",
    "# Why use PySpark in a Jupyter Notebook?\n",
    "\n",
    "**Jupyter Notebook** is a popular application that enables you to edit, run and share Python code into a web view. It allows you to modify and re-execute parts of your code in a very flexible way. That’s why Jupyter is a great tool to test and prototype programs.\n",
    "\n",
    "While using Spark, most data engineers recommends to develop either in Scala (which is the “native” Spark language) or in Python through complete PySpark API.\n",
    "\n",
    "Python for Spark is obviously slower than Scala. However many developers, love Python because it’s flexible, robust, easy to learn, and benefits from all their favorite libraries. Many would agree that, Python is the perfect language for prototyping in Big Data/Machine Learning fields.\n",
    "\n",
    "If you prefer to develop in Scala, you will find many alternatives on the following github repository: alexarchambault/jupyter-scala\n",
    "\n",
    "To learn more about Python vs. Scala pro and cons for Spark context, please refer to this interesting article: Scala vs. Python for Apache Spark.\n",
    "\n",
    "Now, let’s get started.\n",
    "\n",
    "# How is PySpark different than Python?\n",
    "\n",
    "One of the most noteable differences you will find with PySpark as opposed to Python is that it runs on a SparkContext which is a cluster, so certian processes will look different especially when you get in the machine learning libraries. In addition to this main difference, I've note a few attibutes to be aware of below:\n",
    "\n",
    "1. PySpark does not use indexing\n",
    "2. **ALL** objects in PySpark are **immutable**\n",
    "3. Error messages are much less informative\n",
    "4. Many of the libraries you are used to using in Python won't function in PySpark\n",
    "\n",
    "# Contents of this notebook\n",
    "This notebook is intended to give users a tangible guide to get started using Python on Apache Spark. You will notice many of the commands you are used to in Python are not imployed here like Pandas and Numpy, as well as new libraries like pyspark.sql. I hope you all find this documentation useful!\n",
    "\n",
    "# Some helpful resources\n",
    "\n",
    "- Exploring S3 Keys:https://alexwlchan.net/2017/07/listing-s3-keys/\n",
    "- Using S3 Select: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-s3select.html\n",
    "- PySpark Cheat Sheets: \n",
    "    https://www.qubole.com/resources/pyspark-cheatsheet/\n",
    "    https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf\n",
    "\n",
    "## Let's Get started!\n",
    "\n",
    "Starting a PySpark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May take awhile locally\n",
    "spark = SparkSession.builder.appName(\"Operations\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in and view data\n",
    "\n",
    "A DataFrame is equivalent to a relational table in Spark SQL, and can be created using various functions in SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3ddbb7b937d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Let Spark know about the header and infer the Schema types!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'appl_stock.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minferSchema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Start by reading a basic csv dataset\n",
    "# Let Spark know about the header and infer the Schema types!\n",
    "\n",
    "#Some csv data\n",
    "stocks = spark.read.csv('appl_stock.csv',inferSchema=True,header=True)\n",
    "sales = spark.read.csv('sales_info.csv',inferSchema=True,header=True)\n",
    "nulls = spark.read.csv(\"ContainsNull.csv\",header=True,inferSchema=True)\n",
    "\n",
    "# And one json\n",
    "people = spark.read.json('people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Solid Summary of your data:\n",
    "\n",
    "#show the data (like df.head())\n",
    "print(stocks.show())\n",
    "print(\"\")\n",
    "print(stocks.printSchema())\n",
    "print(\"\")\n",
    "print(stocks.columns)\n",
    "print(\"\")\n",
    "print(stocks.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neat \"describe\" function\n",
    "df.describe(['VISA_TYPE_ID']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary function\n",
    "df.select(\"VISA_TYPE_ID\", \"CASE_STATUS_ID\").summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data types make it easier to infer schema (like tabular formats such as csv which we will show later). \n",
    "\n",
    "However you often have to set the schema yourself if you aren't dealing with a .read method that doesn't have inferSchema() built-in.\n",
    "\n",
    "Spark has all the tools you need for this, it just requires a very specific structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField,StringType,IntegerType,StructType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create the list of Structure fields\n",
    "    * :param name: string, name of the field.\n",
    "    * :param dataType: :class:`DataType` of the field.\n",
    "    * :param nullable: boolean, whether the field can be null (None) or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [StructField(\"age\", IntegerType(), True),StructField(\"name\", StringType(), True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_struc = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = spark.read.json('people.json', schema=final_struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now something a bit more complicated: Read in a full parquet\n",
    "bucket = \"dol-ocio-dac-oflc-curated-prod\"\n",
    "#reads in all partitioned keys with this begining....\n",
    "key = \"oflc-cmpdbprod/OFLC_CMS/OFLC_CASE/\"\n",
    "df = spark.read.parquet('s3://'+bucket+'/'+key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in user specified parts of partitions\n",
    "\n",
    "bucket = \"dol-ocio-dac-oflc-datascience-test\"\n",
    "key1 = \"partition_test/oflc-cmpdb/OFLC_CMS/OFLC_CASE/CREATED_YEAR=2015/*\"\n",
    "key2 = \"partition_test/oflc-cmpdb/OFLC_CMS/OFLC_CASE/CREATED_YEAR=2017/*\"\n",
    "# key3 = \"partition_test/oflc-cmpdb/OFLC_CMS/OFLC_CASE/CREATED_YEAR=2018/part-00000-1cf1268f-9a00-443e-93a6-b148021f5753.c000.snappy.parquet\"\n",
    "key3 = \"partition_test/oflc-cmpdb/OFLC_CMS/OFLC_CASE/CREATED_YEAR=2018/*\"\n",
    "\n",
    "test_df = spark.read.parquet('s3://'+bucket+'/'+key1,\\\n",
    "                             's3://'+bucket+'/'+key2, \\\n",
    "                             's3://'+bucket+'/'+key3)\n",
    "\n",
    "test_df.show(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Data\n",
    "\n",
    "A large part of working with DataFrames is the ability to quickly filter out data based on conditions. Spark DataFrames are built on top of the Spark SQL platform, which means that is you already know SQL, you can quickly and easily grab that data using SQL commands, or using the DataFram methods (which is what we focus on in this course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SQL\n",
    "stocks.filter(\"Close<500\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks.select(['Open','Close']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SQL with .select()\n",
    "stocks.filter(\"Close<500\").select(['Open','Close']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it yourself!\n",
    "# Edit the line below to select only closing values above 800\n",
    "stocks.filter(\"Close<500\").select(['Open','Close']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Slicing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing\n",
    "from pyspark.sql.functions import slice\n",
    "# pyspark.sql.functions.slice(x, start, length)[source]\n",
    "# Returns an array containing all the elements in x from index start (or starting from the end if start is negative) with the specified length. \n",
    "# Note: indexing starts at 1 here\n",
    "\n",
    "slicer = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], ['x']) \n",
    "slicer.select(slice(slicer.x, 2, 2).alias(\"sliced\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Like**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like\n",
    "# Show only the emails that are gmail emails\n",
    "df.select(\"CASE_ID\",\"CREATED_BY\").where(df.CREATED_BY.like(\"%GMAIL%\")).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Substrings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substring\n",
    "# Extract the second set of 3 numbers from the Case Number Field\n",
    "df.select(\"CASE_ID\",\"CASE_NUMBER\",df.CASE_NUMBER.substr(4, 3)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case When**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case When \n",
    "# Create a binary classifier for age (ie. label everyone over 30 as 1 and everyone else as 0)\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "values = [('Kyle',10),('Melbourne',36),('Nina',123),('Stephen',48),('Orphan',16),('Imran',1)]\n",
    "case_when = spark.createDataFrame(values,['name','age'])\n",
    "case_when.show()\n",
    "\n",
    "case_when.select(\"name\",\"age\",F.when(case_when.age > 30, 1).otherwise(0).alias(\"Binary\")).show()\n",
    "\n",
    "# df[df.firstName.isin(\"Jane\",\"Boris\")].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IS IN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISIN \n",
    "# Select Specific people from a column\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "values = [('Bananas',10),('Chips',36),('Sandwiches',123),('Chicken Fingers',48),('Fries',16),('Eggplant',1)]\n",
    "IS_IN = spark.createDataFrame(values,['food','Quantity'])\n",
    "IS_IN.show()\n",
    "\n",
    "IS_IN[IS_IN.food.isin(\"Bananas\",\"Eggplant\")].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Starts with Ends with**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Startswith – Endswith\n",
    "# Search for a specific case - begins with \"PW\" and ends with \"1\"\n",
    "df.select(\"CASE_ID\",\"CASE_NUMBER\").where(df.CASE_NUMBER.startswith(\"PW\")) \\\n",
    "                                  .where(df.CASE_NUMBER.endswith(\"1\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Results as Python Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting results as Python objects\n",
    "result = stocks.filter(\"Low=197.16\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the nested structure returns a nested row object\n",
    "type(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows can be called to turn into dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row.asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in result[0]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulate Data\n",
    "\n",
    "Dataframes in pyspark are immutable, so you need to create a new dataframe if you want to manipulate it. If you just want to test some code, you can use the .show() method as I've shown below. This will only display the results and NOT change your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change a datatype\n",
    "# Available types:\n",
    "    # DataType\n",
    "    # NullType\n",
    "    # StringType\n",
    "    # BinaryType\n",
    "    # BooleanType\n",
    "    # DateType\n",
    "    # TimestampType\n",
    "    # DecimalType\n",
    "    # DoubleType\n",
    "    # FloatType\n",
    "    # ByteType\n",
    "    # IntegerType\n",
    "    # LongType\n",
    "    # ShortType\n",
    "    # ArrayType\n",
    "    # MapType\n",
    "    # StructField\n",
    "    # StructType\n",
    "    \n",
    "# Notice all vars are stings above....\n",
    "# let's change that\n",
    "from pyspark.sql.types import * #IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = df.withColumn(\"CASE_ID\", df[\"CASE_ID\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"VISA_TYPE_ID\", df[\"VISA_TYPE_ID\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"MODIFIED_DATE\", to_timestamp(df.MODIFIED_DATE, 'yyyy-MM-dd HH:mm:ss')) \\\n",
    "        .withColumn(\"SUBMIT_DATE\", to_timestamp(df.SUBMIT_DATE, 'yyyy-MM-dd HH:mm:ss'))\n",
    "print(df.printSchema())\n",
    "print(df.show(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column from an existing column like this....\n",
    "# withColumn(colName, col)[source]\n",
    "# Returns a new DataFrame by adding a column or replacing the existing column that has the same name.\n",
    "# The column expression must be an expression over this DataFrame; attempting to add a column from some other dataframe will raise an error.\n",
    "\n",
    "# Parameters\n",
    "# colName – string, name of the new column.\n",
    "\n",
    "# col – a Column expression for the new column.\n",
    "df.withColumn('CASE_NUMBER', df.CASE_NUMBER + 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Rename\n",
    "renamed = people.withColumnRenamed('age','supernewage').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract or Create New Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate columns\n",
    "# pyspark.sql.functions.concat_ws(sep, *cols)[source]\n",
    "# Concatenates multiple input string columns together into a single string column, using the given separator.\n",
    "\n",
    "names = spark.createDataFrame([('Abraham','Lincoln')], ['first_name', 'last_name'])\n",
    "names.select(names.first_name,names.last_name,concat_ws(' ', names.first_name, names.last_name).alias('full_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year, month, day etc. from a date field\n",
    "# Other options: dayofmonth, dayofweek, dayofyear, weekofyear\n",
    "import pyspark.sql.functions as fn\n",
    "year = df.withColumn(\"SUBMIT_YEAR\",fn.year(\"SUBMIT_DATE\")) \\\n",
    "         .withColumn(\"SUBMIT_MONTH\",fn.month(\"SUBMIT_DATE\"))\n",
    "#QA\n",
    "year.filter(\"SUBMIT_YEAR=2019\").select(['SUBMIT_DATE','SUBMIT_YEAR','SUBMIT_MONTH']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference between two dates:\n",
    "# pyspark.sql.functions.datediff(end, start)\n",
    "# Returns the number of days from start to end.\n",
    "\n",
    "date_df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n",
    "date_df.select(datediff(date_df.d2, date_df.d1).alias('diff')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a string around pattern (pattern is a regular expression).\n",
    "from pyspark.sql.functions import *\n",
    "# pyspark.sql.functions.split(str, pattern)[source]\n",
    "\n",
    "abc = spark.createDataFrame([('ab12cd',)], ['s',])\n",
    "abc.select(abc.s,split(abc.s, '[0-9]+').alias('news')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim\n",
    "# pyspark.sql.functions.trim(col) - Trim the spaces from both ends for the specified string column.\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "trim_ex = spark.createDataFrame([(' 2015-04-08 ',' 2015-05-10 ')], ['d1', 'd2']) # create a dataframe - notice the extra whitespaces in the date strings\n",
    "trim_ex.show()\n",
    "print(\"left trim\")\n",
    "trim_ex.select('d1', ltrim(trim_ex.d1)).show()\n",
    "print(\"right trim\")\n",
    "trim_ex.select('d1', rtrim(trim_ex.d1)).show()\n",
    "print(\"trim\")\n",
    "trim_ex.select('d1', trim(trim_ex.d1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex!\n",
    "# Regex is used to replace or extract all substrings of the specified string value that match regexp with rep.\n",
    "# regexp_replace(str, pattern, replacement)\n",
    "# for more info on regex calls visit: https://docs.oracle.com/cd/B19306_01/server.102/b14200/ap_posix001.htm#BABJDBHB\n",
    "from pyspark.sql.functions import regexp_replace, regexp_extract\n",
    "\n",
    "print(\"Example: Remove all commas in a string field:\")\n",
    "reggi = spark.createDataFrame([('2, 5, and 10 are numbers in this example',)], ['dirty'])\n",
    "reggi.select(reggi.dirty,regexp_replace('dirty', r'(\\d)', '#').alias('clean')).show(1, False) #False will prevent col from truncating results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrays - col/cols – list of column names (string) or list of Column expressions that have the same data type.\n",
    "# pyspark.sql.functions\n",
    "from pyspark.sql.functions import *\n",
    "#      .array(*cols)   -   Creates a new array column.\n",
    "#      .array_contains(col, value)  - Collection function: returns null if the array is null, true if the array contains the given value, and false otherwise.\n",
    "#      .array_distinct(col) - Collection function: removes duplicate values from the array. :param col: name of column or expression\n",
    "#      .array_except(col1, col2) - Collection function: returns an array of the elements in col1 but not in col2, without duplicates.\n",
    "#      .array_intersect(col1, col2) - Collection function: returns an array of the elements in the intersection of col1 and col2, without duplicates.\n",
    "#      .array_join(col, delimiter, null_replacement=None) - Concatenates the elements of column using the delimiter. Null values are replaced with null_replacement if set, otherwise they are ignored.\n",
    "#      .array_max(col) - Collection function: returns the maximum value of the array.\n",
    "#      .array_min(col) - Collection function: returns the minimum value of the array.\n",
    "#      .array_position(col, value) - Collection function: Locates the position of the first occurrence of the given value in the given array. Returns null if either of the arguments are null.\n",
    "#      .array_remove(col, element)- Collection function: Remove all elements that equal to element from the given array.\n",
    "#      .array_repeat(col, count) - Collection function: creates an array containing a column repeated count times.\n",
    "#      .array_sort(col) - Collection function: sorts the input array in ascending order. The elements of the input array must be orderable. Null elements will be placed at the end of the returned array.\n",
    "#      .array_union(col1, col2) - Collection function: returns an array of the elements in the union of col1 and col2, without duplicates.\n",
    "#      .arrays_overlap(a1, a2) - Collection function: returns true if the arrays contain any common non-null element; if not, returns null if both the arrays are non-empty and any of them contains a null element; returns false otherwise.\n",
    "#      .arrays_zip(*cols)[source] - Collection function: Returns a merged array of structs in which the N-th struct contains all N-th values of input arrays.\n",
    "\n",
    "customer = spark.createDataFrame([('coffee','milk','coffee','coffee','chocolate','')], ['item1', 'item2','item3','item4','item5','item6'])\n",
    "purchases = customer.select(array('item1', 'item2','item3').alias(\"Monday\"),\\\n",
    "                            array('item4', 'item5','item6').alias(\"Tuesday\"))\n",
    "\n",
    "print(\"array\")\n",
    "purchases.show()\n",
    "\n",
    "print(\"Which customers purchased milk? array_contains\")\n",
    "purchases.select(array_contains(purchases.Monday, \"milk\")).show(1, False)\n",
    "\n",
    "print(\"List of unique products purchased on Monday: array_distinct\")\n",
    "purchases.select(array_distinct(purchases.Monday)).show(1, False)\n",
    "\n",
    "print(\"What did our customers order on Monday but not Tuesday? array_except\")\n",
    "purchases.select(array_except(purchases.Monday, purchases.Tuesday)).show(1, False)\n",
    "\n",
    "print(\"What did our customers order on BOTH Monday and Tuesday?: array_intersect\")\n",
    "purchases.select(array_intersect(purchases.Monday, purchases.Tuesday)).show(1, False)\n",
    "\n",
    "print(\"All purchases on monday in a string: array_join\")\n",
    "purchases.select(array_join(purchases.Monday, ',')).show(1, False)\n",
    "\n",
    "# Now you try!\n",
    "# Can you list all the unique products purchased on BOTH Monday and Tuesday using one of the functions above\n",
    "# The answer should be: 'coffee','milk','chocolate'\n",
    "# Note: we did not use the function yet... you need to figure out which one to use :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins and Appends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPEND two dataframes together that have the same columns\n",
    "new_df = df\n",
    "df_concat = df.union(new_df)\n",
    "print((\"df Counts:\", df.count(), len(df.columns)))\n",
    "print((\"df_concat Counts:\", df_concat.count(), len(df_concat.columns)))\n",
    "print(df_concat.show(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JOINS!\n",
    "\n",
    "valuesA = [('Pirate',1,'Arrrg'),('Monkey',2,'Oooo'),('Ninja',3,'Yaaaa'),('Spaghetti',4,'Slurp!')]\n",
    "TableA = spark.createDataFrame(valuesA,['name','id','sound'])\n",
    "\n",
    "valuesB = [('Rutabaga',1,2),('Pirate',2,45),('Ninja',3,102),('Darth Vader',4,87)]\n",
    "TableB = spark.createDataFrame(valuesB,['name','id','age'])\n",
    "\n",
    "print(\"This is TableA\")\n",
    "print(TableA.show())\n",
    "print(\"And this is TableB\")\n",
    "print(TableB.show())\n",
    "\n",
    "inner_join = TableA.join(TableB, [\"name\",\"id\"],\"inner\")\n",
    "print(\"Inner Join Example\")\n",
    "print(inner_join.show())\n",
    "\n",
    "left_join = TableA.join(TableB, [\"name\",\"id\"], how='left') # Could also use 'left_outer'\n",
    "print(\"Left Join Example\")\n",
    "print(left_join.show())\n",
    "\n",
    "conditional_join = TableA.join(TableB, [\"name\",\"id\"], how='left').filter(TableB.name.isNull())\n",
    "print(\"Conditional Left Join\")\n",
    "print(conditional_join.show())\n",
    "\n",
    "right_join = TableA.join(TableB,  [\"name\",\"id\"],how='right') # Could also use 'right_outer'\n",
    "print(\"Right Join\")\n",
    "print(right_join.show())\n",
    "\n",
    "full_outer_join = TableA.join(TableB, [\"name\",\"id\"],how='full') # Could also use 'full_outer'\n",
    "print(\"Full Outer Join\")\n",
    "print(full_outer_join.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you try!\n",
    "# Can you create a query to show ONLY Ninja and Darth Vader names WITHOUT the age column?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SQL Queries on your DataFrame\n",
    "\n",
    "Spark TempView provides two functions that allow users to run **SQL** queries against a Spark DataFrame: \n",
    "\n",
    "1. **createOrReplaceTempView:** The lifetime of this temporary view is tied to the [[SparkSession]] that was used to create this Dataset. It creates (or replaces if that view name already exists) a lazily evaluated \"view\" that you can then use like a hive table in Spark SQL. It does not persist to memory unless you cache the dataset that underpins the view.\n",
    "\n",
    "2. **createGlobalTempView:** The lifetime of this temporary view is tied to this Spark application.\n",
    "\n",
    "**Spark Session vs. Spark application:**\n",
    "\n",
    "Spark application can be used:\n",
    "\n",
    "- for a single batch job\n",
    "- an interactive session with multiple jobs\n",
    "- a long-lived server continually satisfying requests\n",
    "- A Spark job can consist of more than just a single map and reduce.\n",
    "- A Spark Application can consist of more than one session\n",
    "\n",
    "A SparkSession on the other hand is associated to a Spark Application:\n",
    "\n",
    "Generally, a session is an interaction between two or more entities. In Spark 2.0 you can use SparkSession. A SparkSession can be created without creating SparkConf, SparkContext or SQLContext, (they’re encapsulated within the SparkSession)\n",
    "\n",
    "Global temporary views are introduced in Spark 2.1.0 release. This feature is useful when you want to share data among different sessions and keep alive until your application ends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view of the dataframe\n",
    "people.createOrReplaceTempView(\"tempview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then Query the temp view\n",
    "spark.sql(\"SELECT * FROM people WHERE age=30\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or pass it to an object\n",
    "sql_results = spark.sql(\"SELECT * FROM people\")\n",
    "sql_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupBy and Aggregate Functions\n",
    "\n",
    "Let's learn how to use GroupBy and Aggregate methods on a DataFrame. GroupBy allows you to group rows together based off some column value, for example, you could group together sales data by the day the sale occured, or group repeast customer data based off the name of the customer. Once you've performed the GroupBy operation you can use an aggregate function off that data. An aggregate function aggregates multiple rows of data into a single output, such as taking the sum of inputs, or counting the number of inputs.\n",
    "\n",
    "Let's see some examples on an example dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby Function with count (you can also use sum, min, max)\n",
    "stocks.groupBy(\"Company\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then group there, you can add the following aggregate functions: mean, count, min, max, sum\n",
    "# Like this for example\n",
    "stocks.groupBy(\"Company\").mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is also a pretty neat function you can use:\n",
    "stocks.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do a summary for specific columns first select them:\n",
    "stocks.select(\"age\", \"name\").summary(\"count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate on the entire DataFrame without groups (shorthand for df.groupBy.agg())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate!\n",
    "# agg(*exprs)\n",
    "# Aggregate on the entire DataFrame without groups (shorthand for df.groupBy.agg()).\n",
    "# available agg functions: min, max, count, countDistinct, approx_count_distinct\n",
    "# df.agg.(covar_pop(col1, col2)) Returns a new Column for the population covariance of col1 and col2\n",
    "# df.agg.(covar_samp(col1, col2)) Returns a new Column for the sample covariance of col1 and col2.\n",
    "# df.agg(corr(col1, col2)) Returns a new Column for the Pearson Correlation Coefficient for col1 and col2.\n",
    "from pyspark.sql import functions as F\n",
    "df.agg(F.min(df.VISA_TYPE_ID).alias(\"Min Visa Type\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max sales across everything\n",
    "stocks.agg({'Sales':'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And then if you want to select only certian columns you can do this:\n",
    "stocks.groupBy(\"Company\").agg({\"Sales\":'max'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot Function\n",
    "# pivot(pivot_col, values=None)\n",
    "df.groupBy(\"CASE_CATEGORY_ID\").pivot(\"VISA_TYPE_ID\", [\"7\", \"3\"]).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "There are a variety of functions you can import from pyspark.sql.functions. Check out the documentation for the full list available:\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the functions we will need:\n",
    "from pyspark.sql.functions import countDistinct,avg,stddev\n",
    "from pyspark.sql.functions import abs # Absolute value\n",
    "from pyspark.sql.functions import acos # inverse cosine of col, as if computed by java.lang.Math.acos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.select(countDistinct(\"Sales\"),avg('Sales'),stddev(\"Sales\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often you will want to change the name, use the .alias() method for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.select(countDistinct(\"Sales\").alias(\"Distinct Sales\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order By\n",
    "\n",
    "You can easily sort with the orderBy method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OrderBy\n",
    "# Ascending\n",
    "sales.orderBy(\"Sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descending call off the column itself.\n",
    "sales.orderBy(df[\"Sales\"].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out this link for more info on other methods:\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module\n",
    "\n",
    "Not all methods need a groupby call, instead you can just call the generalized .agg() method, that will call the aggregate across all rows in the dataframe column specified. It can take in arguments as a single column, or create multiple aggregate calls all at once using dictionary notation.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dates and Timestamps\n",
    "\n",
    "Most real world datasets contain Time and Date information, let's see how to work with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import all the functions you will need:\n",
    "from pyspark.sql.functions import format_number,dayofmonth,hour,dayofyear,month,year,weekofyear,date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then you can extract these elements from the date column...\n",
    "sales.select(year(df['Date']),month(df['Date'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for example, let's say we wanted to know the average ________ per year. Easy! With a groupby and the year() function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = sales.withColumn(\"Year\",year(df['Date']))\n",
    "result = newdf.groupBy(\"Year\").mean()[['avg(Year)','avg(Close)']]\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you wanted to format your results at bit you could add the format_number and alias functions as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.select(\\\n",
    "                       'avg(Year)'.alias(\"Year\"),\\\n",
    "                       format_number('avg(Close)',2).alias(\"Mean Close\")\\\n",
    "                      ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data\n",
    "\n",
    "Often data sources are incomplete, which means you will have missing data, you have 3 basic options for filling in missing data (you will personally have to make the decision for what is the right approach:\n",
    "\n",
    "* Just keep the missing data points.\n",
    "* Drop them missing data points (including the entire row)\n",
    "* Fill them in with some other value.\n",
    "\n",
    "Let's cover examples of each of these methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping the missing data\n",
    "A few machine learning algorithms can easily deal with missing data, let's see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using the nulls dataframe for this section:\n",
    "nulls.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop the missing data\n",
    "\n",
    "You can use the .na functions for missing data. The drop command has the following parameters:\n",
    "\n",
    "    df.na.drop(how='any', thresh=None, subset=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any row that contains missing data\n",
    "nulls.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that have at least 2 NON-null values\n",
    "nulls.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only drop the rows whose values in the sales column are null\n",
    "nulls.na.drop(subset=[\"Sales\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any row that has a null value in ANY column;\n",
    "nulls.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a row only if ALL its values are null.\n",
    "nulls.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to do it\n",
    "df.filter(df.CERT_RETURNED.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill the missing values\n",
    "\n",
    "We can also fill the missing values with new values. If you have multiple nulls across multiple data types, Spark is actually smart enough to match up the data types. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill all nulls values with one common value (character value)\n",
    "nulls.na.fill('NEW VALUE').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill all nulls values with one common value (numeric value)\n",
    "nulls.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually you should specify what columns you want to fill with the subset parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls.na.fill('No Name',subset=['Name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very common practice is to fill values with the mean value for the column, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "mean_val = nulls.select(mean(nulls['Sales'])).collect()\n",
    "\n",
    "# Weird nested formatting of Row object!\n",
    "mean_val[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sales = mean_val[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls.na.fill(mean_sales,[\"Sales\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all we need to know for now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c7b7b6fdb857>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# pyspark.sql.functions.levenshtein(left, right)  - Computes the Levenshtein distance of the two given strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Aple'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Apple'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Microsoft'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'IBM'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Input'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Option1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Option2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Option3'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Correct this company name: Aple\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdf0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevenshtein\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Input'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Option1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Apple'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute the levenshtein distance beween two strings\n",
    "# pyspark.sql.functions.levenshtein(left, right)  - Computes the Levenshtein distance of the two given strings.\n",
    "\n",
    "df0 = spark.createDataFrame([('Aple', 'Apple','Microsoft','IBM')], ['Input', 'Option1','Option2','Option3'])\n",
    "print(\"Correct this company name: Aple\")\n",
    "df0.select(levenshtein('Input', 'Option1').alias('Apple')).show()\n",
    "df0.select(levenshtein('Input', 'Option2').alias('Microsoft')).show()\n",
    "df0.select(levenshtein('Input', 'Option3').alias('IBM')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
