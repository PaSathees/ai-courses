{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Structured Streaming Example\n",
    "\n",
    "For this first example we are going to be going over an article and code provided by Nabarun Chakraborti on Medium. While much of the detail he provided was useful, he was running in Databricks so we will need to adapt it to be able to run in a Jupyter Notebook or actually other Python IDE. I had to make just a few adjustments to the structure of the process that I wanted to go through with you all becuase it'll be foundational to learning how to use this really useful feature. So let's dig in!\n",
    "\n",
    "### Goal\n",
    "From live tweet feeds get the count of different hashtag values based on specific topic we are interested in.\n",
    "\n",
    "**Code Source:** https://ch-nabarun.medium.com/easy-to-play-with-twitter-data-using-spark-structured-streaming-76fe86f1f81c\n",
    "\n",
    "### Pre Requisits\n",
    "\n",
    "1. **Twitter Developer Account** (get the authentication keys):\n",
    "    - Login to your developer account : https://developer.twitter.com/en/apps\n",
    "    - Click on ‘create app’, provide a name\n",
    "    - Now, regenerate API keys and auth token keys. We are going to use these keys in our code to connect with twitter and get the live feeds.\n",
    "    - Copy all 4 token keys as mentioned above: access_token, access_secret_token, consumer_key and consumer_secret_key\n",
    "        - *Note: consumer_key and consumer_secret_key are like username and access_token and access_secret_token are like password.*\n",
    "        \n",
    "2. **pip installs** (go to your command prompt and type \"pip install _____\" where the ____ are the items listed below\n",
    "    - tweepy\n",
    "    \n",
    "### Coding concept\n",
    "In this example, we will have one python code file (Tweet_Listener_class.py) which will create a socket for us to use which relies on those 4 authentication keys we created. That code creates the connection with twitter, extracts the feed and channelizes them using Socket or Kafka. For demonstration I’ve used Socket but we can also use Kafka to publish and consume. *If you are willing to use Kafka then you need to install required packages, and start zookeeper service followed by Kafka server.*\n",
    "\n",
    "In the next phase of the flow, the Spark Structured Streaming program will receive the live feeds from the socket or Kafka and then perform required transformations. Finally we will write those transformed data into memory and run our required analysis on top of it.\n",
    "\n",
    "### Program Flow\n",
    "There will be two different steps to this process.\n",
    "1. Tweet_Listener.py (python programming in command line)\n",
    "    - This creates a socket for us that we then can connect our jupyter notebook to\n",
    "2. StreamingTweetData (Spark Structured Streaming in Jupyter)\n",
    "    - Then we connect to our socket (defined by the host and port number) and read the data off of that\n",
    "\n",
    "##### Purpose of Tweets_Listener class\n",
    "1. Import all necessary libraries to create connection with Twitter, read the tweet and keep it available for streaming.\n",
    "2. Read the incoming tweet JSON file (The inflow tweets are in JSON format).\n",
    "3. Retrieve only the actual tweet message and sent it to the client socket.\n",
    "4. Define the host and port. Initialize the socket object and bind host and port together.\n",
    "5. Establish the connection with Client.\n",
    "6. Use the authentication keys (access_token, access_secret_token, consumer_key and consumer_secret_key) to get the live stream data.\n",
    "7. Filter tweets which contains a specific subjects. In my example I searched tweets related to ‘corona’. We can pass multiple tracking criteria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your dependecies\n",
    "import pyspark # run after findspark.init() if you need it\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "# from pyspark.sql.functions import col, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://orcuns-mbp-2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TwitterStream</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa748fd5700>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start up your pyspark session as always\n",
    "# Don't run this more than once\n",
    "spark = SparkSession.builder.appName(\"TwitterStream\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to set up a kind of structure to tell spark where to read our stream from. In this case, we will match the below host and port number specified to the one we set up in our Tweet Listner class python file. \n",
    "\n",
    "Here we are reading the live streaming data from socket and type casting to String."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the tweet data from socket\n",
    "tweet_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"127.0.0.1\") \\\n",
    "    .option(\"port\", 3350) \\\n",
    "    .load()\n",
    "\n",
    "# type cast the column value\n",
    "tweet_df_string = tweet_df.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how it would look for Kafka\n",
    "# tweet_df = spark \\\n",
    "#   .readStream \\\n",
    "#   .format(\"kafka\") \\\n",
    "#   .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "#   .option(\"subscribe\", \"trump\") \\\n",
    "#   .load()\n",
    "\n",
    "# # Type cast the key and column value\n",
    "# tweet_df_string = tweet_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then split words based on space, filter out only hashtag (#) values and group them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tab = tweet_df_string.withColumn('word', explode(split(col('value'), ' '))) \\\n",
    "    .groupBy('word') \\\n",
    "    .count() \\\n",
    "    .sort('count', ascending=False). \\\n",
    "    filter(col('word').contains('#'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that write the above data into memory. Consider all data in each iterations (output mode = complete), and let the trigger run in every 2 seconds.\n",
    "\n",
    "This is where the tweet listener actually starts running. Go check out your command line prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- streaming is running -------\n"
     ]
    }
   ],
   "source": [
    "writeTweet = tweets_tab.writeStream. \\\n",
    "    outputMode(\"complete\"). \\\n",
    "    format(\"memory\"). \\\n",
    "    queryName(\"tweetquery\"). \\\n",
    "    trigger(processingTime='2 seconds'). \\\n",
    "    start()\n",
    "\n",
    "print(\"----- streaming is running -------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Every time you run this cell, there will be fresh data!\n",
    "# And the streaming keeps running\n",
    "spark.sql(\"select * from tweetquery\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the query\n",
    "writeTweet.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writeTweet.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writeTweet.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
