{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow Tracking Example\n",
    "\n",
    "MLflow is organized into four components: **Tracking**, **Projects**, **Models**, and **Model Registry**. You can use each of these components on their own—for example, maybe you want to export models in MLflow’s model format without using Tracking or Projects—but they are also designed to work well together. So this notebook will focus on only the **Tracking** component within the PySpark environment. \n",
    "\n",
    "### Why is tracking useful/important?\n",
    "\n",
    "Machine learning typically requires experimenting with a diverse set of hyperparameter tuning techniques, data preparation steps, and algorithms to build a model that maximizes some target metric. Given this complexity, building a machine learning model can therefore be challenging for a couple of reasons:\n",
    "\n",
    "1. **It’s difficult to keep track of experiments.** When you are just working with files on your laptop, or with an interactive notebook, how do you tell which data, code and parameters went into getting a particular result?\n",
    "2. **It’s difficult to reproduce code.** Even if you have meticulously tracked the code versions and parameters, you need to capture the whole environment (for example, library dependencies) to get the same result again. This is especially challenging if you want another data scientist to use your code, or if you want to run the same code at scale on another platform (for example, in the cloud).\n",
    "\n",
    "### Solution that MLflow Tracking provides\n",
    "\n",
    "MLflow Tracking is an API and UI for logging parameters, code versions, metrics, and artifacts when running your machine learning code and for later visualizing the results. You can use MLflow Tracking in any environment (for example, a standalone script or a notebook) to log results to local files or to a server, then compare multiple runs.\n",
    "\n",
    "### How to install MLflow\n",
    "\n",
    "You simply install MLflow by running *\"pip install mlflow\"* via the command line. Please reference the Quick Start Guide here for more details: https://mlflow.org/docs/latest/quickstart.html\n",
    "\n",
    "### Viewing the Tracking MLflow UI\n",
    "\n",
    "By default, wherever you run your program (Jupyter Notebook in this case), the tracking API writes data into files into a local ./mlruns directory. First you need to open your mlflow intance via the command line (cd into the folder where this notebook is stored). You can then run MLflow’s Tracking UI: http://localhost:5000/#/\n",
    "\n",
    "### How to cd into a folder\n",
    "\n",
    " - **Mac**: https://macpaw.com/how-to/use-terminal-on-mac\n",
    " - **Windows**: https://www.minitool.com/news/how-to-change-directory-in-cmd.html\n",
    " \n",
    "### Getting started\n",
    "\n",
    "Since we using PySpark for this example, first we need to create a Spark Session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://orcuns-mbp-2:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Mlflow</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11ef713d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# Notice that we call our spark session ss instead of spark\n",
    "# This becuase it clashes with the spark lib we need for mlflow (tried to rename but didn't work)\n",
    "ss = SparkSession.builder.appName(\"Mlflow\").getOrCreate()\n",
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "# PySpark modeling libraries\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Mlflow libaries\n",
    "import mlflow\n",
    "from mlflow import spark\n",
    "\n",
    "# Mlflow client\n",
    "from  mlflow.tracking import MlflowClient\n",
    "client = MlflowClient()\n",
    "\n",
    "# Numpy for random number generator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'mlflow-env',\n",
       " 'channels': ['defaults'],\n",
       " 'dependencies': ['python=3.7.4', 'pyspark=2.4.4', 'pip', {'pip': ['mlflow']}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get info about your environment\n",
    "mlflow.spark.get_default_conda_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Eval Model, then Log Results to MLflow\n",
    "\n",
    "### Mexican Vanilla (a bit of flavor added)\n",
    " - Without using cross validator\n",
    " - **With** using the client which allows you to managing experiments and runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing Experiments and Runs with the Tracking Service API\n",
    "\n",
    "https://mlflow.org/docs/latest/tracking.html#managing-experiments-and-runs-with-the-tracking-service-api\n",
    "\n",
    "### Organizing Runs in Experiments\n",
    "\n",
    "https://mlflow.org/docs/latest/tracking.html#organizing-runs-in-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create experiement\n",
    "exp_id = mlflow.create_experiment(\"Experiment-0\")\n",
    "# mlflow.create_experiment(\"Experiment-0\")\n",
    "exp_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experiment\n",
    "# This will actually automatically create one if the one you call on doesn't exist\n",
    "mlflow.set_experiment(experiment_name = \"Experiment-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Experiment: artifact_location='file:///Users/orcuncanlilar/Eden/PySpark%20Essentials%20for%20Data%20Scientists/Jupyter%20Notebooks/Machine%20Learning/mlruns/0', experiment_id='0', lifecycle_stage='active', name='Default', tags={}>\n",
      " \n",
      "<Experiment: artifact_location='file:///Users/orcuncanlilar/Eden/PySpark%20Essentials%20for%20Data%20Scientists/Jupyter%20Notebooks/Machine%20Learning/mlruns/1', experiment_id='1', lifecycle_stage='active', name='first-experiment', tags={'mlflow.note.content': 'This experiment tested various tree classifiers '\n",
      "                        'across various parameters. The training process used '\n",
      "                        'a parameter grid search technique for Hyperparameter '\n",
      "                        'optimization. It was a real success!'}>\n",
      " \n",
      "<Experiment: artifact_location='file:///Users/orcuncanlilar/Eden/PySpark%20Essentials%20for%20Data%20Scientists/Jupyter%20Notebooks/Machine%20Learning/mlruns/4', experiment_id='4', lifecycle_stage='active', name='Experiment-2', tags={}>\n",
      " \n",
      "<Experiment: artifact_location='file:///Users/orcuncanlilar/Eden/PySpark%20Essentials%20for%20Data%20Scientists/Jupyter%20Notebooks/Machine%20Learning/mlruns/3', experiment_id='3', lifecycle_stage='active', name='Experiment-1', tags={}>\n",
      " \n",
      "<Experiment: artifact_location='file:///Users/orcuncanlilar/Eden/PySpark%20Essentials%20for%20Data%20Scientists/Jupyter%20Notebooks/Machine%20Learning/mlruns/2', experiment_id='2', lifecycle_stage='active', name='Experiment-0', tags={}>\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# set up your client and get list of experiments\n",
    "from  mlflow.tracking import MlflowClient\n",
    "client = MlflowClient()\n",
    "experiments = client.list_experiments() # returns a list of mlflow.entities.Experiment\n",
    "for x in experiments:\n",
    "#     print(x.name)\n",
    "    print(x)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Description:  <Experiment: artifact_location='file:///Users/orcuncanlilar/Eden/PySpark%20Essentials%20for%20Data%20Scientists/Jupyter%20Notebooks/Machine%20Learning/mlruns/3', experiment_id='3', lifecycle_stage='active', name='Experiment-1', tags={}>\n",
      " \n",
      "Name:  Experiment-1\n",
      "ID:  3\n",
      "Tags:  {}\n"
     ]
    }
   ],
   "source": [
    "# You can retrieve any of the elements from experiements that you need....\n",
    "print(\"Full Description: \",experiments[2])\n",
    "print(\" \")\n",
    "print(\"Name: \",experiments[2].name)\n",
    "print(\"ID: \",experiments[2].experiment_id)\n",
    "print(\"Tags: \",experiments[2].tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a run and attach it to the experiment you just created\n",
    "# Just to the get the general concept down\n",
    "for x in experiments:\n",
    "    if 'Experiment-1' in x.name:\n",
    "        experiment_index = experiments.index(x)\n",
    "        run = client.create_run(experiments[experiment_index].experiment_id) # returns mlflow.entities.Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conduct a run!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally you would have some ML stuff here\n",
    "\n",
    "# Add tag to a run\n",
    "client.set_tag(run.info.run_id, \"Algorithm\", \"Gradient Boosted Tree\")\n",
    "client.set_tag(run.info.run_id,\"Random Seed\",900)\n",
    "client.set_tag(run.info.run_id,\"Train Perct\",0.8)\n",
    "\n",
    "# Add params and metrics to a run\n",
    "client.log_param(run.info.run_id, \"Max Depth\", 100)\n",
    "client.log_param(run.info.run_id, \"Max Bins\", 50)\n",
    "client.log_metric(run.info.run_id, \"Accuracy\", 0.87)\n",
    "\n",
    "# Terminate the client\n",
    "client.set_terminated(run.info.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Read the wine-quality csv file (make sure you're running this from the root of MLflow!)\n",
    "#     wine_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"wine-quality.csv\")\n",
    "#     data = pd.read_csv(wine_path)\n",
    "    data = ss.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([3.0, 10.1, 3.0]),)\n",
    "], [\"label\", \"features\"])\n",
    "\n",
    "    # Split the data into training and test sets. (0.75, 0.25) split.\n",
    "    train_val = 0.7\n",
    "    test_val = 1-train_val\n",
    "    seed=40\n",
    "    np.random.seed(seed)\n",
    "    train,test = data.randomSplit([train_val,test_val],seed=seed)\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        \n",
    "        # Create a run and attach it to an experiment (using the id) and add tags\n",
    "        run = client.create_run(experiments[2].experiment_id) # returns mlflow.entities.Run\n",
    "        # Instantiate Classifier\n",
    "        classifier = DecisionTreeClassifier(maxDepth=5, maxBins=32)\n",
    "        # Extract the name of the classifier\n",
    "        classifier_name = type(classifier).__name__\n",
    "        \n",
    "        # Add tags to the run\n",
    "        client.set_tag(run.info.run_id, \"Algorithm\", classifier_name)   \n",
    "        client.set_tag(run.info.run_id,\"Random Seed\",seed)\n",
    "        client.set_tag(run.info.run_id,\"Train Perct\",train_val)\n",
    "        \n",
    "        fitModel = classifier.fit(train)\n",
    "        \n",
    "        # Evaluate\n",
    "        predictions = fitModel.transform(test)\n",
    "        \n",
    "        # Calculate Accuracy\n",
    "        MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "        accuracy = (MC_evaluator.evaluate(predictions))*100\n",
    "        print(accuracy)\n",
    "\n",
    "#         mlflow.log_param(\"alpha\", alpha)\n",
    "#         mlflow.log_param(\"l1_ratio\", l1_ratio)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "        mlflow.spark.log_model(fitModel, \"model\")\n",
    "\n",
    "        # Add params and metrics to a run\n",
    "        client.log_param(run.info.run_id, \"Max Depth\", 3)\n",
    "        client.log_param(run.info.run_id, \"Max Bins\", 5)\n",
    "        client.log_metric(run.info.run_id, \"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:  DecisionTreeClassifier_c86026698e30__cacheNodeIds :  False\n",
      "Key:  DecisionTreeClassifier_c86026698e30__checkpointInterval :  10\n",
      "Key:  DecisionTreeClassifier_c86026698e30__featuresCol :  features\n",
      "Key:  DecisionTreeClassifier_c86026698e30__impurity :  gini\n",
      "Key:  DecisionTreeClassifier_c86026698e30__labelCol :  label\n",
      "Key:  DecisionTreeClassifier_c86026698e30__maxBins :  32\n",
      "Key:  DecisionTreeClassifier_c86026698e30__maxDepth :  5\n",
      "Key:  DecisionTreeClassifier_c86026698e30__maxMemoryInMB :  256\n",
      "Key:  DecisionTreeClassifier_c86026698e30__minInfoGain :  0.0\n",
      "Key:  DecisionTreeClassifier_c86026698e30__minInstancesPerNode :  1\n",
      "Key:  DecisionTreeClassifier_c86026698e30__predictionCol :  prediction\n",
      "Key:  DecisionTreeClassifier_c86026698e30__probabilityCol :  probability\n",
      "Key:  DecisionTreeClassifier_c86026698e30__rawPredictionCol :  rawPrediction\n",
      "Key:  DecisionTreeClassifier_c86026698e30__seed :  -8543656710325985320\n"
     ]
    }
   ],
   "source": [
    "### Print a all the parameter names (keys in a dict)\n",
    "paramMap = fitModel.extractParamMap()\n",
    "for k, v in paramMap.items():\n",
    "    print(\"Key: \",k, \": \", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "##### Search for a specific Param by key word\n",
    "\n",
    "paramMap = fitModel.extractParamMap()\n",
    "# Using items() + list comprehension \n",
    "# Substring Key match in dictionary \n",
    "search_key = 'maxBins' \n",
    "\n",
    "for key, val in paramMap.items():\n",
    "    if search_key in key.name:\n",
    "        print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Eval Model, then Log Results to MLflow\n",
    "\n",
    "### Rocky Road (most flavorful option)\n",
    " - **With** using cross validator\n",
    " - **With** using the client which allows you to managing experiments and runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.33333333333333\n"
     ]
    }
   ],
   "source": [
    "# Question\n",
    "# If you use the client, do you have to do the ML start run part?\n",
    "# Test here\n",
    "# Test Result: you do not need to start a run with this method\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "for x in experiments:\n",
    "    if 'Experiment-1' in x.name:\n",
    "        experiment_index = experiments.index(x)\n",
    "        run = client.create_run(experiments[experiment_index].experiment_id)\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "data = ss.createDataFrame([\n",
    "(0, Vectors.dense([1.0, 0.1, -1.0]),),\n",
    "(1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "(1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "(2, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "(1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "(1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "(2, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "(1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "(1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "(2, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "(1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "(1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "(2, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "(2, Vectors.dense([3.0, 10.1, 3.0]),)\n",
    "], [\"label\", \"features\"])\n",
    "\n",
    "# Split the data into training and test sets. (0.75, 0.25) split.\n",
    "seed = 60\n",
    "train_val = 0.7\n",
    "test_val = 1-train_val\n",
    "train,test = data.randomSplit([train_val,test_val],seed=seed)\n",
    "\n",
    "# Create a run and attach it to an experiment (using the id) and add tags\n",
    "run = client.create_run(experiments[0].experiment_id) # returns mlflow.entities.Run\n",
    "# Instantiate Classifier\n",
    "classifier = DecisionTreeClassifier()\n",
    "# Create a parameter grid to search\n",
    "paramGrid = (ParamGridBuilder() \\\n",
    "                     .addGrid(classifier.maxDepth, [2, 5, 10, 20, 30]) \\\n",
    "                     .addGrid(classifier.maxBins, [10, 20, 40, 80, 100]) \\\n",
    "                     .build())\n",
    "\n",
    "#Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          seed=seed, #same as the one set above\n",
    "                          numFolds=2) # 3 + is best practice\n",
    "\n",
    "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Evaluate\n",
    "predictions = fitModel.transform(test)\n",
    "\n",
    "# Calculate Accuracy\n",
    "MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "accuracy = (MC_evaluator.evaluate(predictions))*100\n",
    "print(accuracy)\n",
    "\n",
    "########### Track results in MLflow UI ################\n",
    "\n",
    "# Add tag to a run\n",
    "# Extract the name of the classifier\n",
    "classifier_name = type(classifier).__name__\n",
    "client.set_tag(run.info.run_id, \"Algorithm\", classifier_name) \n",
    "client.set_tag(run.info.run_id,\"Random Seed\",seed)\n",
    "client.set_tag(run.info.run_id,\"Train Perct\",train_val)\n",
    "\n",
    "# Log Model (can't do this to the client)\n",
    "mlflow.spark.log_model(fitModel, \"model\")\n",
    "\n",
    "# Extract params of Best Model\n",
    "BestModel = fitModel.bestModel\n",
    "paramMap = BestModel.extractParamMap()\n",
    "\n",
    "# Log parameters to the client\n",
    "for key, val in paramMap.items():\n",
    "    if 'maxDepth' in key.name:\n",
    "        client.log_param(run.info.run_id, \"Max Depth\", val)\n",
    "for key, val in paramMap.items():\n",
    "    if 'maxBins' in key.name:\n",
    "        client.log_param(run.info.run_id, \"Max Bins\", 5)\n",
    "\n",
    "# Log metrics to the client\n",
    "client.log_metric(run.info.run_id, \"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Resources\n",
    "\n",
    "**Scikit Learn Example:** https://github.com/mlflow/mlflow-example\n",
    "\n",
    "**Link to Mlflow Spark Documentation:** https://mlflow.org/docs/latest/python_api/mlflow.spark.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/orcuncanlilar/Eden/PySpark%20Essentials%20for%20Data%20Scientists/Jupyter%20Notebooks/Machine%20Learning/mlruns/0', experiment_id='0', lifecycle_stage='active', name='Default', tags={}>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without the Mlflow start run bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
