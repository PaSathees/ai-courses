{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working with 1 core(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://orcuns-mbp-2:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Review2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10a44d890>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# May take awhile locally\n",
    "spark = SparkSession.builder.appName(\"Review2\").getOrCreate()\n",
    "\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "print(\"You are working with\", cores, \"core(s)\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small dataframe for quick testing if you need it\n",
    "df = spark.createDataFrame([(3,69,57,56,678,345),(3,67,56,58,678,345),(3,67,54,57,678,345),(3,68,55,58,678,345),(3,68,53,52,678,345)\n",
    "                           ,(2,11,10,907,16,458),(2,12,14,909,12,456),(2,11,13,910,10,459),(2,12,11,905,16,459),(2,10,13,902,10,459)\n",
    "                           ,(1,30,11,123,568,891),(1,32,12,124,567,890),(1,34,10,123,566,895),(1,35,15,121,564,894),(1,30,12,124,560,896)], \n",
    "                           ['flower_type', 'sepal_len','sepal_width','R','G','B'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Customer Segment data for clustering\n",
    "\n",
    "path = \"Datasets/Mall_Customers.csv\"\n",
    "\n",
    "**Content**\n",
    "\n",
    "You are owing a supermarket mall and through membership cards, you have some basic data about your customers like Customer ID, age, gender, annual income and spending score. Spending Score is something you assign to the customer based on your defined parameters like customer behavior and purchasing data.\n",
    "\n",
    "Problem Statement You own the mall and want to understand the customers like who can be easily converge [Target Customers] so that the sense can be given to marketing team and plan the strategy accordingly.\n",
    "\n",
    "**Source:** https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's read our dataset in for this notebook \n",
    "\n",
    "### Success of Bank Telemarketing\n",
    "\n",
    "### Input variables:\n",
    "\n",
    "1. Age (numeric)\n",
    "2. Job: career type (categorical: 'admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired', 'self-employed', 'services', 'student', 'technician', 'unemployed')\n",
    "3. Marital_Status: marital status (categorical: 'divorced', 'married', 'single'; note: 'divorced' means divorced or widowed)\n",
    "4. Education: (categorical: 'Primary_Education', 'Professional_Education', 'Secondary_Education', 'Tertiary_Education')\n",
    "5. Default_Credit: has a credit in default? (binary: 'yes', 'no')\n",
    "6. Housing_Loan: has a home loan? (binary: 'yes', 'no')\n",
    "7. Personal_Loan: has a personal loan? (binary: 'yes', 'no')\n",
    "\n",
    "### Output variable (desired target):\n",
    "\n",
    "- Subscribed - has the client subscribed a long-term deposit? (binary: 'yes', 'no')\n",
    "\n",
    "### Source\n",
    "https://www.kaggle.com/raosuny/success-of-bank-telemarketing-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"\"\n",
    "df = spark.read.csv(path+'telemarket_success.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinMaxScaler\n",
    "\n",
    "See if this can used to correct negative values for input for Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled to range: [0.000000, 100.000000]\n",
      "+--------------+-------------------+\n",
      "|      features|     scaledFeatures|\n",
      "+--------------+-------------------+\n",
      "|[1.0,0.1,-1.0]|      [0.0,0.0,0.0]|\n",
      "| [2.0,1.1,1.0]|   [50.0,10.0,50.0]|\n",
      "|[3.0,10.1,3.0]|[100.0,100.0,100.0]|\n",
      "+--------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([3.0, 10.1, 3.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",min=0,max=100)\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "# rescale each feature to range [min, max].\n",
    "scaledData = scalerModel.transform(dataFrame)\n",
    "print(\"Features scaled to range: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\n",
    "scaledData.select(\"features\", \"scaledFeatures\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MinMaxScalerModel' object has no attribute 'isDistributed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-77ca1d597bdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscalerModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'MinMaxScalerModel' object has no attribute 'isDistributed'"
     ]
    }
   ],
   "source": [
    "scalerModel.isDistributed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow test drive\n",
    "\n",
    "**Code Source:** https://towardsdatascience.com/complete-data-science-project-template-with-mlflow-for-non-dummies-d082165559eb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'MLFLOW_TRACKING_URI'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-dc78c186fd99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mexperiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tracking_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MLFLOW_TRACKING_URI'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'MLFLOW_TRACKING_URI'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "# from project.utility.mlflow import log_artifacts_minio\n",
    "\n",
    "train = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([3.0, 10.1, 3.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "experiment = 'test1'\n",
    "mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])\n",
    "mlflow.set_experiment(experiment)\n",
    "\n",
    "degree = 3\n",
    "with mlflow.start_run() as run:\n",
    "    feature_pipeline = features.train_new_feature_pipeline(train, degree)\n",
    "    mlflow.log_param(\"degree\", degree)\n",
    "    mlflow.spark.save_model(\n",
    "        feature_pipeline,\n",
    "        'feature_pipeline',\n",
    "        sample_input=train.select(\n",
    "            'features'\n",
    "        )\n",
    "    )\n",
    "#     log_artifacts_minio(run, 'feature_pipeline','feature_pipeline', True)\n",
    "\n",
    "#         mlflow.log_param(\"alpha\", alpha)\n",
    "#         mlflow.log_param(\"l1_ratio\", l1_ratio)\n",
    "#         mlflow.log_metric(\"rmse\", rmse)\n",
    "#         mlflow.log_metric(\"r2\", r2)\n",
    "#         mlflow.log_metric(\"mae\", mae)\n",
    "\n",
    "#         mlflow.sklearn.log_model(lr, \"model\")\n",
    "    run_id = run.info.run_id\n",
    "    print(run.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import *\n",
    "\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import ElasticNet\n",
    "\n",
    "import mlflow\n",
    "# import mlflow.sklearn\n",
    "\n",
    "\n",
    "# def eval_metrics(actual, pred):\n",
    "#     rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "#     mae = mean_absolute_error(actual, pred)\n",
    "#     r2 = r2_score(actual, pred)\n",
    "#     return rmse, mae, r2\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    np.random.seed(40)\n",
    "\n",
    "    # Read the wine-quality csv file (make sure you're running this from the root of MLflow!)\n",
    "#     wine_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"wine-quality.csv\")\n",
    "#     data = pd.read_csv(wine_path)\n",
    "    data = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([3.0, 10.1, 3.0]),)\n",
    "], [\"label\", \"features\"])\n",
    "\n",
    "    # Split the data into training and test sets. (0.75, 0.25) split.\n",
    "#     train, test = train_test_split(data)\n",
    "    train,test = data.randomSplit([0.7,0.3])\n",
    "\n",
    "    # The predicted column is \"quality\" which is a scalar from [3, 9]\n",
    "#     train_x = train.drop([\"quality\"], axis=1)\n",
    "#     test_x = test.drop([\"quality\"], axis=1)\n",
    "#     train_y = train[[\"quality\"]]\n",
    "#     test_y = test[[\"quality\"]]\n",
    "\n",
    "#     alpha = float(sys.argv[1]) if len(sys.argv) > 1 else 0.5\n",
    "#     l1_ratio = float(sys.argv[2]) if len(sys.argv) > 2 else 0.5\n",
    "\n",
    "    with mlflow.start_run():\n",
    "#         lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n",
    "#         lr.fit(train_x, train_y)\n",
    "        classifier = LogisticRegression()\n",
    "        fitModel = classifier.fit(train)\n",
    "\n",
    "#         predicted_qualities = lr.predict(test_x)\n",
    "        \n",
    "        # Evaluate\n",
    "        predictionAndLabels = fitModel.evaluate(test)\n",
    "        predictionAndLabels = predictionAndLabels.predictions.select('label','prediction')\n",
    "\n",
    "        predictions = fitModel.transform(test)\n",
    "        MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "        accuracy = (MC_evaluator.evaluate(predictions))*100\n",
    "        print(accuracy)\n",
    "\n",
    "#         (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n",
    "\n",
    "#         print(\"Elasticnet model (alpha=%f, l1_ratio=%f):\" % (alpha, l1_ratio))\n",
    "#         print(\"  RMSE: %s\" % rmse)\n",
    "#         print(\"  MAE: %s\" % mae)\n",
    "#         print(\"  R2: %s\" % r2)\n",
    "\n",
    "#         mlflow.log_param(\"alpha\", alpha)\n",
    "#         mlflow.log_param(\"l1_ratio\", l1_ratio)\n",
    "#         mlflow.log_metric(\"rmse\", rmse)\n",
    "#         mlflow.log_metric(\"r2\", r2)\n",
    "#         mlflow.log_metric(\"mae\", mae)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "#         mlflow.spark.log_model(lr, \"model\")\n",
    "        mlflow.spark.log_model(fitModel, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does PySpark Accept user input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your value: yes\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "val = input(\"Enter your value: \") \n",
    "print(val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For testing for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = spark.read.format(\"libsvm\").load(\"C:/spark-2.3.3-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\")\n",
    "train,test = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cool Dataset for Clustering\n",
    "\n",
    "This case requires to develop a customer segmentation to define marketing strategy. The sample Dataset summarizes the usage behavior of about 9000 active credit card holders during the last 6 months. The file is at a customer level with 18 behavioral variables.\n",
    "\n",
    "Following is the Data Dictionary for Credit Card dataset :-\n",
    "\n",
    "CUST_ID : Identification of Credit Card holder (Categorical) BALANCE : Balance amount left in their account to make purchases ( BALANCE_FREQUENCY : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated) PURCHASES : Amount of purchases made from account ONEOFF_PURCHASES : Maximum purchase amount done in one-go INSTALLMENTS_PURCHASES : Amount of purchase done in installment CASH_ADVANCE : Cash in advance given by the user PURCHASES_FREQUENCY : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased) ONEOFFPURCHASESFREQUENCY : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased) PURCHASESINSTALLMENTSFREQUENCY : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done) CASHADVANCEFREQUENCY : How frequently the cash in advance being paid CASHADVANCETRX : Number of Transactions made with \"Cash in Advanced\" PURCHASES_TRX : Numbe of purchase transactions made CREDIT_LIMIT : Limit of Credit Card for user PAYMENTS : Amount of Payment done by user MINIMUM_PAYMENTS : Minimum amount of payments made by user PRCFULLPAYMENT : Percent of full payment paid by user TENURE : Tenure of credit card service for user\n",
    "\n",
    "**Source:** https://www.kaggle.com/arjunbhasin2013/ccdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"\"\n",
    "df = spark.read.csv(path+'credit_card_clustering.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another cool clustering dataset (SEC)\n",
    "\n",
    "#### Context\n",
    "\n",
    "This dataset is a playground for fundamental and technical analysis. It is said that 30% of traffic on stocks is already generated by machines, can trading be fully automated? If not, there is still a lot to learn from historical data.\n",
    "\n",
    "#### Content\n",
    "\n",
    "Dataset consists of following files:\n",
    "\n",
    "prices.csv: raw, as-is daily prices. Most of data spans from 2010 to the end 2016, for companies new on stock market date range is shorter. There have been approx. 140 stock splits in that time, this set doesn't account for that.\n",
    "prices-split-adjusted.csv: same as prices, but there have been added adjustments for splits.\n",
    "securities.csv: general description of each company with division on sectors\n",
    "fundamentals.csv: metrics extracted from annual SEC 10K fillings (2012-2016), should be enough to derive most of popular fundamental indicators.\n",
    "\n",
    "**Source:** https://www.kaggle.com/dgawlik/nyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"nyse\"\n",
    "df = spark.read.csv(path+'fundamentals.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|    kurtosis(value)|\n",
      "+-------------------+\n",
      "|-1.5000000000000002|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "val1_df = spark.createDataFrame([(1,1),(1,2),(1,3),(2,60000000000000),(1,3),(1,2),(3,1)],['id','value'])\n",
    "val1_df.agg(f.kurtosis(\"value\")).show()\n",
    "# val1_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-6f83e29b4407>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m numFolds = 5)\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Run cross validations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mlrcvModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlrcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrcvModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Get Model Summary Statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# Colby Ford\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Create initial LinearRegression model\n",
    "lr = LinearRegression(labelCol=\"label\", featuresCol=\"features\")\n",
    "# Create ParamGrid for Cross Validation\n",
    "lrparamGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.001, 0.01, 0.1, 0.5, 1.0, 2.0])\n",
    "#  .addGrid(lr.regParam, [0.01, 0.1, 0.5])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "#  .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10, 20, 50])\n",
    "#  .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())\n",
    "# Evaluate model\n",
    "lrevaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"rmse\")\n",
    "# Create 5-fold CrossValidator\n",
    "lrcv = CrossValidator(estimator = lr,\n",
    "estimatorParamMaps = lrparamGrid,\n",
    "evaluator = lrevaluator,\n",
    "numFolds = 5)\n",
    "# Run cross validations\n",
    "lrcvModel = lrcv.fit(train)\n",
    "print(lrcvModel)\n",
    "# Get Model Summary Statistics\n",
    "lrcvSummary = lrcvModel.bestModel.summary\n",
    "print(\"Coefficient Standard Errors: \" + str(lrcvSummary.coefficientStandardErrors))\n",
    "print(\"P Values: \" + str(lrcvSummary.pValues)) # Last element is the intercept\n",
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "lrpredictions = lrcvModel.transform(test)\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "print('RMSE:', lrevaluator.evaluate(lrpredictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results2\n",
      "+------------------+------+\n",
      "|        Classifier|Result|\n",
      "+------------------+------+\n",
      "|      Place Holder|   N/A|\n",
      "|LogisticRegression|  52.3|\n",
      "+------------------+------+\n",
      "\n",
      "Results2\n",
      "+------------------+------+\n",
      "|        Classifier|Result|\n",
      "+------------------+------+\n",
      "|      Place Holder|   N/A|\n",
      "|LogisticRegression|  52.3|\n",
      "|         OneVsRest|  52.3|\n",
      "+------------------+------+\n",
      "\n",
      "Results2\n",
      "+------------------+------+\n",
      "|        Classifier|Result|\n",
      "+------------------+------+\n",
      "|      Place Holder|   N/A|\n",
      "|LogisticRegression|  52.3|\n",
      "|         OneVsRest|  52.3|\n",
      "|         LinearSVC|  52.3|\n",
      "+------------------+------+\n",
      "\n",
      "Replicate DF\n",
      "Results2\n",
      "+------------------+------+\n",
      "|        Classifier|Result|\n",
      "+------------------+------+\n",
      "|      Place Holder|   N/A|\n",
      "|LogisticRegression|  52.3|\n",
      "+------------------+------+\n",
      "\n",
      "Results2\n",
      "+------------------+------+\n",
      "|        Classifier|Result|\n",
      "+------------------+------+\n",
      "|      Place Holder|   N/A|\n",
      "|LogisticRegression|  52.3|\n",
      "|         OneVsRest|  52.3|\n",
      "+------------------+------+\n",
      "\n",
      "Results2\n",
      "+------------------+------+\n",
      "|        Classifier|Result|\n",
      "+------------------+------+\n",
      "|      Place Holder|   N/A|\n",
      "|LogisticRegression|  52.3|\n",
      "|         OneVsRest|  52.3|\n",
      "|         LinearSVC|  52.3|\n",
      "+------------------+------+\n",
      "\n",
      "Replicate DF\n",
      "Results2\n",
      "+------------------+------+\n",
      "|        Classifier|Result|\n",
      "+------------------+------+\n",
      "|      Place Holder|   N/A|\n",
      "|LogisticRegression|  52.3|\n",
      "+------------------+------+\n",
      "\n",
      "Results2\n",
      "+------------------+------+\n",
      "|        Classifier|Result|\n",
      "+------------------+------+\n",
      "|      Place Holder|   N/A|\n",
      "|LogisticRegression|  52.3|\n",
      "|         OneVsRest|  52.3|\n",
      "+------------------+------+\n",
      "\n",
      "Results2\n",
      "+------------------+------+\n",
      "|        Classifier|Result|\n",
      "+------------------+------+\n",
      "|      Place Holder|   N/A|\n",
      "|LogisticRegression|  52.3|\n",
      "|         OneVsRest|  52.3|\n",
      "|         LinearSVC|  52.3|\n",
      "+------------------+------+\n",
      "\n",
      "Replicate DF\n",
      "Results3\n",
      "+------------------+------+------+\n",
      "|        Classifier|Result|Result|\n",
      "+------------------+------+------+\n",
      "|         OneVsRest|  34.5|  52.3|\n",
      "|LogisticRegression|  23.8|  52.3|\n",
      "|         LinearSVC|  76.3|  52.3|\n",
      "+------------------+------+------+\n",
      "\n",
      "+------------------+------+------+\n",
      "|Classifier        |Result|Result|\n",
      "+------------------+------+------+\n",
      "|OneVsRest         |34.5  |52.3  |\n",
      "|LogisticRegression|23.8  |52.3  |\n",
      "|LinearSVC         |76.3  |52.3  |\n",
      "+------------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#problem solving\n",
    "from pyspark.ml.classification import *\n",
    "\n",
    "classifiers = [\n",
    "                LogisticRegression()\n",
    "               ,OneVsRest()\n",
    "               ,LinearSVC()\n",
    "#                ,NaiveBayes()\n",
    "#                ,RandomForestClassifier()\n",
    "#                ,GBTClassifier()\n",
    "#                ,DecisionTreeClassifier()\n",
    "#                ,MultilayerPerceptronClassifier()\n",
    "              ] \n",
    "\n",
    "#set up your results2 table\n",
    "columns = ['Classifier', 'Result']\n",
    "vals = [(\"LogisticRegression\",\"23.8\"),('OneVsRest',\"34.5\"),('LinearSVC','76.3')]\n",
    "results = spark.createDataFrame(vals, columns)\n",
    "\n",
    "for y in range(0,3):\n",
    "    #set up your results2 table\n",
    "    columns = ['Classifier', 'Result']\n",
    "    vals = [(\"Place Holder\",\"N/A\")]\n",
    "    results2 = spark.createDataFrame(vals, columns)\n",
    "    for classifier in classifiers:\n",
    "        score = ['52.3']\n",
    "        # Intstantiate Model\n",
    "        M = classifier\n",
    "        # Learn what it is\n",
    "        Mtype = type(M).__name__\n",
    "        Mtype = [Mtype]\n",
    "        new_result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
    "#         print(\"NewResult\")\n",
    "#         new_result.show()\n",
    "        results2 = results2.union(new_result)\n",
    "        print(\"Results2\")\n",
    "        results2.show()\n",
    "    print(\"Join with results\")\n",
    "    print(\"Replicate DF\")\n",
    "results = results.join(results2, [\"Classifier\"],\"left\")\n",
    "print(\"Results3\")\n",
    "results.show()\n",
    "#     results2 = results2.where(\"Classifier!='Place Holder'\")\n",
    "# results = results.join(results2, [\"Classifier\"],\"inner\")\n",
    "\n",
    "results3.show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+\n",
      "|        Classifier|Result|\n",
      "+------------------+------+\n",
      "|      Place Holder|   N/A|\n",
      "|         LinearSVC|  52.3|\n",
      "|         LinearSVC|  52.3|\n",
      "|         LinearSVC|  52.3|\n",
      "|LogisticRegression|  52.3|\n",
      "|         OneVsRest|  52.3|\n",
      "|         LinearSVC|  52.3|\n",
      "+------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers:\n",
    "    score = ['52.3']\n",
    "    # Intstantiate Model\n",
    "    M = classifier\n",
    "    # Learn what it is\n",
    "    Mtype = type(M).__name__\n",
    "    Mtype = [Mtype]\n",
    "    new_result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
    "#     print(\"Results2\")\n",
    "#     new_result.show()\n",
    "    results2 = results2.union(new_result)\n",
    "results2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "selector = ChiSqSelector(numTopFeatures=5, outputCol=\"selectedFeatures\")\n",
    "model = selector.fit(final_data)\n",
    "print(model.selectedFeatures)\n",
    "test = model.transform(final_data)\n",
    "test = test.select(\"label\",\"selectedFeatures\")\n",
    "test2 = test.withColumnRenamed(\"selectedFeatures\",\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,[0],[1.0])\n"
     ]
    }
   ],
   "source": [
    "#Feature importances for feature selection in regression\n",
    "# To be used for all decision tree analysis\n",
    "import numpy\n",
    "from numpy import allclose\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# PREPARE DATA\n",
    "df = spark.createDataFrame([\n",
    "    (1.1, Vectors.dense([1.0, 0.1, -1.0]),),\n",
    "    (2.4, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (5.7, Vectors.dense([3.0, 10.1, 3.0]),)\n",
    "], [\"label\", \"features\"])\n",
    "\n",
    "#Evaluator\n",
    "revaluator = RegressionEvaluator(metricName=\"rmse\")\n",
    "\n",
    "#regressor\n",
    "regressor = GBTRegressor()\n",
    "\n",
    "#Now train with cross val\n",
    "paramGrid = (ParamGridBuilder() \\\n",
    "               .addGrid(regressor.maxDepth, [2, 5, 10])\n",
    "               .addGrid(regressor.maxBins, [5, 10, 20])\n",
    "             .build())\n",
    "\n",
    "#Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(estimator=regressor,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=revaluator,\n",
    "                          numFolds=2) # 3 is best practice\n",
    "\n",
    "# Run cross validations\n",
    "fitModel = crossval.fit(df)\n",
    "\n",
    "# Get Best Model\n",
    "ModelSummary = fitModel.bestModel\n",
    "\n",
    "# FEATURE IMPORTANCES\n",
    "# Estimate of the importance of each feature.\n",
    "# Each featureâ€™s importance is the average of its importance across all trees \n",
    "# in the ensemble The importance vector is normalized to sum to 1. \n",
    "print(ModelSummary.featureImportances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,[0,2],[0.6936974789915966,0.3063025210084034])\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from numpy import allclose\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# PREPARE DATA\n",
    "df = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([1.0, 0.1, -1.0]),),\n",
    "    (2.0, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (3.0, Vectors.dense([3.0, 10.1, 3.0]),)\n",
    "], [\"label\", \"features\"])\n",
    "\n",
    "# BUILD THE MODEL\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "#Now train with cross val\n",
    "paramGrid = (ParamGridBuilder() \\\n",
    "               .addGrid(regressor.maxDepth, [2, 5, 10])\n",
    "               .addGrid(regressor.maxBins, [5, 10, 20])\n",
    "             .build())\n",
    "\n",
    "#Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=2) # 3 is best practice\n",
    "\n",
    "# Fit model using crossval\n",
    "model = crossval.fit(df)\n",
    "\n",
    "# Get Best Model\n",
    "BestModel = model.bestModel\n",
    "\n",
    "# FEATURE IMPORTANCES\n",
    "print(BestModel.featureImportances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp_array = BestModel.featureImportances.toArray()\n",
    "best_n_features = feature_imp_array.argsort()[-1:][::-1]\n",
    "best_n_features= best_n_features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside function\n",
      "inside function\n"
     ]
    }
   ],
   "source": [
    "Mtype = \"Regresson\"\n",
    "def f():\n",
    "    global Mtype_featureimp\n",
    "    Mtype_featureimp = \"inside function\"\n",
    "    print(Mtype_featureimp)\n",
    "    \n",
    "f()\n",
    "\n",
    "print(Mtype_featureimp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Load training data\n",
    "df = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([1.0, 0.1, -1.0]),),\n",
    "    (2.0, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (4.0, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (3.0, Vectors.dense([3.0, 10.1, 3.0]),)\n",
    "], [\"label\", \"features\"])\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      "DenseMatrix([[ -7.41554638,  -0.31549257,  -0.78537276],\n",
      "             [ -0.37571401,  -0.78437003, -10.87159487],\n",
      "             [  5.46619968,  -1.07892466,   2.85277696],\n",
      "             [ -3.13863114,   3.25191417,   5.9536392 ],\n",
      "             [  5.46369185,  -1.0731269 ,   2.85055146]])\n",
      "Intercept: [-3.152207020523129,9.635285314580297,0.03224977160900043,-6.548441471338726,0.033113405672556884]\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for multinomial logistic regression\n",
    "print(\"Coefficients: \\n\" + str(lrModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = lrModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -7.41554638,  -0.31549257,  -0.78537276],\n",
       "       [ -0.37571401,  -0.78437003, -10.87159487],\n",
       "       [  5.46619968,  -1.07892466,   2.85277696],\n",
       "       [ -3.13863114,   3.25191417,   5.9536392 ],\n",
       "       [  5.46369185,  -1.0731269 ,   2.85055146]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel.coefficientMatrix.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import *\n",
    "\n",
    "# Load training data\n",
    "df = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([1.0, 0.1, -1.0]),),\n",
    "    (2.0, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (4.0, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (3.0, Vectors.dense([3.0, 10.1, 3.0]),)\n",
    "], [\"label\", \"features\"])\n",
    "\n",
    "# instantiate the base classifier.\n",
    "lr = LogisticRegression()\n",
    "# instantiate the One Vs Rest Classifier.\n",
    "OVRclassifier = OneVsRest(classifier=lr)\n",
    "# Add parameters of your choice here:\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "#Cross Validator requires the following parameters:\n",
    "crossval = CrossValidator(estimator=OVRclassifier,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=2) # 3 is best practice\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "fitModel = crossval.fit(df)\n",
    "\n",
    "# Get Best Model\n",
    "BestModel = fitModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = BestModel.models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogisticRegressionModel: uid = LogisticRegression_84191fe3c3a4, numClasses = 1, numFeatures = 3,\n",
       " LogisticRegressionModel: uid = LogisticRegression_84191fe3c3a4, numClasses = 2, numFeatures = 3,\n",
       " LogisticRegressionModel: uid = LogisticRegression_84191fe3c3a4, numClasses = 2, numFeatures = 3,\n",
       " LogisticRegressionModel: uid = LogisticRegression_84191fe3c3a4, numClasses = 2, numFeatures = 3,\n",
       " LogisticRegressionModel: uid = LogisticRegression_84191fe3c3a4, numClasses = 2, numFeatures = 3]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestModel.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.LogisticRegressionModel"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0,0.0,0.0]\n"
     ]
    }
   ],
   "source": [
    "print(model1.coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept:  -inf \n",
      "Coefficients: [0.0,0.0,0.0]\n",
      "Intercept:  1.38998477082603 \n",
      "Coefficients: [-1.2212950975133143,-0.04629705307010703,-0.6106473460416055]\n",
      "Intercept:  -1.3090562599004436 \n",
      "Coefficients: [0.25037290796442024,-0.14245373637849076,0.12518660440773915]\n",
      "Intercept:  -4.570404108638901 \n",
      "Coefficients: [0.8352178836198739,0.21341794704688208,0.41760894947769095]\n",
      "Intercept:  -1.3090562599004434 \n",
      "Coefficients: [0.25037290796442,-0.14245373637849076,0.1251866044077393]\n"
     ]
    }
   ],
   "source": [
    "# Get Best Model\n",
    "# BestModel = fitModel.bestModel\n",
    "\n",
    "models = BestModel.models\n",
    "for model in models:\n",
    "    print('Intercept: ',model.intercept,'\\nCoefficients:',model.coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegressionModel' object has no attribute 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-ee4fee7344ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'LogisticRegressionModel' object has no attribute 'models'"
     ]
    }
   ],
   "source": [
    "[x.intercept for x in model.models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-f52002a0d8d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mBestModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/ml/classification.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m   1964\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1965\u001b[0m         \u001b[0;31m# determine the input columns: these need to be passed through\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1966\u001b[0;31m         \u001b[0morigCols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;31m# add an accumulator column to store predictions of all the models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/ml/linalg/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__neg__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "BestModel.transform(Vectors.dense([1.0, 0.1, -1.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databricks help session\n",
    "\n",
    "*Feb. 6, 2020*\n",
    "\n",
    "#### Can the DFPrep function distribute?\n",
    "Yes, according to Spark UI\n",
    "\n",
    "#### Can any function that is not a UDF distribute?\n",
    "Yes, but check the Spark UI to make sure\n",
    "\n",
    "#### Can For Loops distribute?\n",
    "SAA\n",
    "\n",
    "#### Do DB have any other sessions learned about pythonic things that won't distrubte in PySpark?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized:\n",
      "+-----+----------------------+----------------------------+\n",
      "|label|sentence              |words                       |\n",
      "+-----+----------------------+----------------------------+\n",
      "|0.0  |Hi I heard about Spark|[hi, i, heard, about, spark]|\n",
      "+-----+----------------------+----------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "None\n",
      "HashingTF\n",
      "+-----+----------------------+----------------------------+---------------------------------+\n",
      "|label|sentence              |words                       |rawFeatures                      |\n",
      "+-----+----------------------+----------------------------+---------------------------------+\n",
      "|0.0  |Hi I heard about Spark|[hi, i, heard, about, spark]|(20,[0,5,9,17],[1.0,1.0,1.0,2.0])|\n",
      "+-----+----------------------+----------------------------+---------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "None\n",
      "IDF\n",
      "+-----+----------------------+----------------------------+---------------------------------+----------------------------------------------------------------------------------------------+\n",
      "|label|sentence              |words                       |rawFeatures                      |features                                                                                      |\n",
      "+-----+----------------------+----------------------------+---------------------------------+----------------------------------------------------------------------------------------------+\n",
      "|0.0  |Hi I heard about Spark|[hi, i, heard, about, spark]|(20,[0,5,9,17],[1.0,1.0,1.0,2.0])|(20,[0,5,9,17],[0.6931471805599453,0.6931471805599453,0.28768207245178085,1.3862943611198906])|\n",
      "+-----+----------------------+----------------------------+---------------------------------+----------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "print(\"Tokenized:\")\n",
    "print(wordsData.show(1,False))\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "print(\"HashingTF\")\n",
    "print(featurizedData.show(1,False))\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "print(\"IDF\")\n",
    "print(rescaledData.show(1,False))\n",
    "\n",
    "# rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [0.04547937586903572,-0.0025138825178146365,0.0006965592503547669]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [-0.0015820334000246864,-0.001963341608643532,0.009649708451304051]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [-0.04485992938280106,-0.04581879526376725,0.0262237292714417]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documentDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+\n",
      "|text                                      |\n",
      "+------------------------------------------+\n",
      "|[Hi, I, heard, about, Spark]              |\n",
      "|[I, wish, Java, could, use, case, classes]|\n",
      "|[Logistic, regression, models, are, neat] |\n",
      "+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documentDF.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+\n",
      "|ngrams                                                            |\n",
      "+------------------------------------------------------------------+\n",
      "|[Hi I, I heard, heard about, about Spark]                         |\n",
      "|[I wish, wish Java, Java could, could use, use case, case classes]|\n",
      "|[Logistic regression, regression models, models are, are neat]    |\n",
      "+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "wordDataFrame = spark.createDataFrame([\n",
    "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n",
    "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n",
    "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "\n",
    "ngramDataFrame = ngram.transform(wordDataFrame)\n",
    "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whatevs\n"
     ]
    }
   ],
   "source": [
    "wordDataFrame.name = 'Whatevs'\n",
    "\n",
    "print(wordDataFrame.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this to create an array... kind of pivot\n",
    "from pyspark.sql.functions import collect_set\n",
    "transactions = df.groupBy('seller_id').agg(collect_set('product_id').alias('items'))\n",
    "transactions.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+\n",
      "|   sequence|freq|\n",
      "+-----------+----+\n",
      "|      [[3]]|   2|\n",
      "|      [[2]]|   3|\n",
      "|      [[5]]|   3|\n",
      "|      [[1]]|   3|\n",
      "|   [[2, 5]]|   2|\n",
      "|   [[1, 5]]|   2|\n",
      "|   [[1, 2]]|   3|\n",
      "|[[1, 2, 5]]|   2|\n",
      "+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.fpm import PrefixSpan\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, [[1, 2, 5,1],[3]]),\n",
    "    (1, [[1, 2, 3, 5],[4]]),\n",
    "    (2, [[1, 2],[5]])\n",
    "], [\"id\", \"sequence\"])\n",
    "\n",
    "prefixSpan = PrefixSpan(minSupport=0.5, maxPatternLength=5,\n",
    "                        maxLocalProjDBSize=32000000)\n",
    "\n",
    "# Find frequent sequential patterns.\n",
    "prefixSpan.findFrequentSequentialPatterns(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (0, [1, 2, 5]),\n",
    "    (1, [1, 2, 3, 5]),\n",
    "    (2, [1, 2])\n",
    "], [\"id\", \"items\"])\n",
    "\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.2, minConfidence=0.1)\n",
    "model = fpGrowth.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+------+\n",
      "|sentence                           |words                                     |tokens|\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|sentence                           |words                                     |tokens|\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What data type does a tokenizer return?\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I wish Java could use case classes\"),\n",
    "    (2, \"Logistic,regression,models,are,neat\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False)\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.select(\"sentence\", \"words\")\\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n",
    "\n",
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "regexTokenized.select(\"sentence\", \"words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- sentence: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regexTokenized.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-------------+-------------+\n",
      "|categoryIndex1|categoryIndex2| categoryVec1| categoryVec2|\n",
      "+--------------+--------------+-------------+-------------+\n",
      "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|           1.0|           0.0|(2,[1],[1.0])|(2,[0],[1.0])|\n",
      "|           2.0|           1.0|    (2,[],[])|(2,[1],[1.0])|\n",
      "|           0.0|           2.0|(2,[0],[1.0])|    (2,[],[])|\n",
      "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|           2.0|           0.0|    (2,[],[])|(2,[0],[1.0])|\n",
      "+--------------+--------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0.0, 1.0),\n",
    "    (1.0, 0.0),\n",
    "    (2.0, 1.0),\n",
    "    (0.0, 2.0),\n",
    "    (0.0, 1.0),\n",
    "    (2.0, 0.0)\n",
    "], [\"categoryIndex1\", \"categoryIndex2\"])\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=[\"categoryIndex1\", \"categoryIndex2\"],\n",
    "                                 outputCols=[\"categoryVec1\", \"categoryVec2\"])\n",
    "model = encoder.fit(df)\n",
    "encoded = model.transform(df)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
