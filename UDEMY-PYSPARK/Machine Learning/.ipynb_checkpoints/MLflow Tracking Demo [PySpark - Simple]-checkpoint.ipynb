{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow Tracking Example\n",
    "\n",
    "MLflow is organized into four components: **Tracking**, **Projects**, **Models**, and **Model Registry**. You can use each of these components on their own—for example, maybe you want to export models in MLflow’s model format without using Tracking or Projects—but they are also designed to work well together. So this notebook will focus on only the **Tracking** component within the PySpark environment. \n",
    "\n",
    "### Why is tracking useful/important?\n",
    "\n",
    "Machine learning typically requires experimenting with a diverse set of hyperparameter tuning techniques, data preparation steps, and algorithms to build a model that maximizes some target metric. Given this complexity, building a machine learning model can therefore be challenging for a couple of reasons:\n",
    "\n",
    "1. **It’s difficult to keep track of experiments.** When you are just working with files on your laptop, or with an interactive notebook, how do you tell which data, code and parameters went into getting a particular result?\n",
    "2. **It’s difficult to reproduce code.** Even if you have meticulously tracked the code versions and parameters, you need to capture the whole environment (for example, library dependencies) to get the same result again. This is especially challenging if you want another data scientist to use your code, or if you want to run the same code at scale on another platform (for example, in the cloud).\n",
    "\n",
    "### Solution that MLflow Tracking provides\n",
    "\n",
    "MLflow Tracking is an API and UI for logging parameters, code versions, metrics, and artifacts when running your machine learning code and for later visualizing the results. You can use MLflow Tracking in any environment (for example, a standalone script or a notebook) to log results to local files or to a server, then compare multiple runs.\n",
    "\n",
    "### How to install MLflow\n",
    "\n",
    "You simply install MLflow by running *\"pip install mlflow\"* via the command line. Please reference the Quick Start Guide here for more details: https://mlflow.org/docs/latest/quickstart.html\n",
    "\n",
    "### Viewing the Tracking MLflow UI\n",
    "\n",
    "By default, wherever you run your program (Jupyter Notebook in this case), the tracking API writes data into files into a local ./mlruns directory. First you need to open your mlflow intance via the command line (cd into the folder where this notebook is stored). You can then run MLflow’s Tracking UI: http://localhost:5000/#/\n",
    "\n",
    "### How to cd into a folder\n",
    "\n",
    " - **Mac**: https://macpaw.com/how-to/use-terminal-on-mac\n",
    " - **Windows**: https://www.minitool.com/news/how-to-change-directory-in-cmd.html\n",
    " \n",
    "### Getting started\n",
    "\n",
    "Since we using PySpark for this example, first we need to create a Spark Session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://orcuns-mbp-2:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Mlflow</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe29524e7d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Mlflow\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark modeling libraries\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Mlflow libaries\n",
    "import mlflow\n",
    "\n",
    "# Mlflow client\n",
    "from  mlflow.tracking import MlflowClient\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a fake dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([3.0, 10.1, 3.0]),)\n",
    "], [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets. (0.75, 0.25) split.\n",
    "train_val = 0.7\n",
    "test_val = 1-train_val\n",
    "seed=40\n",
    "train,test = df.randomSplit([train_val,test_val],seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Experiment: artifact_location='file:///Users/orcuncanlilar/Eden/PySpark%20Essentials%20for%20Data%20Scientists/Jupyter%20Notebooks/Machine%20Learning/mlruns/0', experiment_id='0', lifecycle_stage='active', name='Default', tags={}>\n",
      " \n",
      "<Experiment: artifact_location='file:///Users/orcuncanlilar/Eden/PySpark%20Essentials%20for%20Data%20Scientists/Jupyter%20Notebooks/Machine%20Learning/mlruns/7', experiment_id='7', lifecycle_stage='active', name='Tree-Algorithms', tags={}>\n",
      " \n",
      "<Experiment: artifact_location='file:///Users/orcuncanlilar/Eden/PySpark%20Essentials%20for%20Data%20Scientists/Jupyter%20Notebooks/Machine%20Learning/mlruns/6', experiment_id='6', lifecycle_stage='active', name='Experiment-4', tags={}>\n",
      " \n",
      "<Experiment: artifact_location='file:///Users/orcuncanlilar/Eden/PySpark%20Essentials%20for%20Data%20Scientists/Jupyter%20Notebooks/Machine%20Learning/mlruns/1', experiment_id='1', lifecycle_stage='active', name='first-experiment', tags={'mlflow.note.content': 'This experiment tested various tree classifiers '\n",
      "                        'across various parameters. The training process used '\n",
      "                        'a parameter grid search technique for Hyperparameter '\n",
      "                        'optimization. It was a real success!'}>\n",
      " \n",
      "<Experiment: artifact_location='file:///Users/orcuncanlilar/Eden/PySpark%20Essentials%20for%20Data%20Scientists/Jupyter%20Notebooks/Machine%20Learning/mlruns/4', experiment_id='4', lifecycle_stage='active', name='Experiment-2', tags={}>\n",
      " \n",
      "<Experiment: artifact_location='file:///Users/orcuncanlilar/Eden/PySpark%20Essentials%20for%20Data%20Scientists/Jupyter%20Notebooks/Machine%20Learning/mlruns/3', experiment_id='3', lifecycle_stage='active', name='Experiment-1', tags={}>\n",
      " \n",
      "<Experiment: artifact_location='file:///Users/orcuncanlilar/Eden/PySpark%20Essentials%20for%20Data%20Scientists/Jupyter%20Notebooks/Machine%20Learning/mlruns/2', experiment_id='2', lifecycle_stage='active', name='Experiment-0', tags={}>\n",
      " \n",
      "<Experiment: artifact_location='file:///Users/orcuncanlilar/Eden/PySpark%20Essentials%20for%20Data%20Scientists/Jupyter%20Notebooks/Machine%20Learning/mlruns/5', experiment_id='5', lifecycle_stage='active', name='Experiment-3', tags={}>\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Get list of experiments and print them\n",
    "experiments = client.list_experiments() # returns a list of mlflow.entities.Experiment\n",
    "for x in experiments:\n",
    "#     print(x.name)\n",
    "    print(x)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing Experiments and Runs with the Tracking Service API\n",
    "\n",
    "https://mlflow.org/docs/latest/tracking.html#managing-experiments-and-runs-with-the-tracking-service-api\n",
    "\n",
    "### Organizing Runs in Experiments\n",
    "\n",
    "https://mlflow.org/docs/latest/tracking.html#organizing-runs-in-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Description:  <Experiment: artifact_location='file:///Users/orcuncanlilar/Eden/PySpark%20Essentials%20for%20Data%20Scientists/Jupyter%20Notebooks/Machine%20Learning/mlruns/1', experiment_id='1', lifecycle_stage='active', name='first-experiment', tags={'mlflow.note.content': 'This experiment tested various tree classifiers '\n",
      "                        'across various parameters. The training process used '\n",
      "                        'a parameter grid search technique for Hyperparameter '\n",
      "                        'optimization. It was a real success!'}>\n",
      " \n",
      "Name:  first-experiment\n",
      "ID:  1\n",
      "Tags:  {'mlflow.note.content': 'This experiment tested various tree classifiers across various parameters. The training process used a parameter grid search technique for Hyperparameter optimization. It was a real success!'}\n"
     ]
    }
   ],
   "source": [
    "# You can retrieve any of the elements from experiements that you need....\n",
    "print(\"Full Description: \",experiments[2])\n",
    "print(\" \")\n",
    "print(\"Name: \",experiments[2].name)\n",
    "print(\"ID: \",experiments[2].experiment_id)\n",
    "print(\"Tags: \",experiments[2].tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set and experiment name and tie it to a create run function\n",
    "experimentName = \"Tree-Algorithms\"\n",
    "def create_run(experimentName):\n",
    "    mlflow.set_experiment(experiment_name = experimentName)\n",
    "    for x in experiments:\n",
    "        if experimentName in x.name:\n",
    "            experiment_index = experiments.index(x)\n",
    "            run = client.create_run(experiments[experiment_index].experiment_id) # returns mlflow.entities.Run\n",
    "            return run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train and Evaluate your model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Classifier\n",
    "classifier = DecisionTreeClassifier(maxDepth=5, maxBins=32)\n",
    "fitModel = classifier.fit(train)\n",
    "\n",
    "# Evaluate\n",
    "predictions = fitModel.transform(test)\n",
    "\n",
    "# Calculate Accuracy\n",
    "MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "accuracy = (MC_evaluator.evaluate(predictions))*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Log results to MLflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paramMap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-3ebdd35c7ad0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Log parameters to the run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparamMap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'maxDepth'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Max Depth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'paramMap' is not defined"
     ]
    }
   ],
   "source": [
    "run = create_run(experiment_name)\n",
    "\n",
    "# Add tags to the run\n",
    "classifier_name = type(classifier).__name__ # Extract the name of the classifier\n",
    "client.set_tag(run.info.run_id, \"Algorithm\", classifier_name)  \n",
    "client.set_tag(run.info.run_id,\"Random Seed\",seed)\n",
    "client.set_tag(run.info.run_id,\"Train Perct\",train_val)\n",
    "\n",
    "# Log metrics to the run\n",
    "client.log_metric(run.info.run_id, \"accuracy\", accuracy)\n",
    "\n",
    "# Log parameters to the run\n",
    "for key, val in paramMap.items():\n",
    "    if 'maxDepth' in key.name:\n",
    "        client.log_param(run.info.run_id, \"Max Depth\", val)\n",
    "for key, val in paramMap.items():\n",
    "    if 'maxBins' in key.name:\n",
    "        client.log_param(run.info.run_id, \"Max Bins\", val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(classifier_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:  DecisionTreeClassifier_c86026698e30__cacheNodeIds :  False\n",
      "Key:  DecisionTreeClassifier_c86026698e30__checkpointInterval :  10\n",
      "Key:  DecisionTreeClassifier_c86026698e30__featuresCol :  features\n",
      "Key:  DecisionTreeClassifier_c86026698e30__impurity :  gini\n",
      "Key:  DecisionTreeClassifier_c86026698e30__labelCol :  label\n",
      "Key:  DecisionTreeClassifier_c86026698e30__maxBins :  32\n",
      "Key:  DecisionTreeClassifier_c86026698e30__maxDepth :  5\n",
      "Key:  DecisionTreeClassifier_c86026698e30__maxMemoryInMB :  256\n",
      "Key:  DecisionTreeClassifier_c86026698e30__minInfoGain :  0.0\n",
      "Key:  DecisionTreeClassifier_c86026698e30__minInstancesPerNode :  1\n",
      "Key:  DecisionTreeClassifier_c86026698e30__predictionCol :  prediction\n",
      "Key:  DecisionTreeClassifier_c86026698e30__probabilityCol :  probability\n",
      "Key:  DecisionTreeClassifier_c86026698e30__rawPredictionCol :  rawPrediction\n",
      "Key:  DecisionTreeClassifier_c86026698e30__seed :  -8543656710325985320\n"
     ]
    }
   ],
   "source": [
    "### See all params available to you\n",
    "paramMap = fitModel.extractParamMap()\n",
    "for k, v in paramMap.items():\n",
    "    print(\"Key: \",k, \": \", v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
