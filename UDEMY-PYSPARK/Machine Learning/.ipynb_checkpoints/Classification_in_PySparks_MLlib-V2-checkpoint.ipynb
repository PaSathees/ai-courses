{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification in PySpark's MLlib\n",
    "\n",
    "PySpark offers a good variety of algorithms that can be applied to classification machine learning problems. However, because PySpark operates on distributed dataframes, we cannot use popular Python libraries like scikit learn for our machine learning applications. Which means we need to use PySpark's MLlib packages for these tasks. Luckily, MLlib offers a pretty good variety of algorithms! In this notebook we will go over how to prep our data, how to train and test the classification algorithms PySpark offers, and also some pretty neat functions that I created to make the model selection process really efficient. \n",
    "\n",
    "Cover basic structure, training and validation split, model selection, pipelines, Cross validation\n",
    "\n",
    "## Defining Classification\n",
    "\n",
    "In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known (ie. supervised learning). You can think of classification like bucketing. \n",
    "\n",
    "You should use classification if your **dependent variable** (the variable you want to predict) is a **descrete value** (i.e. whole numbers like 1,2,3,4) typically the dependent variable will represent some class or group like types of flowers or colors, as opposed to a continuous variable like dollar amount, age or blood pressure.\n",
    "\n",
    "## Algorithms Available\n",
    "\n",
    "PySpark offers the following algorithms for classification. \n",
    "\n",
    "1. Logistic Regression \n",
    "2. Naive Bayes\n",
    "3. One Vs Rest\n",
    "4. Linear Support Vector Machine (SVC)\n",
    "5. Random Forest Classifier\n",
    "6. GBT Classifier\n",
    "7. Decision Tree Classifier\n",
    "8. Multilayer Perceptron Classifier (Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working with 1 core(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://BAH5CG4421TCH.resource.ds.bah.com:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Review2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1c03c881fd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's create our PySpark instance\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# May take awhile locally\n",
    "spark = SparkSession.builder.appName(\"Review2\").getOrCreate()\n",
    "\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "print(\"You are working with\", cores, \"core(s)\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's read our dataset in for this notebook \n",
    "\n",
    "### Data Set Name: Autistic Spectrum Disorder Screening Data for Adult\n",
    "Autistic Spectrum Disorder (ASD) is a neurodevelopment condition associated with significant healthcare costs, and early diagnosis can significantly reduce these. Unfortunately, waiting times for an ASD diagnosis are lengthy and procedures are not cost effective. The economic impact of autism and the increase in the number of ASD cases across the world reveals an urgent need for the development of easily implemented and effective screening methods. Therefore, a time-efficient and accessible ASD screening is imminent to help health professionals and inform individuals whether they should pursue formal clinical diagnosis. The rapid growth in the number of ASD cases worldwide necessitates datasets related to behaviour traits. However, such datasets are rare making it difficult to perform thorough analyses to improve the efficiency, sensitivity, specificity and predictive accuracy of the ASD screening process. Presently, very limited autism datasets associated with clinical or screening are available and most of them are genetic in nature. Hence, we propose a new dataset related to autism screening of adults that contained 20 features to be utilised for further analysis especially in determining influential autistic traits and improving the classification of ASD cases. In this dataset, we record ten behavioural features (AQ-10-Adult) plus ten individuals characteristics that have proved to be effective in detecting the ASD cases from controls in behaviour science.\n",
    "\n",
    "### Source: \n",
    "https://www.kaggle.com/faizunnabi/autism-screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"autism-screening-for-toddlers/\"\n",
    "df = spark.read.csv(path+'Toddler Autism dataset July 2018.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case_No</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>Age_Mons</th>\n",
       "      <th>Qchat-10-Score</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Jaundice</th>\n",
       "      <th>Family_mem_with_ASD</th>\n",
       "      <th>Who completed the test</th>\n",
       "      <th>Class/ASD Traits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>f</td>\n",
       "      <td>middle eastern</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>m</td>\n",
       "      <td>White European</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>m</td>\n",
       "      <td>middle eastern</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>m</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>f</td>\n",
       "      <td>White European</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>m</td>\n",
       "      <td>black</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case_No  A1  A2  A3  A4  A5  A6  A7  A8  A9  A10  Age_Mons  Qchat-10-Score  \\\n",
       "0        1   0   0   0   0   0   0   1   1   0    1        28               3   \n",
       "1        2   1   1   0   0   0   1   1   0   0    0        36               4   \n",
       "2        3   1   0   0   0   0   0   1   1   0    1        36               4   \n",
       "3        4   1   1   1   1   1   1   1   1   1    1        24              10   \n",
       "4        5   1   1   0   1   1   1   1   1   1    1        20               9   \n",
       "5        6   1   1   0   0   1   1   1   1   1    1        21               8   \n",
       "\n",
       "  Sex       Ethnicity Jaundice Family_mem_with_ASD Who completed the test  \\\n",
       "0   f  middle eastern      yes                  no          family member   \n",
       "1   m  White European      yes                  no          family member   \n",
       "2   m  middle eastern      yes                  no          family member   \n",
       "3   m        Hispanic       no                  no          family member   \n",
       "4   f  White European       no                 yes          family member   \n",
       "5   m           black       no                  no          family member   \n",
       "\n",
       "  Class/ASD Traits   \n",
       "0                No  \n",
       "1               Yes  \n",
       "2               Yes  \n",
       "3               Yes  \n",
       "4               Yes  \n",
       "5               Yes  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Case_No: integer (nullable = true)\n",
      " |-- A1: integer (nullable = true)\n",
      " |-- A2: integer (nullable = true)\n",
      " |-- A3: integer (nullable = true)\n",
      " |-- A4: integer (nullable = true)\n",
      " |-- A5: integer (nullable = true)\n",
      " |-- A6: integer (nullable = true)\n",
      " |-- A7: integer (nullable = true)\n",
      " |-- A8: integer (nullable = true)\n",
      " |-- A9: integer (nullable = true)\n",
      " |-- A10: integer (nullable = true)\n",
      " |-- Age_Mons: integer (nullable = true)\n",
      " |-- Qchat-10-Score: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Ethnicity: string (nullable = true)\n",
      " |-- Jaundice: string (nullable = true)\n",
      " |-- Family_mem_with_ASD: string (nullable = true)\n",
      " |-- Who completed the test: string (nullable = true)\n",
      " |-- Class/ASD Traits : string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many classes do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|Class/ASD Traits |count|\n",
      "+-----------------+-----+\n",
      "|               No|  326|\n",
      "|              Yes|  728|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Class/ASD Traits \").count().show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Data \n",
    "\n",
    "MLlib requires all input columns of your dataframe to be vectorized. You will see that we rename our dependent var to label as that is what is expected for all MLlib applications. If rename once here, we never have to do it again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and create a function to do all of this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Prep function\n",
    "def MLClassifierDFPrep(df,input_columns,dependent_var):\n",
    "    \n",
    "    # change label (class variable) to string type to prep for reindexing\n",
    "    # Pyspark is expecting a zero indexed integer for the label column. \n",
    "    # Just incase our data is not in that format... we will treat it by using the StringIndexer built in method\n",
    "    renamed = df.withColumn(\"label_str\", df[dependent_var].cast(StringType())) #Rename and change to string type\n",
    "    indexer = StringIndexer(inputCol=\"label_str\", outputCol=\"label\") #Pyspark is expecting the this naming convention \n",
    "    indexed = indexer.fit(renamed).transform(renamed)\n",
    "\n",
    "    # Convert all string type data in the input column list to numeric\n",
    "    # Otherwise the Algorithm will not be able to process it\n",
    "    numeric_inputs = []\n",
    "    string_inputs = []\n",
    "    for column in input_columns:\n",
    "        if str(indexed.schema[column].dataType) == 'StringType':\n",
    "            indexer = StringIndexer(inputCol=column, outputCol=column+\"_num\") \n",
    "            indexed = indexer.fit(indexed).transform(indexed)\n",
    "            new_col_name = column+\"_num\"\n",
    "            string_inputs.append(new_col_name)\n",
    "        else:\n",
    "            numeric_inputs.append(column)\n",
    "            \n",
    "    # Check globally for non normal data (using skewness and kurtosis)   \n",
    "    skew = df.select([f.skewness(c).alias(c) for c in df.columns if c in numeric_inputs]) # Calculate the skewness for all columns in the df\n",
    "    skew_array = skew.select(array(numeric_inputs).alias(\"skew\")) #create and array with results\n",
    "    #get the global min\n",
    "    skew_minimum = skew_array.select(array_min(skew_array.skew)).collect() # Collect golobal min as Python object\n",
    "    skew_minimum = skew_minimum[0][0] # Slice to get the number itself\n",
    "    #get the global max\n",
    "    skew_max = skew_array.select(array_max(skew_array.skew)).collect() # Collect golobal min as Python object\n",
    "    skew_max = skew_max[0][0] # Slice to get the number itself\n",
    "            \n",
    "    if skew_max >1 or skew_min < -1:\n",
    "        # Ask user if they want to floor and cap their data hollistically then act accordingly\n",
    "        # Let's use approxQuantile, otherwise it'll take FOREVER\n",
    "        # approxQuantile(cols: Array[String], probabilities: Array[Double], relativeError: Double): Array[Array[Double]] Permalink\n",
    "        # Calculates the approximate quantiles of numerical columns of a DataFrame.\n",
    "        print(\"Looks like your dataframe contains some non normal data which may contain outliers.\")\n",
    "        print(\"Would you like us to treat for that using flooring, capping and log(x+1) or e^(x+1)\")\n",
    "        print(\"This may help improve model accuracy.\")\n",
    "        floor_cap = str(input(\"Enter 1 for yes and 0 for no: \"))\n",
    "        if floor_cap == \"1\":\n",
    "            print(\"Okay, we are correcting for non normality now!\")\n",
    "            # empty dictionary d\n",
    "            d = {}\n",
    "            # Create a dictionary of quantiles\n",
    "            for col in numeric_inputs: \n",
    "                d[col] = indexed.approxQuantile(col,[0.01,0.99],0.25) #if you want to make it go faster increase the last number\n",
    "            #Now fill in the values\n",
    "            for col in numeric_inputs:\n",
    "                skew = indexed.agg(f.skewness(indexed[col])).collect() #check for skewness\n",
    "                skew = skew[0][0]\n",
    "                # This function will floor, cap and then log+1 (just in case there are 0 values)\n",
    "                if skew > 1:\n",
    "                    indexed = indexed.withColumn(col, \\\n",
    "                    F.log(F.when(df[col] < d[col][0],d[col][0])\\\n",
    "                    .when(indexed[col] > d[col][1], d[col][1])\\\n",
    "                    .otherwise(indexed[col] ) +1).alias(col))\n",
    "                    print(col+\" has been treated for positive (right) skewness. (skew =)\",skew,\")\")\n",
    "                elif skew < -1:\n",
    "                    indexed = indexed.withColumn(col, \\\n",
    "                    F.exp(F.when(df[col] < d[col][0],d[col][0])\\\n",
    "                    .when(indexed[col] > d[col][1], d[col][1])\\\n",
    "                    .otherwise(indexed[col] )).alias(col))\n",
    "                    print(col+\" has been treated for negative (left) skewness. (skew =\",skew,\")\")\n",
    "        elif floor_cap == \"0\":\n",
    "            print(\"Okay\") \n",
    "            print(\"We will return the dataframe without flooring and capping.\")\n",
    "        else:\n",
    "            print(\"You have entered an invalid response.\")\n",
    "            print(\"We will return the data without flooring and capping\")\n",
    "\n",
    "            \n",
    "    # Produce a warning if there are negative values in the dataframe that Naive Bayes cannot be used. \n",
    "    # Note: we only need to check the numeric input values since anything that is indexed won't have negative values\n",
    "    minimums = df.select([min(c).alias(c) for c in df.columns if c in numeric_inputs]) # Calculate the mins for all columns in the df\n",
    "    min_array = minimums.select(array(numeric_inputs).alias(\"mins\")) # Create an array for all mins and select only the input cols\n",
    "    df_minimum = min_array.select(array_min(min_array.mins)).collect() # Collect golobal min as Python object\n",
    "    df_minimum = df_minimum[0][0] # Slice to get the number itself\n",
    "\n",
    "    features_list = numeric_inputs + string_inputs\n",
    "    assembler = VectorAssembler(inputCols=features_list,outputCol='features')\n",
    "    output = assembler.transform(indexed).select('features','label')\n",
    "\n",
    "#     final_data = output.select('features','label') #drop everything else\n",
    "    \n",
    "    # Now check for negative values and ask user if they want to correct that? \n",
    "    if df_minimum < 0:\n",
    "        print(\"WARNING: The Naive Bayes Classifier will not be able to process your dataframe as it contains negative values\")\n",
    "        print(\"Would you like to correct that by rescaling all your features to a range of 0 to 1?\")\n",
    "        answer = str(input(\"Enter 1 for yes and 0 for no: \"))\n",
    "        print(\" \")\n",
    "    \n",
    "    if answer == \"1\":\n",
    "        print(\"Okay\")  \n",
    "        print(\"We are rescaling you dataframe....\")\n",
    "        scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "        # Compute summary statistics and generate MinMaxScalerModel\n",
    "        scalerModel = scaler.fit(output)\n",
    "\n",
    "        # rescale each feature to range [min, max].\n",
    "        scaled_data = scalerModel.transform(output)\n",
    "        final_data = scaled_data.select('label','scaledFeatures')\n",
    "        final_data = final_data.withColumnRenamed('scaledFeatures','features')\n",
    "        print(\"Done!\")\n",
    "\n",
    "    elif answer == \"0\":\n",
    "        print(\"Okay\") \n",
    "        print(\"We will return the dataframe unscaled.\")\n",
    "        final_data = output\n",
    "    else:\n",
    "        print(\"You have entered an invalid response.\")\n",
    "        print(\"We will return the data unscaled\")\n",
    "        final_data = output\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take it for a test run!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features  label\n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, ...    1.0\n",
       "1  (1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, ...    0.0\n",
       "2  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, ...    0.0\n",
       "3  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...    0.0\n",
       "4  [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...    0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in functions we will need\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "col_list = [\"A1\",\"A2\",\"A3\",\"A4\",\"A5\",\"A6\",\"A7\",\"A8\",\"A9\",\"A10\",\"Age_Mons\",\"Qchat-10-Score\",\"Sex\",\"Ethnicity\",\"Jaundice\",\"Family_mem_with_ASD\",\"Who completed the test\"]\n",
    "\n",
    "# input_columns = df.columns\n",
    "# input_columns = input_columns[8:] # keep only relevant columns: from column 8 until the end\n",
    "input_columns = col_list\n",
    "dependent_var = 'Class/ASD Traits '\n",
    "\n",
    "final_data = MLClassifierDFPrep(df,input_columns,dependent_var)\n",
    "final_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split into Test and Training datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!\n",
    "\n",
    "Let's go ahead and train a Logistic Regression Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import *\n",
    "classifier = LogisticRegression()\n",
    "fitModel = classifier.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test (Evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 1.0\n",
      "Accuracy: 100.00 %\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import *\n",
    "\n",
    "predictionAndLabels = fitModel.evaluate(test)\n",
    "predictionAndLabels = predictionAndLabels.predictions.select('label','prediction')\n",
    "Bin_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction') #labelCol='label'\n",
    "auc = Bin_evaluator.evaluate(predictionAndLabels)\n",
    "print(\"AUC:\",auc)\n",
    "\n",
    "predictions = fitModel.transform(test)\n",
    "MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\") # redictionCol=\"prediction\",\n",
    "accuracy = (MC_evaluator.evaluate(predictions))*100\n",
    "print(\"Accuracy: {0:.2f}\".format(accuracy),\"%\") #     print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great!\n",
    "\n",
    "We did it! But usually you'll want to test several different algorithms and compare their performance as you try to solve machine learning problems. You generally don't just test out only one and move on. So I've created the function below to do just that. Hopefully it make it easier for you! Once you've selected the best model from here, you can fine tune. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create all encompassing Classification Training and Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClassTrainEval(classifier,features,classes):\n",
    "\n",
    "    def FindMtype(classifier):\n",
    "        # Intstantiate Model\n",
    "        M = classifier\n",
    "        # Learn what it is\n",
    "        Mtype = type(M).__name__\n",
    "        \n",
    "        return Mtype\n",
    "    \n",
    "    Mtype = FindMtype(classifier)\n",
    "    \n",
    "\n",
    "    def IntanceFitModel(Mtype,classifier,classes,features):\n",
    "        \n",
    "        if Mtype == \"OneVsRest\":\n",
    "            # instantiate the base classifier.\n",
    "            lr = LogisticRegression()\n",
    "            # instantiate the One Vs Rest Classifier.\n",
    "            OVRclassifier = OneVsRest(classifier=lr)\n",
    "#             fitModel = OVRclassifier.fit(train)\n",
    "            # Add parameters of your choice here:\n",
    "            paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "                .build()\n",
    "            #Cross Validator requires the following parameters:\n",
    "            crossval = CrossValidator(estimator=OVRclassifier,\n",
    "                                      estimatorParamMaps=paramGrid,\n",
    "                                      evaluator=MulticlassClassificationEvaluator(),\n",
    "                                      numFolds=2) # 3 is best practice\n",
    "            # Run cross-validation, and choose the best set of parameters.\n",
    "            fitModel = crossval.fit(train)\n",
    "            return fitModel\n",
    "        if Mtype == \"MultilayerPerceptronClassifier\":\n",
    "            # specify layers for the neural network:\n",
    "            # input layer of size features, two intermediate of features+1 and same size as features\n",
    "            # and output of size number of classes\n",
    "            # Note: crossvalidator cannot be used here\n",
    "            features_count = len(features[0][0])\n",
    "            layers = [features_count, features_count+1, features_count, classes]\n",
    "            MPC_classifier = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "            fitModel = MPC_classifier.fit(train)\n",
    "            return fitModel\n",
    "        if Mtype in(\"LinearSVC\",\"GBTClassifier\") and classes != 2: #These classifiers currently only accept binary classification\n",
    "            print(Mtype,\" could not be used because PySpark currently only accepts binary classification data for this algorithm\")\n",
    "            return\n",
    "#             fitModel = \"Not Acceptable\"\n",
    "#             return fitModel\n",
    "        if Mtype in(\"LogisticRegression\",\"NaiveBayes\",\"RandomForestClassifier\",\"GBTClassifier\",\"LinearSVC\",\"DecisionTreeClassifier\"):\n",
    "  \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"LogisticRegression\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "                             .addGrid(classifier.regParam, [0.1, 0.01]) \\\n",
    "                             .build())\n",
    "                \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"NaiveBayes\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "                             .addGrid(classifer.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]) \\\n",
    "                             .build())\n",
    "                \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"RandomForestClassifier\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "                               .addGrid(classifier.maxDepth, [2, 5, 10])\n",
    "                               .addGrid(classifier.maxBins, [5, 10, 20])\n",
    "                               .addGrid(classifier.numTrees, [5, 20, 50])\n",
    "                             .build())\n",
    "                \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"GBTClassifier\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "                             .addGrid(classifier.maxDepth, [2, 5, 10, 20, 30]) \\\n",
    "                             .addGrid(classifier.maxBins, [10, 20, 40, 80, 100]) \\\n",
    "                             .addGrid(classifier.maxIter, [10, 15])\n",
    "                             .build())\n",
    "                \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"LinearSVC\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "                             .addGrid(classifier.maxIter, [10, 15]) \\\n",
    "                             .addGrid(classifier.regParam, [0.1, 0.01]) \\\n",
    "                             .build())\n",
    "            \n",
    "            # Add parameters of your choice here:\n",
    "            if Mtype in(\"DecisionTreeClassifier\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "                             .addGrid(classifier.maxDepth, [2, 5, 10, 20, 30]) \\\n",
    "                             .addGrid(classifier.maxBins, [10, 20, 40, 80, 100]) \\\n",
    "                             .build())\n",
    "            \n",
    "            #Cross Validator requires all of the following parameters:\n",
    "            crossval = CrossValidator(estimator=classifier,\n",
    "                                      estimatorParamMaps=paramGrid,\n",
    "                                      evaluator=MulticlassClassificationEvaluator(),\n",
    "                                      numFolds=2) # 3 is best practice\n",
    "            # Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "            fitModel = crossval.fit(train)\n",
    "            return fitModel\n",
    "    \n",
    "    fitModel = IntanceFitModel(Mtype,classifier,classes,features)\n",
    "    \n",
    "    #Results:\n",
    "    columns = ['Classifier', 'Result']\n",
    "    \n",
    "    if Mtype in(\"LinearSVC\",\"GBTClassifier\") and classes != 2:\n",
    "        Mtype = [Mtype] # make this a string\n",
    "        score = [\"N/A\"]\n",
    "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
    "    else:\n",
    "        predictions = fitModel.transform(test)\n",
    "        MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\") # redictionCol=\"prediction\",\n",
    "        accuracy = (MC_evaluator.evaluate(predictions))*100\n",
    "        Mtype = [Mtype] # make this a string\n",
    "        score = [str(accuracy)] #make this a string and convert to a list\n",
    "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
    "        result = result.withColumn('Result',result.Result.substr(0, 5))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLogisticRegression:\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-6283ee36e6a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mClassTrainEval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfinal_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-33-05186a6d18a6>\u001b[0m in \u001b[0;36mClassTrainEval\u001b[1;34m(classifier, final_data, features, classes)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfitModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mfitModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIntanceFitModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfinal_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfitModel\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"Not Acceptable\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-05186a6d18a6>\u001b[0m in \u001b[0;36mIntanceFitModel\u001b[1;34m(Mtype, classifier, classes, final_data, features)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mMtype\u001b[0m \u001b[1;32min\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LogisticRegression\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"NaiveBayes\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"RandomForestClassifier\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"GBTClassifier\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"LinearSVC\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"DecisionTreeClassifier\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;31m# Fit Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0mfitModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfitModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mC:\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    290\u001b[0m         \"\"\"\n\u001b[0;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1255\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32mC:\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run!\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "classifiers = [\n",
    "                LogisticRegression(),OneVsRest(),LinearSVC()\n",
    "               ,NaiveBayes()\n",
    "              ,RandomForestClassifier(),GBTClassifier(),DecisionTreeClassifier()\n",
    "              ,MultilayerPerceptronClassifier()\n",
    "              ] \n",
    "\n",
    "features = final_data.select(['features']).collect()\n",
    "# Learn how many classes there are in order to specify evaluation type based on binary or multi and turn the df into an object\n",
    "class_count = final_data.select(F.countDistinct(\"label\")).collect()\n",
    "classes = class_count[0][0]\n",
    "    \n",
    "for classifier in classifiers:\n",
    "    ClassTrainEval(classifier,final_data,features,classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Diagnostics\n",
    "\n",
    "You can also generate some more detailed diagnostics on individual classifiers using this function too if you want. The output is pretty extensive, so I wouldn't do more than one at a time if I were you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.classification import *\n",
    "\n",
    "def ClassDiag(classifier):\n",
    "    \n",
    "    # Fit our model\n",
    "    C = classifier\n",
    "    fitModel = C.fit(train)\n",
    "\n",
    "    # Load the Summary\n",
    "    trainingSummary = fitModel.summary\n",
    "\n",
    "    # General Describe\n",
    "    trainingSummary.predictions.describe().show()\n",
    "\n",
    "    # View Predictions\n",
    "    pred_and_labels = fitModel.evaluate(test)\n",
    "    pred_and_labels.predictions.show()\n",
    "\n",
    "    # Print the coefficients and intercept for multinomial logistic regression\n",
    "    print(\"Coefficients: \\n\" + str(fitModel.coefficientMatrix))\n",
    "    print(\" \")\n",
    "    print(\"Intercept: \" + str(fitModel.interceptVector))\n",
    "    print(\" \")\n",
    "\n",
    "    # Obtain the objective per iteration\n",
    "    objectiveHistory = trainingSummary.objectiveHistory\n",
    "    print(\" \")\n",
    "    print(\"objectiveHistory:\")\n",
    "    for objective in objectiveHistory:\n",
    "        print(objective)\n",
    "\n",
    "    # for multiclass, we can inspect metrics on a per-label basis\n",
    "    print(\" \")\n",
    "    print(\"False positive rate by label:\")\n",
    "    for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "        print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "    print(\" \")\n",
    "    print(\"True positive rate by label:\")\n",
    "    for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "        print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "    print(\" \")\n",
    "    print(\"Precision by label:\")\n",
    "    for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "        print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "    print(\" \")\n",
    "    print(\"Recall by label:\")\n",
    "    for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "        print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "    print(\" \")\n",
    "    print(\"F-measure by label:\")\n",
    "    for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "        print(\"label %d: %s\" % (i, f))\n",
    "\n",
    "    accuracy = trainingSummary.accuracy\n",
    "    falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "    truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "    fMeasure = trainingSummary.weightedFMeasure()\n",
    "    precision = trainingSummary.weightedPrecision\n",
    "    recall = trainingSummary.weightedRecall\n",
    "    print(\" \")\n",
    "    print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "          % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------+\n",
      "|summary|              label|prediction|\n",
      "+-------+-------------------+----------+\n",
      "|  count|              21247|     21247|\n",
      "|   mean|0.12670024003388713|       0.0|\n",
      "| stddev| 0.3326447010262024|       0.0|\n",
      "|    min|                0.0|       0.0|\n",
      "|    max|                1.0|       0.0|\n",
      "+-------+-------------------+----------+\n",
      "\n",
      "+---------+-----+--------------------+--------------------+----------+\n",
      "| features|label|       rawPrediction|         probability|prediction|\n",
      "+---------+-----+--------------------+--------------------+----------+\n",
      "|(6,[],[])|  0.0|[2.15718124487540...|[0.89633793285060...|       0.0|\n",
      "|(6,[],[])|  0.0|[2.15718124487540...|[0.89633793285060...|       0.0|\n",
      "|(6,[],[])|  0.0|[2.15718124487540...|[0.89633793285060...|       0.0|\n",
      "|(6,[],[])|  0.0|[2.15718124487540...|[0.89633793285060...|       0.0|\n",
      "|(6,[],[])|  0.0|[2.15718124487540...|[0.89633793285060...|       0.0|\n",
      "|(6,[],[])|  0.0|[2.15718124487540...|[0.89633793285060...|       0.0|\n",
      "|(6,[],[])|  0.0|[2.15718124487540...|[0.89633793285060...|       0.0|\n",
      "|(6,[],[])|  0.0|[2.15718124487540...|[0.89633793285060...|       0.0|\n",
      "|(6,[],[])|  0.0|[2.15718124487540...|[0.89633793285060...|       0.0|\n",
      "|(6,[],[])|  0.0|[2.15718124487540...|[0.89633793285060...|       0.0|\n",
      "|(6,[],[])|  0.0|[2.15718124487540...|[0.89633793285060...|       0.0|\n",
      "|(6,[],[])|  0.0|[2.15718124487540...|[0.89633793285060...|       0.0|\n",
      "|(6,[],[])|  0.0|[2.15718124487540...|[0.89633793285060...|       0.0|\n",
      "|(6,[],[])|  0.0|[2.15718124487540...|[0.89633793285060...|       0.0|\n",
      "|(6,[],[])|  0.0|[2.15718124487540...|[0.89633793285060...|       0.0|\n",
      "|(6,[],[])|  0.0|[2.15718124487540...|[0.89633793285060...|       0.0|\n",
      "|(6,[],[])|  0.0|[2.15718124487540...|[0.89633793285060...|       0.0|\n",
      "|(6,[],[])|  0.0|[2.15718124487540...|[0.89633793285060...|       0.0|\n",
      "|(6,[],[])|  0.0|[2.15718124487540...|[0.89633793285060...|       0.0|\n",
      "|(6,[],[])|  0.0|[2.15718124487540...|[0.89633793285060...|       0.0|\n",
      "+---------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Coefficients: \n",
      "DenseMatrix([[  0.06481914,   0.0916818 ,   0.03482926, -13.18038165,\n",
      "               -0.03088175,  -0.07839659]])\n",
      " \n",
      "Intercept: [-2.157181244875402]\n",
      " \n",
      " \n",
      "objectiveHistory:\n",
      "0.3800655114373158\n",
      "0.37923704432094607\n",
      "0.37860436187489044\n",
      "0.37832027646024724\n",
      "0.3782434624598762\n",
      "0.3779943359052065\n",
      "0.37795038384977725\n",
      "0.3779478311205852\n",
      "0.37794772267437965\n",
      "0.3779477159156849\n",
      "0.37794768438710546\n",
      "0.3779476495648323\n",
      "0.3779475584844616\n",
      "0.377947448897252\n",
      "0.3779473946465261\n",
      "0.3779473674333021\n",
      "0.377947358871362\n",
      "0.37794735114975875\n",
      "0.37794734820650083\n",
      "0.37794734465796237\n",
      "0.3779473426862535\n",
      "0.37794734229281096\n",
      "0.3779473417486366\n",
      "0.3779473413082543\n",
      "0.3779473410891747\n",
      "0.3779473410809246\n",
      "0.3779473409586\n",
      "0.37794734093387194\n",
      "0.37794734091501997\n",
      " \n",
      "False positive rate by label:\n",
      "label 0: 1.0\n",
      "label 1: 0.0\n",
      " \n",
      "True positive rate by label:\n",
      "label 0: 1.0\n",
      "label 1: 0.0\n",
      " \n",
      "Precision by label:\n",
      "label 0: 0.8732997599661129\n",
      "label 1: 0.0\n",
      " \n",
      "Recall by label:\n",
      "label 0: 1.0\n",
      "label 1: 0.0\n",
      " \n",
      "F-measure by label:\n",
      "label 0: 0.9323652077785035\n",
      "label 1: 0.0\n",
      " \n",
      "Accuracy: 0.8732997599661129\n",
      "FPR: 0.8732997599661129\n",
      "TPR: 0.8732997599661129\n",
      "F-measure: 0.814234312153722\n",
      "Precision: 0.7626524707568704\n",
      "Recall: 0.8732997599661129\n"
     ]
    }
   ],
   "source": [
    "# classifier = LogisticRegression()\n",
    "ClassDiag(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
