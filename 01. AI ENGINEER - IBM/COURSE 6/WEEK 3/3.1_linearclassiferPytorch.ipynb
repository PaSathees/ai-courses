{"cells":[{"cell_type":"markdown","id":"eb4e5ebe-59b9-44d5-bda1-060fb610e373","metadata":{},"outputs":[],"source":["\u003ca href=\"http://cocl.us/pytorch_link_top\"\u003e\n","    \u003cimg src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \"\u003e\n","\u003c/a\u003e \n"]},{"cell_type":"markdown","id":"640184b8-fa42-4dce-8e20-cdc0a3baf4f4","metadata":{},"outputs":[],"source":["\u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"200\" alt=\"cognitiveclass.ai logo\"\u003e\n"]},{"cell_type":"markdown","id":"ae711f68-15b6-4dbf-8ada-158cf9b52bee","metadata":{},"outputs":[],"source":["\u003ch1\u003eObjective\u003c/h1\u003e\u003cul\u003e\u003cli\u003e How to use linear classifier in pytorch.\u003c/li\u003e\u003c/ul\u003e \n"]},{"cell_type":"markdown","id":"52e067d5-361c-4714-a094-dd54bb67e92d","metadata":{},"outputs":[],"source":["\u003ch1\u003eLinear  Classifier with PyTorch \u003c/h1\u003e\n"]},{"cell_type":"markdown","id":"2a846beb-3264-4fb5-ba18-1a956d2e8431","metadata":{},"outputs":[],"source":["\u003cp\u003eBefore you use a  Deep neural network to solve the classification problem,  it 's a good idea to try and solve the problem with the simplest method. You will need the dataset object from the previous section.\n","In this lab, we solve the problem with a linear classifier.\n"," You will be asked to determine the maximum accuracy your linear classifier can achieve on the validation data for 5 epochs. We will give some free parameter values if you follow the instructions you will be able to answer the quiz. Just like the other labs there are several steps, but in this lab you will only be quizzed on the final result. \u003c/p\u003e\n"]},{"cell_type":"markdown","id":"d6f15bde-18cf-49e2-837e-a11b62a81c6e","metadata":{},"outputs":[],"source":["\u003ch2\u003eTable of Contents\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"93f6d1e8-16ab-486f-8432-8bca70ecbde2","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-block alert-info\" style=\"margin-top: 20px\"\u003e\n","\n","\n","\u003cul\u003e\n","    \u003cli\u003e\u003ca href=\"#auxiliary\"\u003e Imports and Auxiliary Functions \u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#download_data\"\u003e Download data\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#data_class\"\u003e Dataset Class\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#trasform_Data_object\"\u003eTransform Object and Dataset Object\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#Question\"\u003eQuestion\u003c/a\u003e\u003c/li\u003e\n","\u003c/ul\u003e\n","\u003cp\u003eEstimated Time Needed: \u003cstrong\u003e25 min\u003c/strong\u003e\u003c/p\u003e\n"," \u003c/div\u003e\n","\u003chr\u003e\n"]},{"cell_type":"markdown","id":"ea118ece-de02-4f25-ac5f-9e39aaaed821","metadata":{},"outputs":[],"source":["\u003ch2 id=\"auxiliary\"\u003eImports and Auxiliary Functions\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"40e88317-d080-4159-bb6d-010570941fe4","metadata":{},"outputs":[],"source":["The following are the libraries we are going to use for this lab:\n"]},{"cell_type":"code","id":"ab3570b6-20fe-4036-9753-d8358b94a240","metadata":{},"outputs":[],"source":["from PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nimport glob\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nfrom torch import optim \nimport skillsnetwork "]},{"cell_type":"markdown","id":"07885ae3-0b16-46d3-aab8-35d93f227772","metadata":{},"outputs":[],"source":["\u003ch2 id=\"download_data\"\u003eDownload Data\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"5a801430-bedb-4ce9-b15c-2e09257217cb","metadata":{},"outputs":[],"source":["In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. \u003cb\u003eskillsnetwork.prepare\u003c/b\u003e is a command that's used to download a zip file, unzip it and store it in a specified directory. Locally we store the data in the directory  **/resources/data**. \n"]},{"cell_type":"markdown","id":"e93fbed5-6f56-440a-ba86-c74043db78b3","metadata":{},"outputs":[],"source":["First, we download the file that contains the images, if you didn't do this in your first lab uncomment:\n"]},{"cell_type":"code","id":"23724ba2-2761-437f-bf02-e62f56dbefb8","metadata":{},"outputs":[],"source":["# await skillsnetwork.prepare(\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip\", path = \"/resources/data\", overwrite=True)"]},{"cell_type":"markdown","id":"dbea51b1-61c5-4065-a929-9a0090442fd2","metadata":{},"outputs":[],"source":["\u003ch2 id=\"data_class\"\u003eDataset Class\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"2366c6e6-dbb9-49da-b655-c78c04458a71","metadata":{},"outputs":[],"source":["In this section, we will use the previous code to build a dataset class. As before, make sure the even samples are positive, and the odd samples are negative.  If the parameter \u003ccode\u003etrain\u003c/code\u003e is set to \u003ccode\u003eTrue\u003c/code\u003e, use the first 30 000  samples as training data; otherwise, the remaining samples will be used as validation data. Do not forget to sort your files so they are in the same order.  \n"]},{"cell_type":"code","id":"02c27ffc-097f-4aba-9e30-511ba931a91e","metadata":{},"outputs":[],"source":["class Dataset(Dataset):\n\n    # Constructor\n    def __init__(self,transform=None,train=True):\n        directory=\"/resources/data\"\n        positive=\"Positive\"\n        negative=\"Negative\"\n\n        positive_file_path=os.path.join(directory,positive)\n        negative_file_path=os.path.join(directory,negative)\n        positive_files=[os.path.join(positive_file_path,file) for file in  os.listdir(positive_file_path) if file.endswith(\".jpg\")]\n        positive_files.sort()\n        negative_files=[os.path.join(negative_file_path,file) for file in  os.listdir(negative_file_path) if file.endswith(\".jpg\")]\n        negative_files.sort()\n        number_of_samples=len(positive_files)+len(negative_files)\n        self.all_files=[None]*number_of_samples\n        self.all_files[::2]=positive_files\n        self.all_files[1::2]=negative_files \n        # The transform is goint to be used on image\n        self.transform = transform\n        #torch.LongTensor\n        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n        self.Y[::2]=1\n        self.Y[1::2]=0\n        \n        if train:\n            self.all_files=self.all_files[0:30000]\n            self.Y=self.Y[0:30000]\n            self.len=len(self.all_files)\n        else:\n            self.all_files=self.all_files[30000:]\n            self.Y=self.Y[30000:]\n            self.len=len(self.all_files)    \n       \n    # Get the length\n    def __len__(self):\n        return self.len\n    \n    # Getter\n    def __getitem__(self, idx):\n        \n        \n        image=Image.open(self.all_files[idx])\n        y=self.Y[idx]\n          \n        \n        # If there is any transform method, apply it onto the image\n        if self.transform:\n            image = self.transform(image)\n\n        return image, y"]},{"cell_type":"markdown","id":"a257db61-7668-4bf8-bdf0-f0ab5d9db44e","metadata":{},"outputs":[],"source":["\u003ch2 id=\"trasform_Data_object\"\u003eTransform Object and Dataset Object\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"979d3412-c642-4840-ae11-e4b6f1a3f56d","metadata":{},"outputs":[],"source":["Create a transform object, that uses the \u003ccode\u003eCompose\u003c/code\u003e function. First use the transform \u003ccode\u003eToTensor()\u003c/code\u003e and followed by \u003ccode\u003eNormalize(mean, std)\u003c/code\u003e. The value for \u003ccode\u003e mean\u003c/code\u003e and \u003ccode\u003estd\u003c/code\u003e are provided for you.\n"]},{"cell_type":"code","id":"17460b4c-1b50-45b1-8eec-293e3ca2f3ee","metadata":{},"outputs":[],"source":["mean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n# transforms.ToTensor()\n#transforms.Normalize(mean, std)\n#transforms.Compose([])\n\ntransform =transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean, std)])\n"]},{"cell_type":"markdown","id":"c55bea21-9852-420b-9f24-cd6619f17fd6","metadata":{},"outputs":[],"source":["Create object for the training data  \u003ccode\u003edataset_train\u003c/code\u003e and validation \u003ccode\u003edataset_val\u003c/code\u003e. Use the transform object to convert the images to tensors using the transform object:\n"]},{"cell_type":"code","id":"e4270c52-15f6-4e0d-8743-4d5a4ed7d6fb","metadata":{},"outputs":[],"source":["dataset_train=Dataset(transform=transform,train=True)\ndataset_val=Dataset(transform=transform,train=False)"]},{"cell_type":"markdown","id":"adb8c572-20ae-4c1b-9f85-064d8bd31e82","metadata":{},"outputs":[],"source":["We  can find the shape of the image:\n"]},{"cell_type":"code","id":"a67f0aca-be86-43a0-be65-e5cffcb1a313","metadata":{},"outputs":[],"source":["dataset_train[0][0].shape"]},{"cell_type":"markdown","id":"ea8b92eb-1be6-40a6-b9be-b08eea4f3c38","metadata":{},"outputs":[],"source":["We see that it's a color image with three channels:\n"]},{"cell_type":"code","id":"2d908c45-437f-4d27-8ec3-ce0cebf8141a","metadata":{},"outputs":[],"source":["size_of_image=3*227*227\nsize_of_image"]},{"cell_type":"markdown","id":"ee0cb0d5-c8bb-4d9b-aed4-217916f7842a","metadata":{},"outputs":[],"source":["\u003ch2 id=\"Question\"\u003e Question \u003ch2\u003e\n"]},{"cell_type":"markdown","id":"6536aabf-4d39-458c-b39c-b88228ca4370","metadata":{},"outputs":[],"source":["\u003cb\u003e Create a custom module for Softmax for two classes,called model. The input size should be the \u003ccode\u003esize_of_image\u003c/code\u003e, you should record the maximum accuracy achieved on the validation data for the different epochs. For example if the 5 epochs the accuracy was 0.5, 0.2, 0.64,0.77, 0.66 you would select 0.77.\u003c/b\u003e\n"]},{"cell_type":"markdown","id":"1eb5944d-40fa-48a4-abc4-9b3db325415c","metadata":{},"outputs":[],"source":["Train the model with the following free parameter values:\n"]},{"cell_type":"markdown","id":"75efb6de-eabc-4481-996a-8bfdcb59c910","metadata":{},"outputs":[],"source":["\u003cb\u003eParameter Values\u003c/b\u003e\n","   \u003cli\u003elearning rate:0.1 \u003c/li\u003e\n","   \u003cli\u003emomentum term:0.1 \u003c/li\u003e\n","   \u003cli\u003ebatch size training:1000\u003c/li\u003e\n","   \u003cli\u003eLoss function:Cross Entropy Loss \u003c/li\u003e\n","   \u003cli\u003eepochs:5\u003c/li\u003e\n","   \u003cli\u003eset: torch.manual_seed(0)\u003c/li\u003e\n"]},{"cell_type":"code","id":"b8692d62-a72b-4950-bd91-fbd5057f26f7","metadata":{},"outputs":[],"source":["torch.manual_seed(0)"]},{"cell_type":"markdown","id":"ff92264d-5538-4b27-a7e7-8d817276d7a2","metadata":{},"outputs":[],"source":["\u003cb\u003eCustom Module:\u003c/b\u003e\n"]},{"cell_type":"code","id":"438051f7-c75f-48b5-acc2-e41dee4dac2a","metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"7dea3dda-2f8b-41e6-9096-5f4fa6bba30b","metadata":{},"outputs":[],"source":["\u003cb\u003eModel Object:\u003c/b\u003e\n"]},{"cell_type":"code","id":"3149e1df-bfd7-44ac-b683-7b7e462e83e7","metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"c302afd0-5749-43be-9a09-97c7c87adfb8","metadata":{},"outputs":[],"source":["\u003cb\u003eOptimizer:\u003c/b\u003e\n"]},{"cell_type":"code","id":"dde39335-6401-4db0-b22e-a682de5bf59b","metadata":{},"outputs":[],"source":["    "]},{"cell_type":"markdown","id":"2879f0c7-1695-45e2-bc2d-1726d7f15951","metadata":{},"outputs":[],"source":["\u003cb\u003eCriterion:\u003c/b\u003e\n"]},{"cell_type":"code","id":"78806b22-a581-4d00-81b3-05f335a2495c","metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"a00406ab-ca7c-48df-96b2-f5c0ff3f2b21","metadata":{},"outputs":[],"source":["\u003cb\u003eData Loader Training and Validation:\u003c/b\u003e\n"]},{"cell_type":"code","id":"126ae591-410e-4262-8e03-5107a2aa7c46","metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"5c299b89-ae2c-483a-9ecf-19d14f6dbab0","metadata":{},"outputs":[],"source":["\u003cb\u003eTrain Model with 5 epochs, should take 35 minutes: \u003c/b\u003e\n"]},{"cell_type":"code","id":"ff4812b3-df38-4946-9b0c-26caec3ae163","metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"85878f35-4fda-4e05-a584-9954b6d0e332","metadata":{},"outputs":[],"source":["\u003ch2\u003eAbout the Authors:\u003c/h2\u003e\n"," \u003ca href=\\\"https://www.linkedin.com/in/joseph-s-50398b136/\\\"\u003eJoseph Santarcangelo\u003c/a\u003e has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"]},{"cell_type":"markdown","id":"e335534c-e249-4363-b80f-08abc6753528","metadata":{},"outputs":[],"source":["\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","\n"]},{"cell_type":"markdown","id":"eae498d4-4034-4ff6-b13c-e1ab9547f73f","metadata":{},"outputs":[],"source":["Copyright \u0026copy; 2019 \u003ca href=\"cognitiveclass.ai\"\u003e cognitiveclass.ai\u003c/a\u003e. This notebook and its source code are released under the terms of the \u003ca href=\"https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\"\u003eMIT License\u003c/a\u003e\n"]},{"cell_type":"code","id":"4af06455-ee54-4b49-b598-176eee6f89fc","metadata":{},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}