{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1247b080-386d-4935-8c60-6eff988cb7d6",
   "metadata": {},
   "source": [
    "<a href=\"http://cocl.us/pytorch_link_top\">\n",
    "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \">\n",
    "</a> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5cd126-8643-40d2-b99c-da96e4dbfc7d",
   "metadata": {},
   "source": [
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"200\" alt=\"cognitiveclass.ai logo\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb7cc34-2706-4e15-8b48-f0108d774e94",
   "metadata": {},
   "source": [
    "<h1><h1>Pre-trained-Models with PyTorch </h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6faa28-fb73-47ce-a115-adac7796851f",
   "metadata": {},
   "source": [
    "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions: \n",
    "<ul>\n",
    "<li>change the output layer</li>\n",
    "<li> train the model</li> \n",
    "<li>  identify  several  misclassified samples</li> \n",
    " </ul>\n",
    "You will take several screenshots of your work and share your notebook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e88531-5f91-457c-938f-43b18c22d58f",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c017d14-585b-4a09-bfc6-a29387eecff2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#download_data\"> Download Data</a></li>\n",
    "    <li><a href=\"#auxiliary\"> Imports and Auxiliary Functions </a></li>\n",
    "    <li><a href=\"#data_class\"> Dataset Class</a></li>\n",
    "    <li><a href=\"#Question_1\">Question 1</a></li>\n",
    "    <li><a href=\"#Question_2\">Question 2</a></li>\n",
    "    <li><a href=\"#Question_3\">Question 3</a></li>\n",
    "</ul>\n",
    "<p>Estimated Time Needed: <strong>120 min</strong></p>\n",
    " </div>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f50844-38cd-43f0-b437-3ff9e1b0da4c",
   "metadata": {},
   "source": [
    "<h2 id=\"download_data\">Download Data</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab3f1a1-168d-41fc-bd61-8764c54bb61a",
   "metadata": {},
   "source": [
    "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6af40dc-c33d-4e3a-8e8d-61bf6f944b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-05 07:42:30--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\n",
      "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
      "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2598656062 (2.4G) [application/zip]\n",
      "Saving to: ‘Positive_tensors.zip’\n",
      "\n",
      "Positive_tensors.zi 100%[===================>]   2.42G  15.4MB/s    in 3m 22s  \n",
      "\n",
      "2023-01-05 07:45:54 (12.2 MB/s) - ‘Positive_tensors.zip’ saved [2598656062/2598656062]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a2f44fc-8f5f-453a-b5a5-263094e83cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q Positive_tensors.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36098383-127a-4f9c-a3cc-f195c61a694f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-05 07:50:47--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
      "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
      "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2111408108 (2.0G) [application/zip]\n",
      "Saving to: ‘Negative_tensors.zip’\n",
      "\n",
      "Negative_tensors.zi 100%[===================>]   1.97G  25.2MB/s    in 81s     \n",
      "\n",
      "2023-01-05 07:52:08 (25.0 MB/s) - ‘Negative_tensors.zip’ saved [2111408108/2111408108]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
    "!unzip -q Negative_tensors.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13c6dd7-ce72-4d09-9785-e4fdb6958120",
   "metadata": {},
   "source": [
    "We will install torchvision:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4feef7ad-d3c4-4442-ace8-df32b8598677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (0.11.3)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from torchvision) (1.20.3)\r\n",
      "Requirement already satisfied: torch in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from torchvision) (1.10.2)\r\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from torchvision) (9.0.1)\r\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from torch->torchvision) (4.1.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/wsuser/work'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d80ff9-7d21-488e-879a-15e5657a84d8",
   "metadata": {},
   "source": [
    "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a2ea30-c782-4005-8ac2-4481524eb05b",
   "metadata": {},
   "source": [
    "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "963fa546-6f43-4514-bd88-88c554e6c5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f417802e670>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the libraries will be used for this lab.\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import pandas\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import torch \n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import os\n",
    "import glob\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b43c4498-4c49-4f67-9036-e9bde7c58dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pylab as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f8e4ef-82bc-43f7-8f38-e1725efcd5e7",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4690b1c6-5242-4dab-afaf-1012e6db58f0",
   "metadata": {},
   "source": [
    "<h2 id=\"data_class\">Dataset Class</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91a616b-13c1-4d31-8345-e74a160c1cf7",
   "metadata": {},
   "source": [
    " This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c90f244-fe69-42f4-a1ee-656ed7a93a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Create your own dataset object\n",
    "\n",
    "class Dataset(Dataset):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self,transform=None,train=True):\n",
    "        directory=\"/home/wsuser/work\"\n",
    "        positive=\"Positive_tensors\"\n",
    "        negative='Negative_tensors'\n",
    "\n",
    "        positive_file_path=os.path.join(directory,positive)\n",
    "        negative_file_path=os.path.join(directory,negative)\n",
    "        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n",
    "        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n",
    "        number_of_samples=len(positive_files)+len(negative_files)\n",
    "        self.all_files=[None]*number_of_samples\n",
    "        self.all_files[::2]=positive_files\n",
    "        self.all_files[1::2]=negative_files \n",
    "        # The transform is goint to be used on image\n",
    "        self.transform = transform\n",
    "        #torch.LongTensor\n",
    "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
    "        self.Y[::2]=1\n",
    "        self.Y[1::2]=0\n",
    "        \n",
    "        if train:\n",
    "            self.all_files=self.all_files[0:30000]\n",
    "            self.Y=self.Y[0:30000]\n",
    "            self.len=len(self.all_files)\n",
    "        else:\n",
    "            self.all_files=self.all_files[30000:]\n",
    "            self.Y=self.Y[30000:]\n",
    "            self.len=len(self.all_files)     \n",
    "       \n",
    "    # Get the length\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    # Getter\n",
    "    def __getitem__(self, idx):\n",
    "               \n",
    "        image=torch.load(self.all_files[idx])\n",
    "        y=self.Y[idx]\n",
    "                  \n",
    "        # If there is any transform method, apply it onto the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, y\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b026261-027a-4482-a94a-df6a2af6a51e",
   "metadata": {},
   "source": [
    "We create two dataset objects, one for the training data and one for the validation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2710f85e-d352-46ad-8558-be1fbb30d844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(train=True)\n",
    "validation_dataset = Dataset(train=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02983727-3c13-439f-aacc-6fb3769c340c",
   "metadata": {},
   "source": [
    "<h2 id=\"Question_1\">Question 1</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c8f3d2-219f-4150-9b70-e21485a9bf4a",
   "metadata": {},
   "source": [
    "<b>Prepare a pre-trained resnet18 model :</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d424749-e3ed-4432-87ea-7c2b1d38b62c",
   "metadata": {},
   "source": [
    "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6edb541c-0935-4ea7-871f-fccde601ebc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/wsuser/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a208a61441489381bb8a01e4f33189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Load the pre-trained model resnet18\n",
    "\n",
    "# Type your code here\n",
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79280b0e-a54b-460e-910d-dbad38158eaa",
   "metadata": {},
   "source": [
    "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d190b40-2ecb-4703-98cd-a34d694b270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Set the parameter cannot be trained for the pre-trained model\n",
    "\n",
    "\n",
    "# Type your code here\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67f6c32-da28-4d1a-aac2-52eadc94767f",
   "metadata": {},
   "source": [
    "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce25ad-52e5-4bf9-bb65-b9e082f52fa3",
   "metadata": {},
   "source": [
    "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d8d977a-89dd-406e-8b1d-40a573d5fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7882ba6c-ca2b-4bf6-a909-27481a37875e",
   "metadata": {},
   "source": [
    "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15eeb747-fd7a-44eb-9639-0bed997c7cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d15c23a-78fc-4e24-8e1a-b046f73e1ca0",
   "metadata": {},
   "source": [
    "<h2 id=\"Question_2\">Question 2: Train the Model</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c395b9-dec3-4316-8900-cdf72098bc39",
   "metadata": {},
   "source": [
    "In this question you will train your, model:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b174936-0e75-4a42-93a1-dff916a53585",
   "metadata": {},
   "source": [
    "<b>Step 1</b>: Create a cross entropy criterion function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a350110-c9a0-4dfa-9bd5-2407a12fc8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create the loss function\n",
    "\n",
    "# Type your code here\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4c68ba-f53f-4bb7-8f9e-ed7d9c332227",
   "metadata": {},
   "source": [
    "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf2b5a68-8199-4349-92cd-9bd8bf885f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587b863b-0518-4a26-b9dc-5e45faaaa499",
   "metadata": {},
   "source": [
    "<b>Step 3</b>: Use the following optimizer to minimize the loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3524293e-1933-4dd9-847b-bfc00a0d3b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53952d1f-1ed8-435b-8194-ad764f1b41bd",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a22fa1f-a19e-46cb-b736-9866ef149fc1",
   "metadata": {},
   "source": [
    "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e35b8aaf-f2c2-43d9-aa24-635df985e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=1\n",
    "loss_list=[]\n",
    "accuracy_list=[]\n",
    "correct=0\n",
    "N_test=len(validation_dataset)\n",
    "N_train=len(train_dataset)\n",
    "start_time = time.time()\n",
    "#n_epochs\n",
    "\n",
    "Loss=0\n",
    "start_time = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    for x, y in train_loader:\n",
    "\n",
    "        model.train() \n",
    "        #clear gradient \n",
    "        optimizer.zero_grad()\n",
    "        #make a prediction \n",
    "        z = model(x)\n",
    "        # calculate loss \n",
    "        loss = criterion(z, y)\n",
    "        # calculate gradients of parameters \n",
    "        loss.backward()\n",
    "        # update parameters \n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.data)\n",
    "    correct=0\n",
    "    for x_test, y_test in validation_loader:\n",
    "        # set model to eval \n",
    "        model.eval()\n",
    "        #make a prediction \n",
    "        z = model(x_test)\n",
    "        #find max \n",
    "        _, yhat = torch.max(z.data, 1)\n",
    "       \n",
    "       \n",
    "        #Calculate misclassified  samples in mini-batch \n",
    "        #hint +=(yhat==y_test).sum().item()\n",
    "        correct += (yhat==y_test).sum().item()\n",
    "        \n",
    "   \n",
    "    accuracy=correct/N_test\n",
    "    accuracy_list.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2936f31-1fc9-4c26-a8a3-01e0c439f9de",
   "metadata": {},
   "source": [
    "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfc80fb1-f8a8-4cfe-8a55-7c2632ec0bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9945"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "916449d1-cc64-4f16-9324-bba92f4f9a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e661a1c-cf90-4bef-8a81-2c48ed8d7739",
   "metadata": {},
   "source": [
    "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fae0f51-ac87-423c-9012-5ebba688cf72",
   "metadata": {},
   "source": [
    "<b>Identify the first four misclassified samples using the validation data:</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0155e1ce-ba75-49ac-a410-004a284e082c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 56, Predicted value: tensor([0]), Actual value: tensor([1])\n",
      "Sample 67, Predicted value: tensor([1]), Actual value: tensor([0])\n",
      "Sample 102, Predicted value: tensor([0]), Actual value: tensor([1])\n",
      "Sample 610, Predicted value: tensor([0]), Actual value: tensor([1])\n"
     ]
    }
   ],
   "source": [
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=1)\n",
    "count = 1\n",
    "for i, (x_test, y_test) in enumerate(validation_loader):\n",
    "    if count > 4:\n",
    "        break\n",
    "    \n",
    "    model.eval()\n",
    "    z = model(x_test)\n",
    "    _, yhat = torch.max(z.data, 1)\n",
    "\n",
    "    if yhat != y_test:\n",
    "        print(f\"Sample {str(i)}, Predicted value: {str(yhat)}, Actual value: {str(y_test)}\")\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cb2502-67c5-41ba-8168-4a04c18da4be",
   "metadata": {},
   "source": [
    "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\"> CLICK HERE </a> Click here to see how to share your notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72da0282-5545-4faf-9111-ea45a41c72eb",
   "metadata": {},
   "source": [
    "<h2>About the Authors:</h2> \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552a2360-ee44-4531-a1c1-59c7a741c3c2",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-21  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93721e4d-82f7-404f-95b3-544836d83527",
   "metadata": {},
   "source": [
    "Copyright &copy; 2018 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">MIT License</a>.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
