{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark Dataframe Basics Master \n",
    "\n",
    "## Spark DataFrame Basics\n",
    "\n",
    "Spark DataFrames are the workhouse and main way of working with Spark and Python post Spark 2.0. DataFrames act as powerful versions of tables, with rows and columns, easily handling large datasets. The shift to DataFrames provides many advantages:\n",
    "* A much simpler syntax\n",
    "* Ability to use SQL directly in the dataframe\n",
    "* Operations are automatically distributed across RDDs\n",
    "    \n",
    "If you've used R or even the pandas library with Python you are probably already familiar with the concept of DataFrames. Spark DataFrame expand on a lot of these concepts, allowing you to transfer that knowledge easily by understanding the simple syntax of Spark DataFrames. Remember that the main advantage to using Spark DataFrames vs those other programs is that Spark can handle data across many RDDs, huge data sets that would never fit on a single computer. That comes at a slight cost of some \"peculiar\" syntax choices, but after this course you will feel very comfortable with all those topics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PySpark?\n",
    "**PySpark** is the collaboration of **Apache Spark** and **Python**.\n",
    "\n",
    "Apache Spark is an open-source cluster-computing framework, built around speed, ease of use, and streaming analytics whereas Python is a general-purpose, high-level programming language. It provides a wide range of libraries and is majorly used for Machine Learning and Real-Time Streaming Analytics.\n",
    "\n",
    "In other words, it **is a Python API for Spark** that lets you harness the simplicity of Python and the power of Apache Spark in order to tame Big Data and perform massive distributed processing over resilient sets of data. It's a must for Big data’s lovers. \n",
    "\n",
    "## How is PySpark different than Python?\n",
    "\n",
    "One of the most noteable differences you will find with PySpark as opposed to Python is that it runs on a SparkContext which is a cluster, so certian processes will look different especially when you get in the machine learning libraries. In addition to this main difference, I've note a few attibutes to be aware of below:\n",
    "\n",
    "1. PySpark does not use indexing\n",
    "2. **ALL** objects in PySpark are **immutable**\n",
    "3. Error messages are much less informative\n",
    "4. Many of the libraries you are used to using in Python won't function in PySpark\n",
    "\n",
    "## Contents of this notebook\n",
    "This notebook is intended to provide students with a easily searchable repository of all the content covered in the Dataframe Essentials portion of the course. I hope you all find this documentation useful!\n",
    "\n",
    "## Some helpful additional resources\n",
    "\n",
    "- Exploring S3 Keys:https://alexwlchan.net/2017/07/listing-s3-keys/\n",
    "- Using S3 Select: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-s3select.html\n",
    "- PySpark Cheat Sheets: \n",
    "    https://www.qubole.com/resources/pyspark-cheatsheet/\n",
    "    https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf\n",
    "    \n",
    "## Spark UI\n",
    "If you're ever curious about how your PySpark instance is performing, Spark offers a neat Web UI with tons of information. Just navigate to http://[driver]:4040 in your browswer where \"drive\" is you driver name. If you are running PySpark locally, it would be http://localhost:4040\n",
    "\n",
    "\n",
    "## Let's Get started!\n",
    "\n",
    "Starting a PySpark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working with 1 core(s) on appid:  local-1620680853556\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://orcuns-mbp-2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Review2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe0056ba400>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's create our PySpark instance\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# May take awhile locally\n",
    "spark = SparkSession.builder.appName(\"Review2\").getOrCreate()\n",
    "\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "appid = spark._jsc.sc().applicationId()\n",
    "print(\"You are working with\", cores, \"core(s) on appid: \",appid)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/Users/layalai/Documents/Jupyter Notebooks and Datasets AS of 23FEB21/Jupyter Notebooks and Datasets AS of 23FEB21/PySpark DataFrame Essentials/Datasets/users.parquet;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5674e245eec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mparquet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'users.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Partioned Parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    351\u001b[0m         self._set_opts(mergeSchema=mergeSchema, pathGlobFilter=pathGlobFilter,\n\u001b[1;32m    352\u001b[0m                        recursiveFileLookup=recursiveFileLookup)\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/Users/layalai/Documents/Jupyter Notebooks and Datasets AS of 23FEB21/Jupyter Notebooks and Datasets AS of 23FEB21/PySpark DataFrame Essentials/Datasets/users.parquet;"
     ]
    }
   ],
   "source": [
    "path =\"Datasets/\"\n",
    "\n",
    "# CSV\n",
    "df = spark.read.csv(path+'students.csv',inferSchema=True,header=True)\n",
    "\n",
    "# Json\n",
    "people = spark.read.json(path+'people.json')\n",
    "\n",
    "# Parquet\n",
    "parquet = spark.read.parquet(path+'users.parquet')\n",
    "\n",
    "# Partioned Parquet\n",
    "partitioned = spark.read.parquet(path+'users*')\n",
    "\n",
    "# Parts of a partitioned Parquet\n",
    "users1_2 = spark.read.option(\"basePath\", path).parquet(path+'users1.parquet', path+'users2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+\n",
      "|gender|race/ethnicity|parental level of education|       lunch|test preparation course|math score|reading score|writing score|\n",
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+\n",
      "|female|       group B|          bachelor's degree|    standard|                   none|        72|           72|           74|\n",
      "|female|       group C|               some college|    standard|              completed|        69|           90|           88|\n",
      "|female|       group B|            master's degree|    standard|                   none|        90|           95|           93|\n",
      "|  male|       group A|         associate's degree|free/reduced|                   none|        47|           57|           44|\n",
      "|  male|       group C|               some college|    standard|                   none|        76|           78|           75|\n",
      "|female|       group B|         associate's degree|    standard|                   none|        71|           83|           78|\n",
      "|female|       group B|               some college|    standard|              completed|        88|           95|           92|\n",
      "|  male|       group B|               some college|free/reduced|                   none|        40|           43|           39|\n",
      "|  male|       group D|                high school|free/reduced|              completed|        64|           64|           67|\n",
      "|female|       group B|                high school|free/reduced|                   none|        38|           60|           50|\n",
      "|  male|       group C|         associate's degree|    standard|                   none|        58|           54|           52|\n",
      "|  male|       group D|         associate's degree|    standard|                   none|        40|           52|           43|\n",
      "|female|       group B|                high school|    standard|                   none|        65|           81|           73|\n",
      "|  male|       group A|               some college|    standard|              completed|        78|           72|           70|\n",
      "|female|       group A|            master's degree|    standard|                   none|        50|           53|           58|\n",
      "|female|       group C|           some high school|    standard|                   none|        69|           75|           78|\n",
      "|  male|       group C|                high school|    standard|                   none|        88|           89|           86|\n",
      "|female|       group B|           some high school|free/reduced|                   none|        18|           32|           28|\n",
      "|  male|       group C|            master's degree|free/reduced|              completed|        46|           42|           46|\n",
      "|female|       group C|         associate's degree|free/reduced|                   none|        54|           58|           61|\n",
      "+------+--------------+---------------------------+------------+-----------------------+----------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>69</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>master's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>90</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>47</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>female</td>\n",
       "      <td>group E</td>\n",
       "      <td>master's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>88</td>\n",
       "      <td>99</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>male</td>\n",
       "      <td>group C</td>\n",
       "      <td>high school</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>62</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>female</td>\n",
       "      <td>group C</td>\n",
       "      <td>high school</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>completed</td>\n",
       "      <td>59</td>\n",
       "      <td>71</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>female</td>\n",
       "      <td>group D</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>68</td>\n",
       "      <td>78</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>female</td>\n",
       "      <td>group D</td>\n",
       "      <td>some college</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>77</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gender race/ethnicity parental level of education         lunch  \\\n",
       "0    female        group B           bachelor's degree      standard   \n",
       "1    female        group C                some college      standard   \n",
       "2    female        group B             master's degree      standard   \n",
       "3      male        group A          associate's degree  free/reduced   \n",
       "4      male        group C                some college      standard   \n",
       "..      ...            ...                         ...           ...   \n",
       "995  female        group E             master's degree      standard   \n",
       "996    male        group C                 high school  free/reduced   \n",
       "997  female        group C                 high school  free/reduced   \n",
       "998  female        group D                some college      standard   \n",
       "999  female        group D                some college  free/reduced   \n",
       "\n",
       "    test preparation course  math score  reading score  writing score  \n",
       "0                      none          72             72             74  \n",
       "1                 completed          69             90             88  \n",
       "2                      none          90             95             93  \n",
       "3                      none          47             57             44  \n",
       "4                      none          76             78             75  \n",
       "..                      ...         ...            ...            ...  \n",
       "995               completed          88             99             95  \n",
       "996                    none          62             55             55  \n",
       "997               completed          59             71             65  \n",
       "998               completed          68             78             77  \n",
       "999                    none          77             86             86  \n",
       "\n",
       "[1000 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice the type differences here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.toPandas()\n",
    "type(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Schema and content at a glance\n",
    "\n",
    "Always a good idea to do this to ensure that dataframe was read in correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---+-----+--------+\n",
      "|first|     last|gpa|grade|absenses|\n",
      "+-----+---------+---+-----+--------+\n",
      "| Carl|Weilsburg|3.6|    8|       3|\n",
      "|Geane|Grabsbury|4.0|    9|       0|\n",
      "|   Lu|     Wang|3.5|   10|       4|\n",
      "+-----+---------+---+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get an inital view of your dataframe\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- gpa: double (nullable = true)\n",
      " |-- grade: integer (nullable = true)\n",
      " |-- absenses: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# and the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or just the data types which you can call on as a list\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or just the column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or just the type of one column\n",
    "df.schema['Class/ASD Traits '].dataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gpa</th>\n",
       "      <th>grade</th>\n",
       "      <th>absenses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Carl</td>\n",
       "      <td>Weilsburg</td>\n",
       "      <td>3.6</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Geane</td>\n",
       "      <td>Grabsbury</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lu</td>\n",
       "      <td>Wang</td>\n",
       "      <td>3.5</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Imran</td>\n",
       "      <td>Habibi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Marcos</td>\n",
       "      <td>Araleous</td>\n",
       "      <td>2.3</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    first       last  gpa  grade  absenses\n",
       "0    Carl  Weilsburg  3.6      8         3\n",
       "1   Geane  Grabsbury  4.0      9         0\n",
       "2      Lu       Wang  3.5     10         4\n",
       "3   Imran     Habibi  3.9     12         2\n",
       "4  Marcos   Araleous  2.3     11        15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If your dataframe is more than just a few variables, this method is way better\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Photo</th>\n",
       "      <th>Nationality</th>\n",
       "      <th>Flag</th>\n",
       "      <th>Overall</th>\n",
       "      <th>Potential</th>\n",
       "      <th>Club</th>\n",
       "      <th>Club Logo</th>\n",
       "      <th>Value</th>\n",
       "      <th>Wage</th>\n",
       "      <th>Special</th>\n",
       "      <th>Preferred Foot</th>\n",
       "      <th>International Reputation</th>\n",
       "      <th>Weak Foot</th>\n",
       "      <th>Skill Moves</th>\n",
       "      <th>Work Rate</th>\n",
       "      <th>Body Type</th>\n",
       "      <th>Real Face</th>\n",
       "      <th>Position</th>\n",
       "      <th>Jersey Number</th>\n",
       "      <th>Joined</th>\n",
       "      <th>Loaned From</th>\n",
       "      <th>Contract Valid Until</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>LS</th>\n",
       "      <th>ST</th>\n",
       "      <th>RS</th>\n",
       "      <th>LW</th>\n",
       "      <th>LF</th>\n",
       "      <th>CF</th>\n",
       "      <th>RF</th>\n",
       "      <th>RW</th>\n",
       "      <th>LAM</th>\n",
       "      <th>CAM</th>\n",
       "      <th>RAM</th>\n",
       "      <th>LM</th>\n",
       "      <th>LCM</th>\n",
       "      <th>CM</th>\n",
       "      <th>RCM</th>\n",
       "      <th>RM</th>\n",
       "      <th>LWB</th>\n",
       "      <th>LDM</th>\n",
       "      <th>CDM</th>\n",
       "      <th>RDM</th>\n",
       "      <th>RWB</th>\n",
       "      <th>LB</th>\n",
       "      <th>LCB</th>\n",
       "      <th>CB</th>\n",
       "      <th>RCB</th>\n",
       "      <th>RB</th>\n",
       "      <th>Crossing</th>\n",
       "      <th>Finishing</th>\n",
       "      <th>HeadingAccuracy</th>\n",
       "      <th>ShortPassing</th>\n",
       "      <th>Volleys</th>\n",
       "      <th>Dribbling</th>\n",
       "      <th>Curve</th>\n",
       "      <th>FKAccuracy</th>\n",
       "      <th>LongPassing</th>\n",
       "      <th>BallControl</th>\n",
       "      <th>Acceleration</th>\n",
       "      <th>SprintSpeed</th>\n",
       "      <th>Agility</th>\n",
       "      <th>Reactions</th>\n",
       "      <th>Balance</th>\n",
       "      <th>ShotPower</th>\n",
       "      <th>Jumping</th>\n",
       "      <th>Stamina</th>\n",
       "      <th>Strength</th>\n",
       "      <th>LongShots</th>\n",
       "      <th>Aggression</th>\n",
       "      <th>Interceptions</th>\n",
       "      <th>Positioning</th>\n",
       "      <th>Vision</th>\n",
       "      <th>Penalties</th>\n",
       "      <th>Composure</th>\n",
       "      <th>Marking</th>\n",
       "      <th>StandingTackle</th>\n",
       "      <th>SlidingTackle</th>\n",
       "      <th>GKDiving</th>\n",
       "      <th>GKHandling</th>\n",
       "      <th>GKKicking</th>\n",
       "      <th>GKPositioning</th>\n",
       "      <th>GKReflexes</th>\n",
       "      <th>Release Clause</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>158023</td>\n",
       "      <td>L. Messi</td>\n",
       "      <td>31</td>\n",
       "      <td>https://cdn.sofifa.org/players/4/19/158023.png</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>https://cdn.sofifa.org/flags/52.png</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>FC Barcelona</td>\n",
       "      <td>https://cdn.sofifa.org/teams/2/light/241.png</td>\n",
       "      <td>€110.5M</td>\n",
       "      <td>€565K</td>\n",
       "      <td>2202</td>\n",
       "      <td>Left</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Medium/ Medium</td>\n",
       "      <td>Messi</td>\n",
       "      <td>Yes</td>\n",
       "      <td>RF</td>\n",
       "      <td>10</td>\n",
       "      <td>Jul 1, 2004</td>\n",
       "      <td>None</td>\n",
       "      <td>2021</td>\n",
       "      <td>5'7</td>\n",
       "      <td>159lbs</td>\n",
       "      <td>88+2</td>\n",
       "      <td>88+2</td>\n",
       "      <td>88+2</td>\n",
       "      <td>92+2</td>\n",
       "      <td>93+2</td>\n",
       "      <td>93+2</td>\n",
       "      <td>93+2</td>\n",
       "      <td>92+2</td>\n",
       "      <td>93+2</td>\n",
       "      <td>93+2</td>\n",
       "      <td>93+2</td>\n",
       "      <td>91+2</td>\n",
       "      <td>84+2</td>\n",
       "      <td>84+2</td>\n",
       "      <td>84+2</td>\n",
       "      <td>91+2</td>\n",
       "      <td>64+2</td>\n",
       "      <td>61+2</td>\n",
       "      <td>61+2</td>\n",
       "      <td>61+2</td>\n",
       "      <td>64+2</td>\n",
       "      <td>59+2</td>\n",
       "      <td>47+2</td>\n",
       "      <td>47+2</td>\n",
       "      <td>47+2</td>\n",
       "      <td>59+2</td>\n",
       "      <td>84</td>\n",
       "      <td>95</td>\n",
       "      <td>70</td>\n",
       "      <td>90</td>\n",
       "      <td>86</td>\n",
       "      <td>97</td>\n",
       "      <td>93</td>\n",
       "      <td>94</td>\n",
       "      <td>87</td>\n",
       "      <td>96</td>\n",
       "      <td>91</td>\n",
       "      <td>86</td>\n",
       "      <td>91</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>85</td>\n",
       "      <td>68</td>\n",
       "      <td>72</td>\n",
       "      <td>59</td>\n",
       "      <td>94</td>\n",
       "      <td>48</td>\n",
       "      <td>22</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>75</td>\n",
       "      <td>96</td>\n",
       "      <td>33</td>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>€226.5M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20801</td>\n",
       "      <td>Cristiano Ronaldo</td>\n",
       "      <td>33</td>\n",
       "      <td>https://cdn.sofifa.org/players/4/19/20801.png</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>https://cdn.sofifa.org/flags/38.png</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>Juventus</td>\n",
       "      <td>https://cdn.sofifa.org/teams/2/light/45.png</td>\n",
       "      <td>€77M</td>\n",
       "      <td>€405K</td>\n",
       "      <td>2228</td>\n",
       "      <td>Right</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>High/ Low</td>\n",
       "      <td>C. Ronaldo</td>\n",
       "      <td>Yes</td>\n",
       "      <td>ST</td>\n",
       "      <td>7</td>\n",
       "      <td>Jul 10, 2018</td>\n",
       "      <td>None</td>\n",
       "      <td>2022</td>\n",
       "      <td>6'2</td>\n",
       "      <td>183lbs</td>\n",
       "      <td>91+3</td>\n",
       "      <td>91+3</td>\n",
       "      <td>91+3</td>\n",
       "      <td>89+3</td>\n",
       "      <td>90+3</td>\n",
       "      <td>90+3</td>\n",
       "      <td>90+3</td>\n",
       "      <td>89+3</td>\n",
       "      <td>88+3</td>\n",
       "      <td>88+3</td>\n",
       "      <td>88+3</td>\n",
       "      <td>88+3</td>\n",
       "      <td>81+3</td>\n",
       "      <td>81+3</td>\n",
       "      <td>81+3</td>\n",
       "      <td>88+3</td>\n",
       "      <td>65+3</td>\n",
       "      <td>61+3</td>\n",
       "      <td>61+3</td>\n",
       "      <td>61+3</td>\n",
       "      <td>65+3</td>\n",
       "      <td>61+3</td>\n",
       "      <td>53+3</td>\n",
       "      <td>53+3</td>\n",
       "      <td>53+3</td>\n",
       "      <td>61+3</td>\n",
       "      <td>84</td>\n",
       "      <td>94</td>\n",
       "      <td>89</td>\n",
       "      <td>81</td>\n",
       "      <td>87</td>\n",
       "      <td>88</td>\n",
       "      <td>81</td>\n",
       "      <td>76</td>\n",
       "      <td>77</td>\n",
       "      <td>94</td>\n",
       "      <td>89</td>\n",
       "      <td>91</td>\n",
       "      <td>87</td>\n",
       "      <td>96</td>\n",
       "      <td>70</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>88</td>\n",
       "      <td>79</td>\n",
       "      <td>93</td>\n",
       "      <td>63</td>\n",
       "      <td>29</td>\n",
       "      <td>95</td>\n",
       "      <td>82</td>\n",
       "      <td>85</td>\n",
       "      <td>95</td>\n",
       "      <td>28</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>€127.1M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>190871</td>\n",
       "      <td>Neymar Jr</td>\n",
       "      <td>26</td>\n",
       "      <td>https://cdn.sofifa.org/players/4/19/190871.png</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>https://cdn.sofifa.org/flags/54.png</td>\n",
       "      <td>92</td>\n",
       "      <td>93</td>\n",
       "      <td>Paris Saint-Germain</td>\n",
       "      <td>https://cdn.sofifa.org/teams/2/light/73.png</td>\n",
       "      <td>€118.5M</td>\n",
       "      <td>€290K</td>\n",
       "      <td>2143</td>\n",
       "      <td>Right</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>High/ Medium</td>\n",
       "      <td>Neymar</td>\n",
       "      <td>Yes</td>\n",
       "      <td>LW</td>\n",
       "      <td>10</td>\n",
       "      <td>Aug 3, 2017</td>\n",
       "      <td>None</td>\n",
       "      <td>2022</td>\n",
       "      <td>5'9</td>\n",
       "      <td>150lbs</td>\n",
       "      <td>84+3</td>\n",
       "      <td>84+3</td>\n",
       "      <td>84+3</td>\n",
       "      <td>89+3</td>\n",
       "      <td>89+3</td>\n",
       "      <td>89+3</td>\n",
       "      <td>89+3</td>\n",
       "      <td>89+3</td>\n",
       "      <td>89+3</td>\n",
       "      <td>89+3</td>\n",
       "      <td>89+3</td>\n",
       "      <td>88+3</td>\n",
       "      <td>81+3</td>\n",
       "      <td>81+3</td>\n",
       "      <td>81+3</td>\n",
       "      <td>88+3</td>\n",
       "      <td>65+3</td>\n",
       "      <td>60+3</td>\n",
       "      <td>60+3</td>\n",
       "      <td>60+3</td>\n",
       "      <td>65+3</td>\n",
       "      <td>60+3</td>\n",
       "      <td>47+3</td>\n",
       "      <td>47+3</td>\n",
       "      <td>47+3</td>\n",
       "      <td>60+3</td>\n",
       "      <td>79</td>\n",
       "      <td>87</td>\n",
       "      <td>62</td>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>96</td>\n",
       "      <td>88</td>\n",
       "      <td>87</td>\n",
       "      <td>78</td>\n",
       "      <td>95</td>\n",
       "      <td>94</td>\n",
       "      <td>90</td>\n",
       "      <td>96</td>\n",
       "      <td>94</td>\n",
       "      <td>84</td>\n",
       "      <td>80</td>\n",
       "      <td>61</td>\n",
       "      <td>81</td>\n",
       "      <td>49</td>\n",
       "      <td>82</td>\n",
       "      <td>56</td>\n",
       "      <td>36</td>\n",
       "      <td>89</td>\n",
       "      <td>87</td>\n",
       "      <td>81</td>\n",
       "      <td>94</td>\n",
       "      <td>27</td>\n",
       "      <td>24</td>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>€228.1M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>193080</td>\n",
       "      <td>De Gea</td>\n",
       "      <td>27</td>\n",
       "      <td>https://cdn.sofifa.org/players/4/19/193080.png</td>\n",
       "      <td>Spain</td>\n",
       "      <td>https://cdn.sofifa.org/flags/45.png</td>\n",
       "      <td>91</td>\n",
       "      <td>93</td>\n",
       "      <td>Manchester United</td>\n",
       "      <td>https://cdn.sofifa.org/teams/2/light/11.png</td>\n",
       "      <td>€72M</td>\n",
       "      <td>€260K</td>\n",
       "      <td>1471</td>\n",
       "      <td>Right</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Medium/ Medium</td>\n",
       "      <td>Lean</td>\n",
       "      <td>Yes</td>\n",
       "      <td>GK</td>\n",
       "      <td>1</td>\n",
       "      <td>Jul 1, 2011</td>\n",
       "      <td>None</td>\n",
       "      <td>2020</td>\n",
       "      <td>6'4</td>\n",
       "      <td>168lbs</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>50</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>51</td>\n",
       "      <td>42</td>\n",
       "      <td>57</td>\n",
       "      <td>58</td>\n",
       "      <td>60</td>\n",
       "      <td>90</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "      <td>67</td>\n",
       "      <td>43</td>\n",
       "      <td>64</td>\n",
       "      <td>12</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>68</td>\n",
       "      <td>40</td>\n",
       "      <td>68</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>90</td>\n",
       "      <td>85</td>\n",
       "      <td>87</td>\n",
       "      <td>88</td>\n",
       "      <td>94</td>\n",
       "      <td>€138.6M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _c0      ID               Name  Age  \\\n",
       "0    0  158023           L. Messi   31   \n",
       "1    1   20801  Cristiano Ronaldo   33   \n",
       "2    2  190871          Neymar Jr   26   \n",
       "3    3  193080             De Gea   27   \n",
       "\n",
       "                                            Photo Nationality  \\\n",
       "0  https://cdn.sofifa.org/players/4/19/158023.png   Argentina   \n",
       "1   https://cdn.sofifa.org/players/4/19/20801.png    Portugal   \n",
       "2  https://cdn.sofifa.org/players/4/19/190871.png      Brazil   \n",
       "3  https://cdn.sofifa.org/players/4/19/193080.png       Spain   \n",
       "\n",
       "                                  Flag  Overall  Potential  \\\n",
       "0  https://cdn.sofifa.org/flags/52.png       94         94   \n",
       "1  https://cdn.sofifa.org/flags/38.png       94         94   \n",
       "2  https://cdn.sofifa.org/flags/54.png       92         93   \n",
       "3  https://cdn.sofifa.org/flags/45.png       91         93   \n",
       "\n",
       "                  Club                                     Club Logo    Value  \\\n",
       "0         FC Barcelona  https://cdn.sofifa.org/teams/2/light/241.png  €110.5M   \n",
       "1             Juventus   https://cdn.sofifa.org/teams/2/light/45.png     €77M   \n",
       "2  Paris Saint-Germain   https://cdn.sofifa.org/teams/2/light/73.png  €118.5M   \n",
       "3    Manchester United   https://cdn.sofifa.org/teams/2/light/11.png     €72M   \n",
       "\n",
       "    Wage  Special Preferred Foot  International Reputation  Weak Foot  \\\n",
       "0  €565K     2202           Left                         5          4   \n",
       "1  €405K     2228          Right                         5          4   \n",
       "2  €290K     2143          Right                         5          5   \n",
       "3  €260K     1471          Right                         4          3   \n",
       "\n",
       "   Skill Moves       Work Rate   Body Type Real Face Position  Jersey Number  \\\n",
       "0            4  Medium/ Medium       Messi       Yes       RF             10   \n",
       "1            5       High/ Low  C. Ronaldo       Yes       ST              7   \n",
       "2            5    High/ Medium      Neymar       Yes       LW             10   \n",
       "3            1  Medium/ Medium        Lean       Yes       GK              1   \n",
       "\n",
       "         Joined Loaned From Contract Valid Until Height  Weight    LS    ST  \\\n",
       "0   Jul 1, 2004        None                 2021    5'7  159lbs  88+2  88+2   \n",
       "1  Jul 10, 2018        None                 2022    6'2  183lbs  91+3  91+3   \n",
       "2   Aug 3, 2017        None                 2022    5'9  150lbs  84+3  84+3   \n",
       "3   Jul 1, 2011        None                 2020    6'4  168lbs  None  None   \n",
       "\n",
       "     RS    LW    LF    CF    RF    RW   LAM   CAM   RAM    LM   LCM    CM  \\\n",
       "0  88+2  92+2  93+2  93+2  93+2  92+2  93+2  93+2  93+2  91+2  84+2  84+2   \n",
       "1  91+3  89+3  90+3  90+3  90+3  89+3  88+3  88+3  88+3  88+3  81+3  81+3   \n",
       "2  84+3  89+3  89+3  89+3  89+3  89+3  89+3  89+3  89+3  88+3  81+3  81+3   \n",
       "3  None  None  None  None  None  None  None  None  None  None  None  None   \n",
       "\n",
       "    RCM    RM   LWB   LDM   CDM   RDM   RWB    LB   LCB    CB   RCB    RB  \\\n",
       "0  84+2  91+2  64+2  61+2  61+2  61+2  64+2  59+2  47+2  47+2  47+2  59+2   \n",
       "1  81+3  88+3  65+3  61+3  61+3  61+3  65+3  61+3  53+3  53+3  53+3  61+3   \n",
       "2  81+3  88+3  65+3  60+3  60+3  60+3  65+3  60+3  47+3  47+3  47+3  60+3   \n",
       "3  None  None  None  None  None  None  None  None  None  None  None  None   \n",
       "\n",
       "   Crossing  Finishing  HeadingAccuracy  ShortPassing  Volleys  Dribbling  \\\n",
       "0        84         95               70            90       86         97   \n",
       "1        84         94               89            81       87         88   \n",
       "2        79         87               62            84       84         96   \n",
       "3        17         13               21            50       13         18   \n",
       "\n",
       "   Curve  FKAccuracy  LongPassing  BallControl  Acceleration  SprintSpeed  \\\n",
       "0     93          94           87           96            91           86   \n",
       "1     81          76           77           94            89           91   \n",
       "2     88          87           78           95            94           90   \n",
       "3     21          19           51           42            57           58   \n",
       "\n",
       "   Agility  Reactions  Balance  ShotPower  Jumping  Stamina  Strength  \\\n",
       "0       91         95       95         85       68       72        59   \n",
       "1       87         96       70         95       95       88        79   \n",
       "2       96         94       84         80       61       81        49   \n",
       "3       60         90       43         31       67       43        64   \n",
       "\n",
       "   LongShots  Aggression  Interceptions  Positioning  Vision  Penalties  \\\n",
       "0         94          48             22           94      94         75   \n",
       "1         93          63             29           95      82         85   \n",
       "2         82          56             36           89      87         81   \n",
       "3         12          38             30           12      68         40   \n",
       "\n",
       "   Composure  Marking  StandingTackle  SlidingTackle  GKDiving  GKHandling  \\\n",
       "0         96       33              28             26         6          11   \n",
       "1         95       28              31             23         7          11   \n",
       "2         94       27              24             33         9           9   \n",
       "3         68       15              21             13        90          85   \n",
       "\n",
       "   GKKicking  GKPositioning  GKReflexes Release Clause  \n",
       "0         15             14           8        €226.5M  \n",
       "1         15             14          11        €127.1M  \n",
       "2         15             15          11        €228.1M  \n",
       "3         87             88          94        €138.6M  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To display ALL columns accross a huge dataset\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "path = 'Datasets/'\n",
    "fifa = spark.read.csv('Datasets/fifa19.csv',inferSchema=True,header=True)\n",
    "\n",
    "# Take a look at the first few lines\n",
    "fifa.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               gpa|\n",
      "+-------+------------------+\n",
      "|  count|                 7|\n",
      "|   mean| 3.457142857142857|\n",
      "| stddev|0.5740416444159863|\n",
      "|    min|               2.3|\n",
      "|    max|               4.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Neat \"describe\" function\n",
    "df.describe(['gpa']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+\n",
      "|summary|gpa|grade|\n",
      "+-------+---+-----+\n",
      "|  count|  7|    7|\n",
      "|    min|2.3|    8|\n",
      "|    25%|3.2|    9|\n",
      "|    75%|3.9|   12|\n",
      "|    max|4.0|   12|\n",
      "+-------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary function\n",
    "df.select(\"gpa\", \"grade\").summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify data types as you read in datasets.\n",
    "\n",
    "Some data types make it easier to infer schema (like tabular formats such as csv which we will show later). \n",
    "\n",
    "However you often have to set the schema yourself if you aren't dealing with a .read method that doesn't have inferSchema() built-in.\n",
    "\n",
    "Spark has all the tools you need for this, it just requires a very specific structure.\n",
    "\n",
    "I've also included Spark's link to their latest list of data types for your reference in case you need it: https://spark.apache.org/docs/latest/sql-reference.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import * #StructField,StringType,IntegerType,StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [StructField(\"age\", IntegerType(), True),StructField(\"name\", StringType(), True)]\n",
    "final_struc = StructType(fields=data_schema)\n",
    "people = spark.read.json(path+'people.json', schema=final_struc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Data\n",
    "\n",
    "CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the funky naming convention of the file in your output folder. There is no way to directly change this. \n",
    "df.write.mode(\"overwrite\").csv('write_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parquet files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"parquet/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those who got an error attempting to run the above code. Try this solution: https://stackoverflow.com/questions/59220832/unable-to-write-spark-dataframe-to-a-parquet-file-format-to-c-drive-in-pyspark\n",
    "\n",
    "#### Writting Partitioned Parquet Files\n",
    "\n",
    "Best practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").partitionBy(\"grade\").parquet(\"partitioned_parquet/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writting your own dataframes in Jupyter Notebooks!\n",
    "\n",
    "You can also create your own dataframes directly here in your Juypter Notebook too if you want. \n",
    "\n",
    "Like this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [('Pear',10),('Orange',36),('Banana',123),('Kiwi',48),('Peach',16),('Strawberry',1)]\n",
    "df = spark.createDataFrame(values,['fruit','quantity'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Select**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "| first|     last|\n",
      "+------+---------+\n",
      "|  Carl|Weilsburg|\n",
      "| Geane|Grabsbury|\n",
      "|    Lu|     Wang|\n",
      "| Imran|   Habibi|\n",
      "|Marcos| Araleous|\n",
      "+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['first','last']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Order By**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---+\n",
      "|  first|     last|gpa|\n",
      "+-------+---------+---+\n",
      "| Marcos| Araleous|2.3|\n",
      "|William|     Penn|3.2|\n",
      "|     Lu|     Wang|3.5|\n",
      "|   Carl|Weilsburg|3.6|\n",
      "| Hector|     Liam|3.7|\n",
      "+-------+---------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['first','last','gpa']).orderBy(\"gpa\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Order By Descending**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---+\n",
      "| first|     last|gpa|\n",
      "+------+---------+---+\n",
      "| Geane|Grabsbury|4.0|\n",
      "| Imran|   Habibi|3.9|\n",
      "|Hector|     Liam|3.7|\n",
      "|  Carl|Weilsburg|3.6|\n",
      "|    Lu|     Wang|3.5|\n",
      "+------+---------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['first','last','gpa']).orderBy(df[\"gpa\"].desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Like**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('first','last').where(df.first.like(\"%I%\")).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Substrings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------+\n",
      "|last     |substring(last, 1, 3)|\n",
      "+---------+---------------------+\n",
      "|Weilsburg|Wei                  |\n",
      "|Grabsbury|Gra                  |\n",
      "|Wang     |Wan                  |\n",
      "|Habibi   |Hab                  |\n",
      "|Araleous |Ara                  |\n",
      "+---------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"last\",df.last.substr(1,3)).show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IS IN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.first.isin(\"Imran\",\"Lu\")].limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Starts with Ends with**\n",
    "\n",
    "Search for a specific case - begins with \"x\" and ends with \"x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"first\",\"last\").where(df.first.startswith(\"I\")) \\\n",
    "                                  .where(df.first.endswith(\"n\")).limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Slicing**\n",
    "\n",
    "pyspark.sql.functions.slice(x, start, length)[source] <br>\n",
    "Returns an array containing all the elements in x from index start (or starting from the end if start is negative) with the specified length.  <br>\n",
    "<br>\n",
    "*Note: indexing starts at 1 here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|           x|\n",
      "+------------+\n",
      "|[1, 2, 3, 4]|\n",
      "|[5, 6, 7, 8]|\n",
      "+------------+\n",
      "\n",
      "+---------------+\n",
      "|Middle 2 Values|\n",
      "+---------------+\n",
      "|         [2, 3]|\n",
      "|         [6, 7]|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import slice\n",
    "\n",
    "df = spark.createDataFrame([([1, 2, 3, 4],), ([5, 6, 7, 8],)], ['x']) \n",
    "df.show()\n",
    "df.select(slice(df.x, 2, 2).alias(\"Middle 2 Values\")).show() #first number is starting index (index starts with 1), second number is for how many"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to just slice your dataframe you can do this...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting\n",
    "print('Starting row cound:',df.count())\n",
    "print('Starting column count:',len(df.columns))\n",
    "\n",
    "# Slice rows\n",
    "df2 = df.limit(300)\n",
    "print('Sliced row count:',df2.count())\n",
    "\n",
    "# Slice columns\n",
    "cols_list = df.columns[0:5]\n",
    "df3 = df.select(cols_list)\n",
    "print('Sliced column count:',len(df3.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Data\n",
    "\n",
    "A large part of working with DataFrames is the ability to quickly filter out data based on conditions. Spark DataFrames are built on top of the Spark SQL platform, which means that is you already know SQL, you can quickly and easily grab that data using SQL commands, or using the DataFram methods (which is what we focus on in this course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifa.filter(\"Overall>50\").limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SQL with .select()\n",
    "fifa.filter(\"Overall>50\").select(['ID','Name','Nationality','Overall']).limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Results as Python Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting results as Python objects\n",
    "# you need the \".collect()\" call at the end to \"collect\" the results\n",
    "result = fifa.select(['Nationality','Name','Age','Overall']).filter(\"Overall>70\").orderBy(fifa[\"Overall\"].desc()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Player Over 70: \",result[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows can also be called to turn into dictionaries if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row.asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in result[0]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Options in Spark\n",
    "\n",
    "### Spark SQL\n",
    "\n",
    "Spark TempView provides two functions that allow users to run SQL queries against a Spark DataFrame:\n",
    "\n",
    "createOrReplaceTempView: The lifetime of this temporary view is tied to the [[SparkSession]] that was used to create this Dataset. It creates (or replaces if that view name already exists) a lazily evaluated \"view\" that you can then use like a hive table in Spark SQL. It does not persist to memory unless you cache the dataset that underpins the view.\n",
    "\n",
    "createGlobalTempView: The lifetime of this temporary view is tied to this Spark application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view of the dataframe\n",
    "df.createOrReplaceTempView(\"tempview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then Query the temp view\n",
    "spark.sql(\"SELECT * FROM tempview WHERE Count > 1000\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or pass it to an object\n",
    "sql_results = spark.sql(\"SELECT * FROM tempview WHERE Count > 1000 AND Region='South West'\")\n",
    "sql_results.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT Region, sum(Count) AS Total FROM tempview GROUP BY Region\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Transformer\n",
    "\n",
    "You also have the option to use the SQL transformer option where you can write freeform SQL scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to import SQL transformer\n",
    "from pyspark.ml.feature import SQLTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlTrans = SQLTransformer(\n",
    "    statement=\"SELECT PFA,Region,Offence FROM __THIS__\") \n",
    "sqlTrans.transform(df).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlTrans = SQLTransformer(\n",
    "    statement=\"SELECT Offence, SUM(Count) as Total FROM __THIS__ GROUP BY Offence\") \n",
    "sqlTrans.transform(df).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupBy and Aggregate Functions\n",
    "\n",
    "Let's learn how to use GroupBy and Aggregate methods on a DataFrame. GroupBy allows you to group rows together based off some column value, for example, you could group together sales data by the day the sale occured, or group repeast customer data based off the name of the customer. Once you've performed the GroupBy operation you can use an aggregate function off that data. An aggregate function aggregates multiple rows of data into a single output, such as taking the sum of inputs, or counting the number of inputs.\n",
    "\n",
    "Let's see some examples on an example dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby Function with count (you can also use sum, min, max)\n",
    "df.groupBy(\"neighbourhood_group\").count().show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then you can add the following aggregate functions: mean, count, min, max, sum\n",
    "# Like this for example\n",
    "df.groupBy(\"neighbourhood_group\").mean(\"price\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is also a pretty neat function you can use:\n",
    "summary = df.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\")\n",
    "summary.toPandas()\n",
    "\n",
    "# or a prettier version\n",
    "limit_summary = df.select(\"price\",\"minimum_nights\",\"number_of_reviews\",\"last_review\",\"reviews_per_month\",\"calculated_host_listings_count\",\"availability_365\").summary(\"count\",\"min\",\"max\")\n",
    "limit_summary.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's another way of doing it\n",
    "df.select(countDistinct(\"neighbourhood_group\"),avg('price'),stddev(\"price\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aggregate on the entire DataFrame without groups (shorthand for df.groupBy.agg()).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate!\n",
    "# agg(*exprs)\n",
    "# Aggregate on the entire DataFrame without groups (shorthand for df.groupBy.agg()).\n",
    "# available agg functions: min, max, count, countDistinct, approx_count_distinct\n",
    "# df.agg.(covar_pop(col1, col2)) Returns a new Column for the population covariance of col1 and col2\n",
    "# df.agg.(covar_samp(col1, col2)) Returns a new Column for the sample covariance of col1 and col2.\n",
    "# df.agg(corr(col1, col2)) Returns a new Column for the Pearson Correlation Coefficient for col1 and col2.\n",
    "from pyspark.sql import functions as F\n",
    "df.agg(F.min(df.price).alias(\"Min Price\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|covar_pop(math score, writing score)|\n",
      "+------------------------------------+\n",
      "|                  184.75419400000004|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(F.covar_pop(df[\"math score\"],df[\"writing score\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|min(math score)|\n",
      "+---------------+\n",
      "|              0|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(F.min(df[\"math score\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max sales across everything\n",
    "df.agg({'number_of_reviews':'max'}).withColumnRenamed(\"max(number_of_reviews)\", \"Max Reviews\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And then if you want to group by you can do this:\n",
    "df.groupBy(\"neighbourhood\").agg({'number_of_reviews':'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|gender|min(math score)|\n",
      "+------+---------------+\n",
      "|female|              0|\n",
      "|  male|             27|\n",
      "+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"gender\").agg(F.min(df[\"math score\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GroupedData' object has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-50ddd2cbd6b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gender\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"math score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'GroupedData' object has no attribute 'select'"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"gender\").select(F.min(df[\"math score\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pivot Function**\n",
    "\n",
    "Provides a two way table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot Function\n",
    "# pivot(pivot_col, values=None)\n",
    "df.filter(\"room_type='Shared room'\").groupBy(\"room_type\").pivot(\"neighbourhood_group\", [\"Queens\", \"Brooklyn\"]).count().show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining and Appending DataFrames in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesA = [('Pirate',1,'Arrrg'),('Monkey',2,'Oooo'),('Ninja',3,'Yaaaa'),('Spaghetti',4,'Slurp!')]\n",
    "TableA = spark.createDataFrame(valuesA,['name','id','sound'])\n",
    "\n",
    "valuesB = [('Rutabaga',1,2),('Pirate',2,45),('Ninja',3,102),('Darth Vader',4,87)]\n",
    "TableB = spark.createDataFrame(valuesB,['name','id','age'])\n",
    "\n",
    "print(\"This is TableA\")\n",
    "print(TableA.show())\n",
    "print(\"And this is TableB\")\n",
    "print(TableB.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appends\n",
    "\n",
    "Appending \"appends\" two dataframes together that have the exact same variables. You can think of it like stacking two or more blocks ON TOP of each other. To demonstrate this, we will simply join the same dataframe to itself since we don't really have a use case for this with our courses database. But hopefully this will help you imagine what to do. \n",
    "\n",
    "A common usecase would be joining the same table of infomation from one year to another year (i.e. 2012 + 2013 + ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = TableA\n",
    "df_concat = TableA.union(new_df)\n",
    "print((\"TableA df Counts:\", TableA.count(), len(c)))\n",
    "print((\"df_concat Counts:\", df_concat.count(), len(df_concat.columns)))\n",
    "print(TableA.show(5))\n",
    "print(df_concat.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins!\n",
    "\n",
    "All options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_join = TableA.join(TableB, [\"name\",\"id\"],\"inner\")\n",
    "print(\"Inner Join Example\")\n",
    "print(inner_join.show())\n",
    "\n",
    "left_join = TableA.join(TableB, [\"name\",\"id\"], how='left') # Could also use 'left_outer'\n",
    "print(\"Left Join Example\")\n",
    "print(left_join.show())\n",
    "\n",
    "conditional_join = TableA.join(TableB, [\"name\",\"id\"], how='left').filter(TableB.name.isNull())\n",
    "print(\"Conditional Left Join\")\n",
    "print(conditional_join.show())\n",
    "\n",
    "right_join = TableA.join(TableB,  [\"name\",\"id\"],how='right') # Could also use 'right_outer'\n",
    "print(\"Right Join\")\n",
    "print(right_join.show())\n",
    "\n",
    "full_outer_join = TableA.join(TableB, [\"name\",\"id\"],how='full') # Could also use 'full_outer'\n",
    "print(\"Full Outer Join\")\n",
    "print(full_outer_join.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.cuisines.isNull()).select(['name','cuisines']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Missing Data Statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "nulls = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "percent = df.select([format_number(((count(when(isnan(c) | col(c).isNull(), c))/df.count())*100),1).alias(c) for c in df.columns])\n",
    "\n",
    "result = nulls.union(percent)\n",
    "\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "def null_value_count(df):\n",
    "    null_columns_counts = []\n",
    "    numRows = df.count()\n",
    "    for k in df.columns:\n",
    "        nullRows = df.where(col(k).isNull()).count()\n",
    "        if(nullRows > 0):\n",
    "            temp = k,nullRows\n",
    "            null_columns_counts.append(temp)\n",
    "    return(null_columns_counts)\n",
    "\n",
    "null_columns_count_list = null_value_count(df)\n",
    "spark.createDataFrame(null_columns_count_list, ['Column_With_Null_Value', 'Null_Values_Count']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop all missing data**\n",
    "\n",
    "PySpark has a really handy .na function for working with missing data. The drop command has the following parameters:\n",
    "\n",
    "    df.na.drop(how='any', thresh=None, subset=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.drop().limit(4).toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of course you will want to know how many rows that affected before you actually execute it..\n",
    "og_len = df.count()\n",
    "drop_len = df.na.drop().count()\n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that have at least 8 NON-null values\n",
    "og_len = df.count()\n",
    "drop_len = df.na.drop(thresh=8).count()\n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only drop the rows whose values in the sales column are null\n",
    "og_len = df.count()\n",
    "drop_len = df.na.drop(subset=[\"rate\"]).count() \n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to do the above\n",
    "og_len = df.count()\n",
    "drop_len = df.filter(zomato.rate.isNotNull()).count() \n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a row only if ALL its values are null.\n",
    "og_len = df.count()\n",
    "drop_len = df.na.drop(how='all').count() \n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill the missing values\n",
    "\n",
    "We can also fill the missing values with new values. If you have multiple nulls across multiple data types, Spark is actually smart enough to match up the data types. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill all nulls values with one common value (character value)\n",
    "df.na.fill('MISSING').limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill all nulls values with one common value (numeric value)\n",
    "df.na.fill(999).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually you should specify what columns you want to fill with the subset parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.name.isNull()).na.fill('No Name',subset=['name']).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very common practice is to fill values with the mean value for the column. Here is a fun function to that in an automatted way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_mean(df, include=set()): \n",
    "    stats = df.agg(*(\n",
    "        avg(c).alias(c) for c in df.columns if c in include\n",
    "    ))\n",
    "#     stats = stats.select(*(col(c).cast(\"int\").alias(c) for c in stats.columns)) #IntegerType()\n",
    "    return df.na.fill(stats.first().asDict())\n",
    "\n",
    "updated_df = fill_with_mean(df, [\"approx_cost(for two people)\",\"votes\"])\n",
    "updated_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating Data in DataFrames\n",
    "\n",
    "Change data types\n",
    "\n",
    "### Available types:\n",
    "    - DataType\n",
    "    - NullType\n",
    "    - StringType\n",
    "    - BinaryType\n",
    "    - BooleanType\n",
    "    - DateType\n",
    "    - TimestampType\n",
    "    - DecimalType\n",
    "    - DoubleType\n",
    "    - FloatType\n",
    "    - ByteType\n",
    "    - IntegerType\n",
    "    - LongType\n",
    "    - ShortType\n",
    "    - ArrayType\n",
    "    - MapType\n",
    "    - StructField\n",
    "    - StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "df = videos.withColumn(\"views\", videos[\"views\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"likes\", videos[\"likes\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"dislikes\", videos[\"dislikes\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"trending_date\", to_date(videos.trending_date, 'dd.mm.yy'))\n",
    "#         .withColumn(\"publish_time\", to_timestamp(videos.publish_time, 'yyyy-MM-dd HH:mm:ss:ms'))\n",
    "print(df.printSchema())\n",
    "df.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regex**\n",
    "\n",
    "Regex is used to replace or extract all substrings of the specified string value that match regexp with rep.\n",
    "regexp_replace(str, pattern, replacement)\n",
    "for more info on regex calls visit: https://docs.oracle.com/cd/B19306_01/server.102/b14200/ap_posix001.htm#BABJDBHB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, regexp_extract\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "df = df.withColumn('publish_time',regexp_replace(df.publish_time, 'T', ' '))\n",
    "df = df.withColumn('publish_time',regexp_replace(df.publish_time, 'Z', ''))\n",
    "df = df.withColumn(\"publish_time\", to_timestamp(df.publish_time, 'yyyy-MM-dd HH:mm:ss.SSS'))\n",
    "print(df.printSchema())\n",
    "df.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translate Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use the translate function for cases like this \n",
    "# where you wanted to replace ('$', '#', ',') with ('X', 'Y', 'Z')\n",
    "import pyspark.sql.functions as f\n",
    "foobar = spark.createDataFrame([(\"$100,00\",),(\"#foobar\",),(\"foo, bar, #, and $\",)], [\"A\"])\n",
    "foobar.select(\"A\", f.translate(f.col(\"A\"), \"$#,\", \"XYZ\").alias(\"replaced\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim\n",
    "# pyspark.sql.functions.trim(col) - Trim the spaces from both ends for the specified string column.\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "trim_ex = spark.createDataFrame([(' 2015-04-08 ',' 2015-05-10 ')], ['d1', 'd2']) # create a dataframe - notice the extra whitespaces in the date strings\n",
    "trim_ex.show()\n",
    "print(\"left trim\")\n",
    "trim_ex.select('d1', ltrim(trim_ex.d1)).show()\n",
    "print(\"right trim\")\n",
    "trim_ex.select('d1', rtrim(trim_ex.d1)).show()\n",
    "print(\"trim\")\n",
    "trim_ex.select('d1', trim(trim_ex.d1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case When**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Dataframe:\n",
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  1|    1|\n",
      "|  2|    2|\n",
      "|  3|    3|\n",
      "+---+-----+\n",
      "\n",
      "Option#1: withColumn() using when-otherwise\n",
      "+---+-----+----------+\n",
      "| id|value|value_desc|\n",
      "+---+-----+----------+\n",
      "|  1|    1|       one|\n",
      "|  2|    2|       two|\n",
      "|  3|    3|     other|\n",
      "+---+-----+----------+\n",
      "\n",
      "Option2: withColumn() using expr function\n",
      "+---+-----+----------+\n",
      "| id|value|value_desc|\n",
      "+---+-----+----------+\n",
      "|  1|    1|       one|\n",
      "|  2|    2|       two|\n",
      "|  3|    3|     other|\n",
      "+---+-----+----------+\n",
      "\n",
      "Option 3: selectExpr() using SQL equivalent CASE expression\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fifa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c1c2bfd047a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Option 3: selectExpr() using SQL equivalent CASE expression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mfifa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"CASE WHEN value == 1 THEN  'one' WHEN value == 2 THEN  'two' ELSE 'other' END AS value_desc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Option 4: select() using expr function\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fifa' is not defined"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1,1),(2,2),(3,3)],['id','value'])\n",
    "\n",
    "print(\"Sample Dataframe:\")\n",
    "df.show()\n",
    "\n",
    "print(\"Option#1: withColumn() using when-otherwise\")\n",
    "from pyspark.sql.functions import when\n",
    "df.withColumn(\"value_desc\",when(df.value == 1, 'one').when(df.value == 2, 'two').otherwise('other')).show()\n",
    "\n",
    "print(\"Option2: withColumn() using expr function\")\n",
    "from pyspark.sql.functions import expr \n",
    "df.withColumn(\"value_desc\",expr(\"CASE WHEN value == 1 THEN  'one' WHEN value == 2 THEN  'two' ELSE 'other' END AS value_desc\")).show()\n",
    "\n",
    "print(\"Option 3: selectExpr() using SQL equivalent CASE expression\")\n",
    "fifa.selectExpr(\"*\",\"CASE WHEN value == 1 THEN  'one' WHEN value == 2 THEN  'two' ELSE 'other' END AS value_desc\").show()\n",
    "\n",
    "print(\"Option 4: select() using expr function\")\n",
    "from pyspark.sql.functions import expr \n",
    "df.select(\"*\",expr(\"CASE WHEN value == 1 THEN  'one' WHEN value == 2 THEN  'two' ELSE 'other' END AS value_desc\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  1|    1|\n",
      "|  2|    2|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use in between to filter values\n",
    "df = spark.createDataFrame([(1,1),(2,2),(3,3)],['id','value'])\n",
    "df.filter(df.value.between(1,2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating new columns calculated using existing columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column from an existing column like this....\n",
    "# withColumn(colName, col)[source]\n",
    "# Returns a new DataFrame by adding a column or replacing the existing column that has the same name.\n",
    "# The column expression must be an expression over this DataFrame; attempting to add a column from some other dataframe will raise an error.\n",
    "\n",
    "# Parameters\n",
    "# colName – string, name of the new column.\n",
    "\n",
    "# col – a Column expression for the new column.\n",
    "views = df.withColumn('views_x_2', df.views * 2)\n",
    "views.select(['views','views_x_2']).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use this method to overwrite a column\n",
    "views = views.withColumn('views', views.views * 2)\n",
    "views.select(['views','views_x_2']).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Renaming Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Rename\n",
    "renamed = df.withColumnRenamed('channel_title','channel_title_new')\n",
    "renamed.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concatenate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import * #IntegerType\n",
    "\n",
    "# Concatenate columns\n",
    "# pyspark.sql.functions.concat_ws(sep, *cols)[source]\n",
    "# Concatenates multiple input string columns together into a single string column, using the given separator.\n",
    "\n",
    "names = spark.createDataFrame([('Abraham','Lincoln')], ['first_name', 'last_name'])\n",
    "names.select(names.first_name,names.last_name,concat_ws(' ', names.first_name, names.last_name).alias('full_name')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting from Date and Timestamp variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year, month, day etc. from a date field\n",
    "# Other options: dayofmonth, dayofweek, dayofyear, weekofyear\n",
    "import pyspark.sql.functions as fn\n",
    "year = df.withColumn(\"TRENDING_YEAR\",fn.year(\"trending_date\")) \\\n",
    "         .withColumn(\"TRENDING_MONTH\",fn.month(\"trending_date\"))\n",
    "#QA\n",
    "year.filter(\"TRENDING_YEAR=2011\").select(['trending_date','TRENDING_YEAR','TRENDING_MONTH']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference between two dates:\n",
    "# pyspark.sql.functions.datediff(end, start)\n",
    "# Returns the number of days from start to end.\n",
    "\n",
    "date_df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n",
    "date_df.select(datediff(date_df.d2, date_df.d1).alias('diff')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting a string around a pattern**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a string around pattern (pattern is a regular expression).\n",
    "from pyspark.sql.functions import *\n",
    "# pyspark.sql.functions.split(str, pattern)[source]\n",
    "\n",
    "abc = spark.createDataFrame([('ab12cd',)], ['s',])\n",
    "abc.select(abc.s,split(abc.s, '[0-9]+').alias('news')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arrays**\n",
    "\n",
    "*Note that the array_distinct feature is new in Spark 2.4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrays - col/cols – list of column names (string) or list of Column expressions that have the same data type.\n",
    "# pyspark.sql.functions\n",
    "# note this is only available in pyspark 2.4+\n",
    "from pyspark.sql.functions import *\n",
    "#      .array(*cols)   -   Creates a new array column.\n",
    "#      .array_contains(col, value)  - Collection function: returns null if the array is null, true if the array contains the given value, and false otherwise.\n",
    "#      .array_distinct(col) - Collection function: removes duplicate values from the array. :param col: name of column or expression\n",
    "#      .array_except(col1, col2) - Collection function: returns an array of the elements in col1 but not in col2, without duplicates.\n",
    "#      .array_intersect(col1, col2) - Collection function: returns an array of the elements in the intersection of col1 and col2, without duplicates.\n",
    "#      .array_join(col, delimiter, null_replacement=None) - Concatenates the elements of column using the delimiter. Null values are replaced with null_replacement if set, otherwise they are ignored.\n",
    "#      .array_max(col) - Collection function: returns the maximum value of the array.\n",
    "#      .array_min(col) - Collection function: returns the minimum value of the array.\n",
    "#      .array_position(col, value) - Collection function: Locates the position of the first occurrence of the given value in the given array. Returns null if either of the arguments are null.\n",
    "#      .array_remove(col, element)- Collection function: Remove all elements that equal to element from the given array.\n",
    "#      .array_repeat(col, count) - Collection function: creates an array containing a column repeated count times.\n",
    "#      .array_sort(col) - Collection function: sorts the input array in ascending order. The elements of the input array must be orderable. Null elements will be placed at the end of the returned array.\n",
    "#      .array_union(col1, col2) - Collection function: returns an array of the elements in the union of col1 and col2, without duplicates.\n",
    "#      .arrays_overlap(a1, a2) - Collection function: returns true if the arrays contain any common non-null element; if not, returns null if both the arrays are non-empty and any of them contains a null element; returns false otherwise.\n",
    "#      .arrays_zip(*cols)[source] - Collection function: Returns a merged array of structs in which the N-th struct contains all N-th values of input arrays.\n",
    "\n",
    "customer = spark.createDataFrame([('coffee','milk','coffee','coffee','chocolate','')], ['item1', 'item2','item3','item4','item5','item6'])\n",
    "purchases = customer.select(array('item1', 'item2','item3').alias(\"Monday\"),\\\n",
    "                            array('item4', 'item5','item6').alias(\"Tuesday\"))\n",
    "\n",
    "print(\"array\")\n",
    "purchases.show()\n",
    "\n",
    "print(\"Which customers purchased milk? array_contains\")\n",
    "purchases.select(array_contains(purchases.Monday, \"milk\")).show(1, False)\n",
    "\n",
    "print(\"List of unique products purchased on Monday: array_distinct\")\n",
    "purchases.select(array_distinct(purchases.Monday)).show(1, False)\n",
    "\n",
    "print(\"What did our customers order on Monday but not Tuesday? array_except\")\n",
    "purchases.select(array_except(purchases.Monday, purchases.Tuesday)).show(1, False)\n",
    "\n",
    "print(\"What did our customers order on BOTH Monday and Tuesday?: array_intersect\")\n",
    "purchases.select(array_intersect(purchases.Monday, purchases.Tuesday)).show(1, False)\n",
    "\n",
    "print(\"All purchases on monday in a string: array_join\")\n",
    "purchases.select(array_join(purchases.Monday, ',')).show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create an array by splitting a string field** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "values = [(45,'I like to ride bikes'), \\\n",
    "          (14,'I like chicken'), \\\n",
    "          (63,'I like bubbles'), \\\n",
    "          (75,'I like roller coasters'), \\\n",
    "          (24,'I like shuffle board'), \\\n",
    "          (45,'I like to swim')]\n",
    "sentences = spark.createDataFrame(values,['age', 'sentence'])\n",
    "df = sentences.withColumn(\"array\", split(col(\"sentence\"), \" \"))\n",
    "df.show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Functions\n",
    "\n",
    "Functions as you know them in Python work a bit differently in Pyspark because it operates on a cluster. If you define a function the traditional Python way in PySpark, you will not recieve an error message but the call will not distribute on all nodes. So it will run slower. \n",
    "\n",
    "So to convert a Python function to what's called a user defined function (UDF) in PySpark. This is what you do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def square(x):\n",
    "    return int(x**2)\n",
    "square_udf = udf(lambda z: square(z), IntegerType())\n",
    "\n",
    "df.select('age',square_udf('age').alias('age_squared')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
